{"cells":[{"cell_type":"code","execution_count":264,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27006,"status":"ok","timestamp":1669810968948,"user":{"displayName":"hiroto iwasaki","userId":"11312076570438010727"},"user_tz":-540},"id":"8F1DxAOKhMg8"},"outputs":[],"source":["import os\n","import glob\n","import pandas as pd\n","from sklearn.decomposition import PCA\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn import metrics\n","from sklearn.metrics import (roc_curve, auc)\n","import matplotlib.pyplot as plt\n","from imblearn.under_sampling import CondensedNearestNeighbour\n","from imblearn.under_sampling import EditedNearestNeighbours\n","from collections import Counter\n","import glob\n","import os\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import optuna\n","import pandas as pd\n","import seaborn as sns\n","from imblearn.under_sampling import ClusterCentroids\n","from optuna.integration import lightgbm as lgb\n","from sklearn import metrics\n","from sklearn.decomposition import PCA\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import (PrecisionRecallDisplay, RocCurveDisplay,\n","                             accuracy_score, adjusted_mutual_info_score,\n","                             adjusted_rand_score, auc, average_precision_score,\n","                             balanced_accuracy_score, brier_score_loss,\n","                             calinski_harabasz_score, check_scoring,\n","                             classification_report, cluster, cohen_kappa_score,\n","                             completeness_score, confusion_matrix,\n","                             consensus_score, coverage_error, d2_tweedie_score,\n","                             davies_bouldin_score, dcg_score, det_curve,\n","                             euclidean_distances, explained_variance_score,\n","                             f1_score, fbeta_score, fowlkes_mallows_score,\n","                             get_scorer, hamming_loss, hinge_loss,\n","                             homogeneity_completeness_v_measure,\n","                             homogeneity_score, jaccard_score,\n","                             label_ranking_average_precision_score,\n","                             label_ranking_loss, log_loss, make_scorer,\n","                             matthews_corrcoef, max_error, mean_absolute_error,\n","                             mean_absolute_percentage_error,\n","                             mean_gamma_deviance, mean_pinball_loss,\n","                             mean_poisson_deviance, mean_squared_error,\n","                             mean_squared_log_error, mean_tweedie_deviance,\n","                             median_absolute_error,\n","                             multilabel_confusion_matrix, mutual_info_score,\n","                             nan_euclidean_distances, ndcg_score,\n","                             normalized_mutual_info_score,\n","                             pair_confusion_matrix, pairwise_distances,\n","                             pairwise_distances_argmin,\n","                             pairwise_distances_argmin_min,\n","                             pairwise_distances_chunked, pairwise_kernels,\n","                             plot_confusion_matrix, plot_det_curve,\n","                             plot_precision_recall_curve, plot_roc_curve,\n","                             precision_recall_curve,\n","                             precision_recall_fscore_support, precision_score,\n","                             r2_score, rand_score, recall_score, roc_auc_score,\n","                             roc_curve, silhouette_samples, silhouette_score,\n","                             top_k_accuracy_score, v_measure_score,\n","                             zero_one_loss)\n","from sklearn.model_selection import (BaseCrossValidator, BaseShuffleSplit,\n","                                     GridSearchCV, GroupKFold,\n","                                     GroupShuffleSplit, KFold,\n","                                     LeaveOneGroupOut, LeaveOneOut,\n","                                     LeavePGroupsOut, LeavePOut,\n","                                     PredefinedSplit, RepeatedKFold,\n","                                     RepeatedStratifiedKFold, ShuffleSplit,\n","                                     StratifiedGroupKFold, StratifiedKFold,\n","                                     StratifiedShuffleSplit,\n","                                     check_cv, cross_val_predict,\n","                                     cross_val_score, cross_validate,\n","                                     learning_curve, permutation_test_score,\n","                                     train_test_split, validation_curve)\n","from xgboost import XGBClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.manifold import TSNE"]},{"cell_type":"code","execution_count":103,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":589},"executionInfo":{"elapsed":1464,"status":"ok","timestamp":1669811128743,"user":{"displayName":"hiroto iwasaki","userId":"11312076570438010727"},"user_tz":-540},"id":"EDS52sgYi3Ny","outputId":"242a8cbe-72d8-4a76-dafa-e99985a841b8"},"outputs":[],"source":["train = pd.read_csv(\"train.csv\")\n","test = pd.read_csv(\"test.csv\")\n","colname = ['HomePlanet', 'CryoSleep', 'Cabin', 'Destination', 'Age', 'VIP', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'Transported']"]},{"cell_type":"code","execution_count":104,"metadata":{},"outputs":[],"source":["df = train[colname]"]},{"cell_type":"code","execution_count":105,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["df_obj_colname: Index(['HomePlanet', 'CryoSleep', 'Cabin', 'Destination', 'VIP'], dtype='object')\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>PassengerId</th>\n","      <th>HomePlanet</th>\n","      <th>CryoSleep</th>\n","      <th>Cabin</th>\n","      <th>Destination</th>\n","      <th>Age</th>\n","      <th>VIP</th>\n","      <th>RoomService</th>\n","      <th>FoodCourt</th>\n","      <th>ShoppingMall</th>\n","      <th>Spa</th>\n","      <th>VRDeck</th>\n","      <th>Name</th>\n","      <th>Transported</th>\n","      <th>CabinDeck</th>\n","      <th>CabinNum</th>\n","      <th>CabinSide</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0001_01</td>\n","      <td>Europa</td>\n","      <td>False</td>\n","      <td>B/0/P</td>\n","      <td>TRAPPIST-1e</td>\n","      <td>39.0</td>\n","      <td>False</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>Maham Ofracculy</td>\n","      <td>False</td>\n","      <td>B</td>\n","      <td>0</td>\n","      <td>P</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0002_01</td>\n","      <td>Earth</td>\n","      <td>False</td>\n","      <td>F/0/S</td>\n","      <td>TRAPPIST-1e</td>\n","      <td>24.0</td>\n","      <td>False</td>\n","      <td>109.0</td>\n","      <td>9.0</td>\n","      <td>25.0</td>\n","      <td>549.0</td>\n","      <td>44.0</td>\n","      <td>Juanna Vines</td>\n","      <td>True</td>\n","      <td>F</td>\n","      <td>0</td>\n","      <td>S</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0003_01</td>\n","      <td>Europa</td>\n","      <td>False</td>\n","      <td>A/0/S</td>\n","      <td>TRAPPIST-1e</td>\n","      <td>58.0</td>\n","      <td>True</td>\n","      <td>43.0</td>\n","      <td>3576.0</td>\n","      <td>0.0</td>\n","      <td>6715.0</td>\n","      <td>49.0</td>\n","      <td>Altark Susent</td>\n","      <td>False</td>\n","      <td>A</td>\n","      <td>0</td>\n","      <td>S</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0003_02</td>\n","      <td>Europa</td>\n","      <td>False</td>\n","      <td>A/0/S</td>\n","      <td>TRAPPIST-1e</td>\n","      <td>33.0</td>\n","      <td>False</td>\n","      <td>0.0</td>\n","      <td>1283.0</td>\n","      <td>371.0</td>\n","      <td>3329.0</td>\n","      <td>193.0</td>\n","      <td>Solam Susent</td>\n","      <td>False</td>\n","      <td>A</td>\n","      <td>0</td>\n","      <td>S</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0004_01</td>\n","      <td>Earth</td>\n","      <td>False</td>\n","      <td>F/1/S</td>\n","      <td>TRAPPIST-1e</td>\n","      <td>16.0</td>\n","      <td>False</td>\n","      <td>303.0</td>\n","      <td>70.0</td>\n","      <td>151.0</td>\n","      <td>565.0</td>\n","      <td>2.0</td>\n","      <td>Willy Santantines</td>\n","      <td>True</td>\n","      <td>F</td>\n","      <td>1</td>\n","      <td>S</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>8688</th>\n","      <td>9276_01</td>\n","      <td>Europa</td>\n","      <td>False</td>\n","      <td>A/98/P</td>\n","      <td>55 Cancri e</td>\n","      <td>41.0</td>\n","      <td>True</td>\n","      <td>0.0</td>\n","      <td>6819.0</td>\n","      <td>0.0</td>\n","      <td>1643.0</td>\n","      <td>74.0</td>\n","      <td>Gravior Noxnuther</td>\n","      <td>False</td>\n","      <td>A</td>\n","      <td>98</td>\n","      <td>P</td>\n","    </tr>\n","    <tr>\n","      <th>8689</th>\n","      <td>9278_01</td>\n","      <td>Earth</td>\n","      <td>True</td>\n","      <td>G/1499/S</td>\n","      <td>PSO J318.5-22</td>\n","      <td>18.0</td>\n","      <td>False</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>Kurta Mondalley</td>\n","      <td>False</td>\n","      <td>G</td>\n","      <td>1499</td>\n","      <td>S</td>\n","    </tr>\n","    <tr>\n","      <th>8690</th>\n","      <td>9279_01</td>\n","      <td>Earth</td>\n","      <td>False</td>\n","      <td>G/1500/S</td>\n","      <td>TRAPPIST-1e</td>\n","      <td>26.0</td>\n","      <td>False</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1872.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>Fayey Connon</td>\n","      <td>True</td>\n","      <td>G</td>\n","      <td>1500</td>\n","      <td>S</td>\n","    </tr>\n","    <tr>\n","      <th>8691</th>\n","      <td>9280_01</td>\n","      <td>Europa</td>\n","      <td>False</td>\n","      <td>E/608/S</td>\n","      <td>55 Cancri e</td>\n","      <td>32.0</td>\n","      <td>False</td>\n","      <td>0.0</td>\n","      <td>1049.0</td>\n","      <td>0.0</td>\n","      <td>353.0</td>\n","      <td>3235.0</td>\n","      <td>Celeon Hontichre</td>\n","      <td>False</td>\n","      <td>E</td>\n","      <td>608</td>\n","      <td>S</td>\n","    </tr>\n","    <tr>\n","      <th>8692</th>\n","      <td>9280_02</td>\n","      <td>Europa</td>\n","      <td>False</td>\n","      <td>E/608/S</td>\n","      <td>TRAPPIST-1e</td>\n","      <td>44.0</td>\n","      <td>False</td>\n","      <td>126.0</td>\n","      <td>4688.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>12.0</td>\n","      <td>Propsh Hontichre</td>\n","      <td>True</td>\n","      <td>E</td>\n","      <td>608</td>\n","      <td>S</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>8693 rows × 17 columns</p>\n","</div>"],"text/plain":["     PassengerId HomePlanet CryoSleep     Cabin    Destination   Age    VIP  \\\n","0        0001_01     Europa     False     B/0/P    TRAPPIST-1e  39.0  False   \n","1        0002_01      Earth     False     F/0/S    TRAPPIST-1e  24.0  False   \n","2        0003_01     Europa     False     A/0/S    TRAPPIST-1e  58.0   True   \n","3        0003_02     Europa     False     A/0/S    TRAPPIST-1e  33.0  False   \n","4        0004_01      Earth     False     F/1/S    TRAPPIST-1e  16.0  False   \n","...          ...        ...       ...       ...            ...   ...    ...   \n","8688     9276_01     Europa     False    A/98/P    55 Cancri e  41.0   True   \n","8689     9278_01      Earth      True  G/1499/S  PSO J318.5-22  18.0  False   \n","8690     9279_01      Earth     False  G/1500/S    TRAPPIST-1e  26.0  False   \n","8691     9280_01     Europa     False   E/608/S    55 Cancri e  32.0  False   \n","8692     9280_02     Europa     False   E/608/S    TRAPPIST-1e  44.0  False   \n","\n","      RoomService  FoodCourt  ShoppingMall     Spa  VRDeck               Name  \\\n","0             0.0        0.0           0.0     0.0     0.0    Maham Ofracculy   \n","1           109.0        9.0          25.0   549.0    44.0       Juanna Vines   \n","2            43.0     3576.0           0.0  6715.0    49.0      Altark Susent   \n","3             0.0     1283.0         371.0  3329.0   193.0       Solam Susent   \n","4           303.0       70.0         151.0   565.0     2.0  Willy Santantines   \n","...           ...        ...           ...     ...     ...                ...   \n","8688          0.0     6819.0           0.0  1643.0    74.0  Gravior Noxnuther   \n","8689          0.0        0.0           0.0     0.0     0.0    Kurta Mondalley   \n","8690          0.0        0.0        1872.0     1.0     0.0       Fayey Connon   \n","8691          0.0     1049.0           0.0   353.0  3235.0   Celeon Hontichre   \n","8692        126.0     4688.0           0.0     0.0    12.0   Propsh Hontichre   \n","\n","      Transported CabinDeck CabinNum CabinSide  \n","0           False         B        0         P  \n","1            True         F        0         S  \n","2           False         A        0         S  \n","3           False         A        0         S  \n","4            True         F        1         S  \n","...           ...       ...      ...       ...  \n","8688        False         A       98         P  \n","8689        False         G     1499         S  \n","8690         True         G     1500         S  \n","8691        False         E      608         S  \n","8692         True         E      608         S  \n","\n","[8693 rows x 17 columns]"]},"metadata":{},"output_type":"display_data"}],"source":["df_obj_colname = df.columns[df.dtypes == \"object\"]\n","print(f\"df_obj_colname: {df_obj_colname}\")\n","\n","train[['CabinDeck','CabinNum','CabinSide']] = train['Cabin'].str.split('/', expand=True)\n","test[['CabinDeck','CabinNum','CabinSide']] = test['Cabin'].str.split('/', expand=True)\n","display(train)"]},{"cell_type":"code","execution_count":106,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":533},"executionInfo":{"elapsed":75,"status":"ok","timestamp":1669811128747,"user":{"displayName":"hiroto iwasaki","userId":"11312076570438010727"},"user_tz":-540},"id":"LxpeYI4GZJef","outputId":"e4baa999-c5c4-4056-d3b0-bc9ee6ccf365"},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["CabinDeck\n","A     256\n","B     779\n","C     747\n","D     478\n","E     876\n","F    2794\n","G    2559\n","T       5\n","Name: PassengerId, dtype: int64\n","CabinSide\n","P    4206\n","S    4288\n","Name: PassengerId, dtype: int64\n"]}],"source":["plt.hist(train.groupby(by=[\"CabinNum\"])[\"PassengerId\"].count())\n","plt.title(\"CabinNum-PassengerID\")\n","plt.show()\n","\n","train[\"CabinNum\"] = pd.to_numeric(train[\"CabinNum\"])\n","test[\"CabinNum\"] = pd.to_numeric(test[\"CabinNum\"])\n","\n","print(train.groupby(by=[\"CabinDeck\"])[\"PassengerId\"].count())\n","\n","print(train.groupby(by=[\"CabinSide\"])[\"PassengerId\"].count())"]},{"cell_type":"code","execution_count":107,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":477},"executionInfo":{"elapsed":67,"status":"ok","timestamp":1669811128749,"user":{"displayName":"hiroto iwasaki","userId":"11312076570438010727"},"user_tz":-540},"id":"3yRhDjM6ZFvG","outputId":"f46c5745-481f-4593-e125-d2d764617d7a"},"outputs":[{"name":"stdout","output_type":"stream","text":["train_dummy_colname: Index(['HomePlanet', 'CryoSleep', 'Destination', 'VIP', 'CabinDeck',\n","       'CabinNum', 'CabinSide'],\n","      dtype='object')\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>HomePlanet</th>\n","      <th>CryoSleep</th>\n","      <th>Destination</th>\n","      <th>Age</th>\n","      <th>VIP</th>\n","      <th>RoomService</th>\n","      <th>FoodCourt</th>\n","      <th>ShoppingMall</th>\n","      <th>Spa</th>\n","      <th>VRDeck</th>\n","      <th>Name</th>\n","      <th>Transported</th>\n","      <th>CabinDeck</th>\n","      <th>CabinNum</th>\n","      <th>CabinSide</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Europa</td>\n","      <td>False</td>\n","      <td>TRAPPIST-1e</td>\n","      <td>39.0</td>\n","      <td>False</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>Maham Ofracculy</td>\n","      <td>False</td>\n","      <td>B</td>\n","      <td>0.0</td>\n","      <td>P</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Earth</td>\n","      <td>False</td>\n","      <td>TRAPPIST-1e</td>\n","      <td>24.0</td>\n","      <td>False</td>\n","      <td>109.0</td>\n","      <td>9.0</td>\n","      <td>25.0</td>\n","      <td>549.0</td>\n","      <td>44.0</td>\n","      <td>Juanna Vines</td>\n","      <td>True</td>\n","      <td>F</td>\n","      <td>0.0</td>\n","      <td>S</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Europa</td>\n","      <td>False</td>\n","      <td>TRAPPIST-1e</td>\n","      <td>58.0</td>\n","      <td>True</td>\n","      <td>43.0</td>\n","      <td>3576.0</td>\n","      <td>0.0</td>\n","      <td>6715.0</td>\n","      <td>49.0</td>\n","      <td>Altark Susent</td>\n","      <td>False</td>\n","      <td>A</td>\n","      <td>0.0</td>\n","      <td>S</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Europa</td>\n","      <td>False</td>\n","      <td>TRAPPIST-1e</td>\n","      <td>33.0</td>\n","      <td>False</td>\n","      <td>0.0</td>\n","      <td>1283.0</td>\n","      <td>371.0</td>\n","      <td>3329.0</td>\n","      <td>193.0</td>\n","      <td>Solam Susent</td>\n","      <td>False</td>\n","      <td>A</td>\n","      <td>0.0</td>\n","      <td>S</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Earth</td>\n","      <td>False</td>\n","      <td>TRAPPIST-1e</td>\n","      <td>16.0</td>\n","      <td>False</td>\n","      <td>303.0</td>\n","      <td>70.0</td>\n","      <td>151.0</td>\n","      <td>565.0</td>\n","      <td>2.0</td>\n","      <td>Willy Santantines</td>\n","      <td>True</td>\n","      <td>F</td>\n","      <td>1.0</td>\n","      <td>S</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>8688</th>\n","      <td>Europa</td>\n","      <td>False</td>\n","      <td>55 Cancri e</td>\n","      <td>41.0</td>\n","      <td>True</td>\n","      <td>0.0</td>\n","      <td>6819.0</td>\n","      <td>0.0</td>\n","      <td>1643.0</td>\n","      <td>74.0</td>\n","      <td>Gravior Noxnuther</td>\n","      <td>False</td>\n","      <td>A</td>\n","      <td>98.0</td>\n","      <td>P</td>\n","    </tr>\n","    <tr>\n","      <th>8689</th>\n","      <td>Earth</td>\n","      <td>True</td>\n","      <td>PSO J318.5-22</td>\n","      <td>18.0</td>\n","      <td>False</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>Kurta Mondalley</td>\n","      <td>False</td>\n","      <td>G</td>\n","      <td>1499.0</td>\n","      <td>S</td>\n","    </tr>\n","    <tr>\n","      <th>8690</th>\n","      <td>Earth</td>\n","      <td>False</td>\n","      <td>TRAPPIST-1e</td>\n","      <td>26.0</td>\n","      <td>False</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1872.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>Fayey Connon</td>\n","      <td>True</td>\n","      <td>G</td>\n","      <td>1500.0</td>\n","      <td>S</td>\n","    </tr>\n","    <tr>\n","      <th>8691</th>\n","      <td>Europa</td>\n","      <td>False</td>\n","      <td>55 Cancri e</td>\n","      <td>32.0</td>\n","      <td>False</td>\n","      <td>0.0</td>\n","      <td>1049.0</td>\n","      <td>0.0</td>\n","      <td>353.0</td>\n","      <td>3235.0</td>\n","      <td>Celeon Hontichre</td>\n","      <td>False</td>\n","      <td>E</td>\n","      <td>608.0</td>\n","      <td>S</td>\n","    </tr>\n","    <tr>\n","      <th>8692</th>\n","      <td>Europa</td>\n","      <td>False</td>\n","      <td>TRAPPIST-1e</td>\n","      <td>44.0</td>\n","      <td>False</td>\n","      <td>126.0</td>\n","      <td>4688.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>12.0</td>\n","      <td>Propsh Hontichre</td>\n","      <td>True</td>\n","      <td>E</td>\n","      <td>608.0</td>\n","      <td>S</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>6606 rows × 15 columns</p>\n","</div>"],"text/plain":["     HomePlanet CryoSleep    Destination   Age    VIP  RoomService  FoodCourt  \\\n","0        Europa     False    TRAPPIST-1e  39.0  False          0.0        0.0   \n","1         Earth     False    TRAPPIST-1e  24.0  False        109.0        9.0   \n","2        Europa     False    TRAPPIST-1e  58.0   True         43.0     3576.0   \n","3        Europa     False    TRAPPIST-1e  33.0  False          0.0     1283.0   \n","4         Earth     False    TRAPPIST-1e  16.0  False        303.0       70.0   \n","...         ...       ...            ...   ...    ...          ...        ...   \n","8688     Europa     False    55 Cancri e  41.0   True          0.0     6819.0   \n","8689      Earth      True  PSO J318.5-22  18.0  False          0.0        0.0   \n","8690      Earth     False    TRAPPIST-1e  26.0  False          0.0        0.0   \n","8691     Europa     False    55 Cancri e  32.0  False          0.0     1049.0   \n","8692     Europa     False    TRAPPIST-1e  44.0  False        126.0     4688.0   \n","\n","      ShoppingMall     Spa  VRDeck               Name  Transported CabinDeck  \\\n","0              0.0     0.0     0.0    Maham Ofracculy        False         B   \n","1             25.0   549.0    44.0       Juanna Vines         True         F   \n","2              0.0  6715.0    49.0      Altark Susent        False         A   \n","3            371.0  3329.0   193.0       Solam Susent        False         A   \n","4            151.0   565.0     2.0  Willy Santantines         True         F   \n","...            ...     ...     ...                ...          ...       ...   \n","8688           0.0  1643.0    74.0  Gravior Noxnuther        False         A   \n","8689           0.0     0.0     0.0    Kurta Mondalley        False         G   \n","8690        1872.0     1.0     0.0       Fayey Connon         True         G   \n","8691           0.0   353.0  3235.0   Celeon Hontichre        False         E   \n","8692           0.0     0.0    12.0   Propsh Hontichre         True         E   \n","\n","      CabinNum CabinSide  \n","0          0.0         P  \n","1          0.0         S  \n","2          0.0         S  \n","3          0.0         S  \n","4          1.0         S  \n","...        ...       ...  \n","8688      98.0         P  \n","8689    1499.0         S  \n","8690    1500.0         S  \n","8691     608.0         S  \n","8692     608.0         S  \n","\n","[6606 rows x 15 columns]"]},"metadata":{},"output_type":"display_data"}],"source":["train = train.drop(\"Cabin\", axis=1)\n","test = test.drop(\"Cabin\", axis=1)\n","\n","train_dummy_colname = train[['HomePlanet', 'CryoSleep', 'Destination', 'VIP', \"CabinDeck\", \"CabinNum\", \"CabinSide\"]].columns\n","print(f\"train_dummy_colname: {train_dummy_colname}\")\n","\n","train_drop = train.drop(\"PassengerId\", axis=1)\n","train_drop.dropna(inplace=True)\n","\n","display(train_drop)"]},{"cell_type":"code","execution_count":108,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":49,"status":"ok","timestamp":1669811128754,"user":{"displayName":"hiroto iwasaki","userId":"11312076570438010727"},"user_tz":-540},"id":"QHta1Upvtp89","outputId":"d21ede07-ab55-4bc7-c58e-84f70f40dca1"},"outputs":[{"data":{"text/plain":["[array(['Europa', 'Earth', 'Mars', nan], dtype=object),\n"," array([False, True, nan], dtype=object),\n"," array(['B/0/P', 'F/0/S', 'A/0/S', ..., 'G/1499/S', 'G/1500/S', 'E/608/S'],\n","       dtype=object),\n"," array(['TRAPPIST-1e', 'PSO J318.5-22', '55 Cancri e', nan], dtype=object),\n"," array([39., 24., 58., 33., 16., 44., 26., 28., 35., 14., 34., 45., 32.,\n","        48., 31., 27.,  0.,  1., 49., 29., 10.,  7., 21., 62., 15., 43.,\n","        47.,  2., 20., 23., 30., 17., 55.,  4., 19., 56., nan, 25., 38.,\n","        36., 22., 18., 42., 37., 13.,  8., 40.,  3., 54.,  9.,  6., 64.,\n","        67., 61., 50., 41., 57., 11., 52., 51., 46., 60., 63., 59.,  5.,\n","        79., 68., 74., 12., 53., 65., 71., 75., 70., 76., 78., 73., 66.,\n","        69., 72., 77.])]"]},"execution_count":108,"metadata":{},"output_type":"execute_result"}],"source":["df_data_list = []\n","for i in range(len(df_obj_colname)):\n","  df_colname = df.columns[i]\n","  df_data = pd.unique(df[df_colname])\n","  df_data_list.append(df_data)\n","df_data_list"]},{"cell_type":"code","execution_count":130,"metadata":{"executionInfo":{"elapsed":42,"status":"ok","timestamp":1669811128756,"user":{"displayName":"hiroto iwasaki","userId":"11312076570438010727"},"user_tz":-540},"id":"sDYlrNn96iCB"},"outputs":[],"source":["dataset = pd.get_dummies(train_drop[train_dummy_colname])"]},{"cell_type":"code","execution_count":152,"metadata":{},"outputs":[],"source":["y = train[\"Transported\"]"]},{"cell_type":"code","execution_count":155,"metadata":{},"outputs":[{"data":{"text/plain":["Int64Index([   0,    1,    2,    3,    4,    5,    6,    8,    9,   11,\n","            ...\n","            8681, 8682, 8683, 8685, 8686, 8688, 8689, 8690, 8691, 8692],\n","           dtype='int64', length=6606)"]},"execution_count":155,"metadata":{},"output_type":"execute_result"}],"source":["dataset.index"]},{"cell_type":"code","execution_count":153,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>CabinNum</th>\n","      <th>HomePlanet_Earth</th>\n","      <th>HomePlanet_Europa</th>\n","      <th>HomePlanet_Mars</th>\n","      <th>CryoSleep_False</th>\n","      <th>CryoSleep_True</th>\n","      <th>Destination_55 Cancri e</th>\n","      <th>Destination_PSO J318.5-22</th>\n","      <th>Destination_TRAPPIST-1e</th>\n","      <th>VIP_False</th>\n","      <th>...</th>\n","      <th>CabinDeck_A</th>\n","      <th>CabinDeck_B</th>\n","      <th>CabinDeck_C</th>\n","      <th>CabinDeck_D</th>\n","      <th>CabinDeck_E</th>\n","      <th>CabinDeck_F</th>\n","      <th>CabinDeck_G</th>\n","      <th>CabinDeck_T</th>\n","      <th>CabinSide_P</th>\n","      <th>CabinSide_S</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>8688</th>\n","      <td>98.0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>8689</th>\n","      <td>1499.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>8690</th>\n","      <td>1500.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>8691</th>\n","      <td>608.0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>8692</th>\n","      <td>608.0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>6606 rows × 21 columns</p>\n","</div>"],"text/plain":["      CabinNum  HomePlanet_Earth  HomePlanet_Europa  HomePlanet_Mars  \\\n","0          0.0                 0                  1                0   \n","1          0.0                 1                  0                0   \n","2          0.0                 0                  1                0   \n","3          0.0                 0                  1                0   \n","4          1.0                 1                  0                0   \n","...        ...               ...                ...              ...   \n","8688      98.0                 0                  1                0   \n","8689    1499.0                 1                  0                0   \n","8690    1500.0                 1                  0                0   \n","8691     608.0                 0                  1                0   \n","8692     608.0                 0                  1                0   \n","\n","      CryoSleep_False  CryoSleep_True  Destination_55 Cancri e  \\\n","0                   1               0                        0   \n","1                   1               0                        0   \n","2                   1               0                        0   \n","3                   1               0                        0   \n","4                   1               0                        0   \n","...               ...             ...                      ...   \n","8688                1               0                        1   \n","8689                0               1                        0   \n","8690                1               0                        0   \n","8691                1               0                        1   \n","8692                1               0                        0   \n","\n","      Destination_PSO J318.5-22  Destination_TRAPPIST-1e  VIP_False  ...  \\\n","0                             0                        1          1  ...   \n","1                             0                        1          1  ...   \n","2                             0                        1          0  ...   \n","3                             0                        1          1  ...   \n","4                             0                        1          1  ...   \n","...                         ...                      ...        ...  ...   \n","8688                          0                        0          0  ...   \n","8689                          1                        0          1  ...   \n","8690                          0                        1          1  ...   \n","8691                          0                        0          1  ...   \n","8692                          0                        1          1  ...   \n","\n","      CabinDeck_A  CabinDeck_B  CabinDeck_C  CabinDeck_D  CabinDeck_E  \\\n","0               0            1            0            0            0   \n","1               0            0            0            0            0   \n","2               1            0            0            0            0   \n","3               1            0            0            0            0   \n","4               0            0            0            0            0   \n","...           ...          ...          ...          ...          ...   \n","8688            1            0            0            0            0   \n","8689            0            0            0            0            0   \n","8690            0            0            0            0            0   \n","8691            0            0            0            0            1   \n","8692            0            0            0            0            1   \n","\n","      CabinDeck_F  CabinDeck_G  CabinDeck_T  CabinSide_P  CabinSide_S  \n","0               0            0            0            1            0  \n","1               1            0            0            0            1  \n","2               0            0            0            0            1  \n","3               0            0            0            0            1  \n","4               1            0            0            0            1  \n","...           ...          ...          ...          ...          ...  \n","8688            0            0            0            1            0  \n","8689            0            1            0            0            1  \n","8690            0            1            0            0            1  \n","8691            0            0            0            0            1  \n","8692            0            0            0            0            1  \n","\n","[6606 rows x 21 columns]"]},"execution_count":153,"metadata":{},"output_type":"execute_result"}],"source":["dataset"]},{"cell_type":"code","execution_count":162,"metadata":{},"outputs":[],"source":["y = y.iloc[dataset.index, ]"]},{"cell_type":"code","execution_count":164,"metadata":{},"outputs":[],"source":["df = pd.concat([dataset, y], axis=1)"]},{"cell_type":"code","execution_count":165,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>CabinNum</th>\n","      <th>HomePlanet_Earth</th>\n","      <th>HomePlanet_Europa</th>\n","      <th>HomePlanet_Mars</th>\n","      <th>CryoSleep_False</th>\n","      <th>CryoSleep_True</th>\n","      <th>Destination_55 Cancri e</th>\n","      <th>Destination_PSO J318.5-22</th>\n","      <th>Destination_TRAPPIST-1e</th>\n","      <th>VIP_False</th>\n","      <th>...</th>\n","      <th>CabinDeck_B</th>\n","      <th>CabinDeck_C</th>\n","      <th>CabinDeck_D</th>\n","      <th>CabinDeck_E</th>\n","      <th>CabinDeck_F</th>\n","      <th>CabinDeck_G</th>\n","      <th>CabinDeck_T</th>\n","      <th>CabinSide_P</th>\n","      <th>CabinSide_S</th>\n","      <th>Transported</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>8688</th>\n","      <td>98.0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>8689</th>\n","      <td>1499.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>8690</th>\n","      <td>1500.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>8691</th>\n","      <td>608.0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>8692</th>\n","      <td>608.0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>True</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>6606 rows × 22 columns</p>\n","</div>"],"text/plain":["      CabinNum  HomePlanet_Earth  HomePlanet_Europa  HomePlanet_Mars  \\\n","0          0.0                 0                  1                0   \n","1          0.0                 1                  0                0   \n","2          0.0                 0                  1                0   \n","3          0.0                 0                  1                0   \n","4          1.0                 1                  0                0   \n","...        ...               ...                ...              ...   \n","8688      98.0                 0                  1                0   \n","8689    1499.0                 1                  0                0   \n","8690    1500.0                 1                  0                0   \n","8691     608.0                 0                  1                0   \n","8692     608.0                 0                  1                0   \n","\n","      CryoSleep_False  CryoSleep_True  Destination_55 Cancri e  \\\n","0                   1               0                        0   \n","1                   1               0                        0   \n","2                   1               0                        0   \n","3                   1               0                        0   \n","4                   1               0                        0   \n","...               ...             ...                      ...   \n","8688                1               0                        1   \n","8689                0               1                        0   \n","8690                1               0                        0   \n","8691                1               0                        1   \n","8692                1               0                        0   \n","\n","      Destination_PSO J318.5-22  Destination_TRAPPIST-1e  VIP_False  ...  \\\n","0                             0                        1          1  ...   \n","1                             0                        1          1  ...   \n","2                             0                        1          0  ...   \n","3                             0                        1          1  ...   \n","4                             0                        1          1  ...   \n","...                         ...                      ...        ...  ...   \n","8688                          0                        0          0  ...   \n","8689                          1                        0          1  ...   \n","8690                          0                        1          1  ...   \n","8691                          0                        0          1  ...   \n","8692                          0                        1          1  ...   \n","\n","      CabinDeck_B  CabinDeck_C  CabinDeck_D  CabinDeck_E  CabinDeck_F  \\\n","0               1            0            0            0            0   \n","1               0            0            0            0            1   \n","2               0            0            0            0            0   \n","3               0            0            0            0            0   \n","4               0            0            0            0            1   \n","...           ...          ...          ...          ...          ...   \n","8688            0            0            0            0            0   \n","8689            0            0            0            0            0   \n","8690            0            0            0            0            0   \n","8691            0            0            0            1            0   \n","8692            0            0            0            1            0   \n","\n","      CabinDeck_G  CabinDeck_T  CabinSide_P  CabinSide_S  Transported  \n","0               0            0            1            0        False  \n","1               0            0            0            1         True  \n","2               0            0            0            1        False  \n","3               0            0            0            1        False  \n","4               0            0            0            1         True  \n","...           ...          ...          ...          ...          ...  \n","8688            0            0            1            0        False  \n","8689            1            0            0            1        False  \n","8690            1            0            0            1         True  \n","8691            0            0            0            1        False  \n","8692            0            0            0            1         True  \n","\n","[6606 rows x 22 columns]"]},"execution_count":165,"metadata":{},"output_type":"execute_result"}],"source":["df"]},{"cell_type":"code","execution_count":166,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1669811131785,"user":{"displayName":"hiroto iwasaki","userId":"11312076570438010727"},"user_tz":-540},"id":"1yzRvnqggOBe"},"outputs":[],"source":["X = df.iloc[:, 0:-1]\n","y = df.iloc[:, -1] "]},{"cell_type":"code","execution_count":167,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1669811145603,"user":{"displayName":"hiroto iwasaki","userId":"11312076570438010727"},"user_tz":-540},"id":"DHhfUsD-lpcL"},"outputs":[],"source":["X.to_csv(\"X.csv\")\n","y.to_csv(\"y.csv\")"]},{"cell_type":"code","execution_count":168,"metadata":{},"outputs":[],"source":["X_scaled = (X-X.mean())/X.std()"]},{"cell_type":"code","execution_count":169,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>CabinNum</th>\n","      <th>HomePlanet_Earth</th>\n","      <th>HomePlanet_Europa</th>\n","      <th>HomePlanet_Mars</th>\n","      <th>CryoSleep_False</th>\n","      <th>CryoSleep_True</th>\n","      <th>Destination_55 Cancri e</th>\n","      <th>Destination_PSO J318.5-22</th>\n","      <th>Destination_TRAPPIST-1e</th>\n","      <th>VIP_False</th>\n","      <th>...</th>\n","      <th>CabinDeck_A</th>\n","      <th>CabinDeck_B</th>\n","      <th>CabinDeck_C</th>\n","      <th>CabinDeck_D</th>\n","      <th>CabinDeck_E</th>\n","      <th>CabinDeck_F</th>\n","      <th>CabinDeck_G</th>\n","      <th>CabinDeck_T</th>\n","      <th>CabinSide_P</th>\n","      <th>CabinSide_S</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>-1.166962</td>\n","      <td>-1.082981</td>\n","      <td>1.717017</td>\n","      <td>-0.510772</td>\n","      <td>0.738608</td>\n","      <td>-0.738608</td>\n","      <td>-0.520181</td>\n","      <td>-0.322665</td>\n","      <td>0.665997</td>\n","      <td>0.158543</td>\n","      <td>...</td>\n","      <td>-0.179844</td>\n","      <td>3.085072</td>\n","      <td>-0.312266</td>\n","      <td>-0.244957</td>\n","      <td>-0.339552</td>\n","      <td>-0.695045</td>\n","      <td>-0.652529</td>\n","      <td>-0.017401</td>\n","      <td>1.012721</td>\n","      <td>-1.012721</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>-1.166962</td>\n","      <td>0.923237</td>\n","      <td>-0.582317</td>\n","      <td>-0.510772</td>\n","      <td>0.738608</td>\n","      <td>-0.738608</td>\n","      <td>-0.520181</td>\n","      <td>-0.322665</td>\n","      <td>0.665997</td>\n","      <td>0.158543</td>\n","      <td>...</td>\n","      <td>-0.179844</td>\n","      <td>-0.324093</td>\n","      <td>-0.312266</td>\n","      <td>-0.244957</td>\n","      <td>-0.339552</td>\n","      <td>1.438537</td>\n","      <td>-0.652529</td>\n","      <td>-0.017401</td>\n","      <td>-0.987289</td>\n","      <td>0.987289</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>-1.166962</td>\n","      <td>-1.082981</td>\n","      <td>1.717017</td>\n","      <td>-0.510772</td>\n","      <td>0.738608</td>\n","      <td>-0.738608</td>\n","      <td>-0.520181</td>\n","      <td>-0.322665</td>\n","      <td>0.665997</td>\n","      <td>-6.306485</td>\n","      <td>...</td>\n","      <td>5.559529</td>\n","      <td>-0.324093</td>\n","      <td>-0.312266</td>\n","      <td>-0.244957</td>\n","      <td>-0.339552</td>\n","      <td>-0.695045</td>\n","      <td>-0.652529</td>\n","      <td>-0.017401</td>\n","      <td>-0.987289</td>\n","      <td>0.987289</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>-1.166962</td>\n","      <td>-1.082981</td>\n","      <td>1.717017</td>\n","      <td>-0.510772</td>\n","      <td>0.738608</td>\n","      <td>-0.738608</td>\n","      <td>-0.520181</td>\n","      <td>-0.322665</td>\n","      <td>0.665997</td>\n","      <td>0.158543</td>\n","      <td>...</td>\n","      <td>5.559529</td>\n","      <td>-0.324093</td>\n","      <td>-0.312266</td>\n","      <td>-0.244957</td>\n","      <td>-0.339552</td>\n","      <td>-0.695045</td>\n","      <td>-0.652529</td>\n","      <td>-0.017401</td>\n","      <td>-0.987289</td>\n","      <td>0.987289</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>-1.165015</td>\n","      <td>0.923237</td>\n","      <td>-0.582317</td>\n","      <td>-0.510772</td>\n","      <td>0.738608</td>\n","      <td>-0.738608</td>\n","      <td>-0.520181</td>\n","      <td>-0.322665</td>\n","      <td>0.665997</td>\n","      <td>0.158543</td>\n","      <td>...</td>\n","      <td>-0.179844</td>\n","      <td>-0.324093</td>\n","      <td>-0.312266</td>\n","      <td>-0.244957</td>\n","      <td>-0.339552</td>\n","      <td>1.438537</td>\n","      <td>-0.652529</td>\n","      <td>-0.017401</td>\n","      <td>-0.987289</td>\n","      <td>0.987289</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>8688</th>\n","      <td>-0.976097</td>\n","      <td>-1.082981</td>\n","      <td>1.717017</td>\n","      <td>-0.510772</td>\n","      <td>0.738608</td>\n","      <td>-0.738608</td>\n","      <td>1.922118</td>\n","      <td>-0.322665</td>\n","      <td>-1.501281</td>\n","      <td>-6.306485</td>\n","      <td>...</td>\n","      <td>5.559529</td>\n","      <td>-0.324093</td>\n","      <td>-0.312266</td>\n","      <td>-0.244957</td>\n","      <td>-0.339552</td>\n","      <td>-0.695045</td>\n","      <td>-0.652529</td>\n","      <td>-0.017401</td>\n","      <td>1.012721</td>\n","      <td>-1.012721</td>\n","    </tr>\n","    <tr>\n","      <th>8689</th>\n","      <td>1.752501</td>\n","      <td>0.923237</td>\n","      <td>-0.582317</td>\n","      <td>-0.510772</td>\n","      <td>-1.353693</td>\n","      <td>1.353693</td>\n","      <td>-0.520181</td>\n","      <td>3.098722</td>\n","      <td>-1.501281</td>\n","      <td>0.158543</td>\n","      <td>...</td>\n","      <td>-0.179844</td>\n","      <td>-0.324093</td>\n","      <td>-0.312266</td>\n","      <td>-0.244957</td>\n","      <td>-0.339552</td>\n","      <td>-0.695045</td>\n","      <td>1.532268</td>\n","      <td>-0.017401</td>\n","      <td>-0.987289</td>\n","      <td>0.987289</td>\n","    </tr>\n","    <tr>\n","      <th>8690</th>\n","      <td>1.754448</td>\n","      <td>0.923237</td>\n","      <td>-0.582317</td>\n","      <td>-0.510772</td>\n","      <td>0.738608</td>\n","      <td>-0.738608</td>\n","      <td>-0.520181</td>\n","      <td>-0.322665</td>\n","      <td>0.665997</td>\n","      <td>0.158543</td>\n","      <td>...</td>\n","      <td>-0.179844</td>\n","      <td>-0.324093</td>\n","      <td>-0.312266</td>\n","      <td>-0.244957</td>\n","      <td>-0.339552</td>\n","      <td>-0.695045</td>\n","      <td>1.532268</td>\n","      <td>-0.017401</td>\n","      <td>-0.987289</td>\n","      <td>0.987289</td>\n","    </tr>\n","    <tr>\n","      <th>8691</th>\n","      <td>0.017183</td>\n","      <td>-1.082981</td>\n","      <td>1.717017</td>\n","      <td>-0.510772</td>\n","      <td>0.738608</td>\n","      <td>-0.738608</td>\n","      <td>1.922118</td>\n","      <td>-0.322665</td>\n","      <td>-1.501281</td>\n","      <td>0.158543</td>\n","      <td>...</td>\n","      <td>-0.179844</td>\n","      <td>-0.324093</td>\n","      <td>-0.312266</td>\n","      <td>-0.244957</td>\n","      <td>2.944609</td>\n","      <td>-0.695045</td>\n","      <td>-0.652529</td>\n","      <td>-0.017401</td>\n","      <td>-0.987289</td>\n","      <td>0.987289</td>\n","    </tr>\n","    <tr>\n","      <th>8692</th>\n","      <td>0.017183</td>\n","      <td>-1.082981</td>\n","      <td>1.717017</td>\n","      <td>-0.510772</td>\n","      <td>0.738608</td>\n","      <td>-0.738608</td>\n","      <td>-0.520181</td>\n","      <td>-0.322665</td>\n","      <td>0.665997</td>\n","      <td>0.158543</td>\n","      <td>...</td>\n","      <td>-0.179844</td>\n","      <td>-0.324093</td>\n","      <td>-0.312266</td>\n","      <td>-0.244957</td>\n","      <td>2.944609</td>\n","      <td>-0.695045</td>\n","      <td>-0.652529</td>\n","      <td>-0.017401</td>\n","      <td>-0.987289</td>\n","      <td>0.987289</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>6606 rows × 21 columns</p>\n","</div>"],"text/plain":["      CabinNum  HomePlanet_Earth  HomePlanet_Europa  HomePlanet_Mars  \\\n","0    -1.166962         -1.082981           1.717017        -0.510772   \n","1    -1.166962          0.923237          -0.582317        -0.510772   \n","2    -1.166962         -1.082981           1.717017        -0.510772   \n","3    -1.166962         -1.082981           1.717017        -0.510772   \n","4    -1.165015          0.923237          -0.582317        -0.510772   \n","...        ...               ...                ...              ...   \n","8688 -0.976097         -1.082981           1.717017        -0.510772   \n","8689  1.752501          0.923237          -0.582317        -0.510772   \n","8690  1.754448          0.923237          -0.582317        -0.510772   \n","8691  0.017183         -1.082981           1.717017        -0.510772   \n","8692  0.017183         -1.082981           1.717017        -0.510772   \n","\n","      CryoSleep_False  CryoSleep_True  Destination_55 Cancri e  \\\n","0            0.738608       -0.738608                -0.520181   \n","1            0.738608       -0.738608                -0.520181   \n","2            0.738608       -0.738608                -0.520181   \n","3            0.738608       -0.738608                -0.520181   \n","4            0.738608       -0.738608                -0.520181   \n","...               ...             ...                      ...   \n","8688         0.738608       -0.738608                 1.922118   \n","8689        -1.353693        1.353693                -0.520181   \n","8690         0.738608       -0.738608                -0.520181   \n","8691         0.738608       -0.738608                 1.922118   \n","8692         0.738608       -0.738608                -0.520181   \n","\n","      Destination_PSO J318.5-22  Destination_TRAPPIST-1e  VIP_False  ...  \\\n","0                     -0.322665                 0.665997   0.158543  ...   \n","1                     -0.322665                 0.665997   0.158543  ...   \n","2                     -0.322665                 0.665997  -6.306485  ...   \n","3                     -0.322665                 0.665997   0.158543  ...   \n","4                     -0.322665                 0.665997   0.158543  ...   \n","...                         ...                      ...        ...  ...   \n","8688                  -0.322665                -1.501281  -6.306485  ...   \n","8689                   3.098722                -1.501281   0.158543  ...   \n","8690                  -0.322665                 0.665997   0.158543  ...   \n","8691                  -0.322665                -1.501281   0.158543  ...   \n","8692                  -0.322665                 0.665997   0.158543  ...   \n","\n","      CabinDeck_A  CabinDeck_B  CabinDeck_C  CabinDeck_D  CabinDeck_E  \\\n","0       -0.179844     3.085072    -0.312266    -0.244957    -0.339552   \n","1       -0.179844    -0.324093    -0.312266    -0.244957    -0.339552   \n","2        5.559529    -0.324093    -0.312266    -0.244957    -0.339552   \n","3        5.559529    -0.324093    -0.312266    -0.244957    -0.339552   \n","4       -0.179844    -0.324093    -0.312266    -0.244957    -0.339552   \n","...           ...          ...          ...          ...          ...   \n","8688     5.559529    -0.324093    -0.312266    -0.244957    -0.339552   \n","8689    -0.179844    -0.324093    -0.312266    -0.244957    -0.339552   \n","8690    -0.179844    -0.324093    -0.312266    -0.244957    -0.339552   \n","8691    -0.179844    -0.324093    -0.312266    -0.244957     2.944609   \n","8692    -0.179844    -0.324093    -0.312266    -0.244957     2.944609   \n","\n","      CabinDeck_F  CabinDeck_G  CabinDeck_T  CabinSide_P  CabinSide_S  \n","0       -0.695045    -0.652529    -0.017401     1.012721    -1.012721  \n","1        1.438537    -0.652529    -0.017401    -0.987289     0.987289  \n","2       -0.695045    -0.652529    -0.017401    -0.987289     0.987289  \n","3       -0.695045    -0.652529    -0.017401    -0.987289     0.987289  \n","4        1.438537    -0.652529    -0.017401    -0.987289     0.987289  \n","...           ...          ...          ...          ...          ...  \n","8688    -0.695045    -0.652529    -0.017401     1.012721    -1.012721  \n","8689    -0.695045     1.532268    -0.017401    -0.987289     0.987289  \n","8690    -0.695045     1.532268    -0.017401    -0.987289     0.987289  \n","8691    -0.695045    -0.652529    -0.017401    -0.987289     0.987289  \n","8692    -0.695045    -0.652529    -0.017401    -0.987289     0.987289  \n","\n","[6606 rows x 21 columns]"]},"execution_count":169,"metadata":{},"output_type":"execute_result"}],"source":["X_scaled"]},{"cell_type":"code","execution_count":219,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":270,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["train dataset shape Counter({True: 3327, False: 3279})\n","train dataset shape Counter({True: 2994, False: 2951})\n"]}],"source":[]},{"cell_type":"code","execution_count":281,"metadata":{},"outputs":[{"ename":"TypeError","evalue":"metaclass conflict: the metaclass of a derived class must be a (non-strict) subclass of the metaclasses of all its bases","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2180\\3149409981.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimblearn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mimblearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimblearn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mTypeError\u001b[0m: metaclass conflict: the metaclass of a derived class must be a (non-strict) subclass of the metaclasses of all its bases"]}],"source":["class XGBClassifier:\n","    def __init__(self, name):\n","        self.name = name\n","        \n","    def validation():\n","        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=y)\n","        print('train dataset shape %s' % Counter(y))\n","        print('train dataset shape %s' % Counter(y_train))\n","        \n","    def sampler(X_train, y_train, sampler):\n","        X_res, y_res = sampler.fit_resample(X_train, y_train)\n","        print('Resampled dataset shape %s' % Counter(y_res))\n","        return X_res, y_res\n","\n","    def XGBoostClassifier_opt(X_train, X_test, y_train, y_test):\n","        def objective(trial):\n","          params = {\n","              'n_estimators': trial.suggest_int('n_estimators', 0, 1000), \n","              'max_depth': trial.suggest_int('max_depth', 1, 20), \n","              'min_child_weight': trial.suggest_int('min_child_weight', 1, 20), \n","              'subsample':trial.suggest_discrete_uniform('subsample', 0.5, 0.9, 0.1), \n","              'colsample_bytree': trial.suggest_discrete_uniform('colsample_bytree', 0.5, 0.9, 0.1),\n","              'verbose': 0\n","              }\n","          model = XGBClassifier(**params)\n","          model.fit(X_train, y_train)\n","          pred = model.predict(X_test)\n","          accuracy = accuracy_score(y_test, pred)\n","          return (1-accuracy)\n","\n","        study = optuna.create_study()\n","        study.optimize(objective, n_trials=300)\n","        return study"]},{"cell_type":"code","execution_count":276,"metadata":{},"outputs":[],"source":["def XGBoostClassifier(X_train, X_test, y_train, y_test, study):\n","  model = XGBClassifier(params=study.best_params,\n","                  dtrain=X_train,\n","                  num_boost_round=1000,\n","                  early_stopping_rounds=5,\n","                  evals=[(y_train, \"test\")])\n","  model = model.fit(X_train, y_train)\n","  y_pred = model.predict(X_test)\n","  \n","  print(\"accuracy scores\", accuracy_score(y_test, y_pred))\n","  metrics.roc_auc_score(y_test, y_pred)\n","  matrix = metrics.confusion_matrix(y_test, y_pred, normalize=\"all\", labels=[False, True])\n","  ax = sns.heatmap(matrix, annot=True, cmap=\"Blues\")\n","  \n","  fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)\n","  auc = metrics.auc(fpr, tpr)\n","  \n","  plt.plot(fpr, tpr, label='ROC curve (area = %.2f)'%auc)\n","  plt.legend()\n","  plt.xlabel('FPR: False positive rate')\n","  plt.ylabel('TPR: True positive rate')\n","  plt.grid()\n","  plt.show()"]},{"cell_type":"code","execution_count":277,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Resampled dataset shape Counter({False: 2951, True: 1385})\n"]}],"source":["X_res, y_res = sampler(X_train, y_train, EditedNearestNeighbours())"]},{"cell_type":"code","execution_count":278,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:19:31,023]\u001b[0m A new study created in memory with name: no-name-255b01b1-531f-46f1-87f2-9a519fd6b8a2\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:19:31] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:19:31] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:19:31,617]\u001b[0m Trial 0 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 243, 'max_depth': 9, 'min_child_weight': 13, 'subsample': 0.7, 'colsample_bytree': 0.9}. Best is trial 0 with value: 0.25113464447806355.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:19:31] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:19:31] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:19:32,479]\u001b[0m Trial 1 finished with value: 0.2571860816944024 and parameters: {'n_estimators': 228, 'max_depth': 20, 'min_child_weight': 4, 'subsample': 0.9, 'colsample_bytree': 0.9}. Best is trial 0 with value: 0.25113464447806355.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","\u001b[32m[I 2022-12-11 00:19:32,499]\u001b[0m Trial 2 finished with value: 0.5037821482602118 and parameters: {'n_estimators': 0, 'max_depth': 20, 'min_child_weight': 5, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 0 with value: 0.25113464447806355.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:19:32] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:19:32] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:19:32] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:19:33,378]\u001b[0m Trial 3 finished with value: 0.25567322239031776 and parameters: {'n_estimators': 703, 'max_depth': 1, 'min_child_weight': 10, 'subsample': 0.7, 'colsample_bytree': 0.9}. Best is trial 0 with value: 0.25113464447806355.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:19:33] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:19:33] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:19:35,061]\u001b[0m Trial 4 finished with value: 0.2617246596066566 and parameters: {'n_estimators': 931, 'max_depth': 3, 'min_child_weight': 1, 'subsample': 0.5, 'colsample_bytree': 0.7}. Best is trial 0 with value: 0.25113464447806355.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:19:35] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:19:35] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:19:35,288]\u001b[0m Trial 5 finished with value: 0.254160363086233 and parameters: {'n_estimators': 102, 'max_depth': 12, 'min_child_weight': 13, 'subsample': 0.5, 'colsample_bytree': 0.7}. Best is trial 0 with value: 0.25113464447806355.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:19:35] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:19:35] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:19:35,556]\u001b[0m Trial 6 finished with value: 0.26475037821482605 and parameters: {'n_estimators': 175, 'max_depth': 2, 'min_child_weight': 10, 'subsample': 0.5, 'colsample_bytree': 0.5}. Best is trial 0 with value: 0.25113464447806355.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","\u001b[32m[I 2022-12-11 00:19:35,754]\u001b[0m Trial 7 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 75, 'max_depth': 16, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.5}. Best is trial 0 with value: 0.25113464447806355.\u001b[0m\n"]},{"name":"stdout","output_type":"stream","text":["[00:19:35] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:19:35] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:19:35] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:19:35] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:19:36,420]\u001b[0m Trial 8 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 241, 'max_depth': 9, 'min_child_weight': 7, 'subsample': 0.7, 'colsample_bytree': 0.9}. Best is trial 0 with value: 0.25113464447806355.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:19:36] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:19:36] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:19:37,833]\u001b[0m Trial 9 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 767, 'max_depth': 10, 'min_child_weight': 12, 'subsample': 0.6, 'colsample_bytree': 0.6}. Best is trial 0 with value: 0.25113464447806355.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:19:37] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:19:37] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:19:38,764]\u001b[0m Trial 10 finished with value: 0.254160363086233 and parameters: {'n_estimators': 449, 'max_depth': 6, 'min_child_weight': 20, 'subsample': 0.8, 'colsample_bytree': 0.8}. Best is trial 0 with value: 0.25113464447806355.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:19:38] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:19:38] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:19:39,564]\u001b[0m Trial 11 finished with value: 0.24810892586989408 and parameters: {'n_estimators': 393, 'max_depth': 15, 'min_child_weight': 18, 'subsample': 0.9, 'colsample_bytree': 0.5}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:19:39] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:19:39] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:19:40,688]\u001b[0m Trial 12 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 444, 'max_depth': 14, 'min_child_weight': 16, 'subsample': 0.8, 'colsample_bytree': 0.6}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:19:40] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:19:40] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:19:41,665]\u001b[0m Trial 13 finished with value: 0.24810892586989408 and parameters: {'n_estimators': 460, 'max_depth': 15, 'min_child_weight': 17, 'subsample': 0.8, 'colsample_bytree': 0.6}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:19:41] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:19:41] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:19:42,794]\u001b[0m Trial 14 finished with value: 0.2526475037821483 and parameters: {'n_estimators': 582, 'max_depth': 16, 'min_child_weight': 20, 'subsample': 0.8, 'colsample_bytree': 0.6}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:19:42] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:19:42] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:19:43,524]\u001b[0m Trial 15 finished with value: 0.24810892586989408 and parameters: {'n_estimators': 347, 'max_depth': 16, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.5}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:19:43] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:19:43] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:19:44,633]\u001b[0m Trial 16 finished with value: 0.2526475037821483 and parameters: {'n_estimators': 573, 'max_depth': 14, 'min_child_weight': 15, 'subsample': 0.8, 'colsample_bytree': 0.6}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:19:44] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:19:44] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:19:45,302]\u001b[0m Trial 17 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 354, 'max_depth': 18, 'min_child_weight': 19, 'subsample': 0.9, 'colsample_bytree': 0.5}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:19:45] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:19:45] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:19:46,475]\u001b[0m Trial 18 finished with value: 0.254160363086233 and parameters: {'n_estimators': 571, 'max_depth': 18, 'min_child_weight': 15, 'subsample': 0.8, 'colsample_bytree': 0.6}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:19:46] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:19:46] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:19:47,199]\u001b[0m Trial 19 finished with value: 0.254160363086233 and parameters: {'n_estimators': 339, 'max_depth': 12, 'min_child_weight': 17, 'subsample': 0.6, 'colsample_bytree': 0.8}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:19:47] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:19:47] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:19:49,010]\u001b[0m Trial 20 finished with value: 0.254160363086233 and parameters: {'n_estimators': 724, 'max_depth': 7, 'min_child_weight': 8, 'subsample': 0.9, 'colsample_bytree': 0.5}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:19:49] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:19:49] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:19:49,710]\u001b[0m Trial 21 finished with value: 0.24810892586989408 and parameters: {'n_estimators': 359, 'max_depth': 15, 'min_child_weight': 18, 'subsample': 0.9, 'colsample_bytree': 0.5}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:19:49] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:19:49] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:19:50,368]\u001b[0m Trial 22 finished with value: 0.24810892586989408 and parameters: {'n_estimators': 316, 'max_depth': 12, 'min_child_weight': 15, 'subsample': 0.9, 'colsample_bytree': 0.5}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:19:50] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:19:50] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:19:51,254]\u001b[0m Trial 23 finished with value: 0.254160363086233 and parameters: {'n_estimators': 450, 'max_depth': 18, 'min_child_weight': 18, 'subsample': 0.8, 'colsample_bytree': 0.6}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:19:51] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:19:51] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:19:51,897]\u001b[0m Trial 24 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 331, 'max_depth': 17, 'min_child_weight': 19, 'subsample': 0.9, 'colsample_bytree': 0.5}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:19:51] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:19:51] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:19:52,487]\u001b[0m Trial 25 finished with value: 0.24810892586989408 and parameters: {'n_estimators': 276, 'max_depth': 12, 'min_child_weight': 14, 'subsample': 0.9, 'colsample_bytree': 0.5}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:19:52] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:19:52] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:19:52,972]\u001b[0m Trial 26 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 219, 'max_depth': 12, 'min_child_weight': 14, 'subsample': 0.9, 'colsample_bytree': 0.5}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:19:53] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:19:53] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:19:54,002]\u001b[0m Trial 27 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 503, 'max_depth': 14, 'min_child_weight': 12, 'subsample': 0.8, 'colsample_bytree': 0.6}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:19:54] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:19:54] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:19:55,218]\u001b[0m Trial 28 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 642, 'max_depth': 11, 'min_child_weight': 15, 'subsample': 0.8, 'colsample_bytree': 0.6}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:19:55] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:19:55] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:19:55,552]\u001b[0m Trial 29 finished with value: 0.24810892586989408 and parameters: {'n_estimators': 142, 'max_depth': 7, 'min_child_weight': 13, 'subsample': 0.6, 'colsample_bytree': 0.5}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:19:55] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:19:55] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:19:55,996]\u001b[0m Trial 30 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 163, 'max_depth': 7, 'min_child_weight': 12, 'subsample': 0.6, 'colsample_bytree': 0.8}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:19:56] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:19:56] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:19:56,620]\u001b[0m Trial 31 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 283, 'max_depth': 13, 'min_child_weight': 16, 'subsample': 0.9, 'colsample_bytree': 0.5}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:19:56] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:19:56] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:19:57,170]\u001b[0m Trial 32 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 281, 'max_depth': 5, 'min_child_weight': 13, 'subsample': 0.6, 'colsample_bytree': 0.5}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","\u001b[32m[I 2022-12-11 00:19:57,411]\u001b[0m Trial 33 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 100, 'max_depth': 9, 'min_child_weight': 14, 'subsample': 0.6, 'colsample_bytree': 0.5}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n"]},{"name":"stdout","output_type":"stream","text":["[00:19:57] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:19:57] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","\u001b[32m[I 2022-12-11 00:19:57,485]\u001b[0m Trial 34 finished with value: 0.2586989409984871 and parameters: {'n_estimators': 8, 'max_depth': 20, 'min_child_weight': 18, 'subsample': 0.7, 'colsample_bytree': 0.6}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:19:57] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:19:57] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n","[00:19:57] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:19:57] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:19:57,936]\u001b[0m Trial 35 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 173, 'max_depth': 4, 'min_child_weight': 9, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:19:57] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:19:57] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:19:58,836]\u001b[0m Trial 36 finished with value: 0.24810892586989408 and parameters: {'n_estimators': 426, 'max_depth': 15, 'min_child_weight': 18, 'subsample': 0.9, 'colsample_bytree': 0.6}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:19:58] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:19:58] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:00,002]\u001b[0m Trial 37 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 504, 'max_depth': 16, 'min_child_weight': 16, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:00] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:00] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:01,279]\u001b[0m Trial 38 finished with value: 0.25567322239031776 and parameters: {'n_estimators': 408, 'max_depth': 19, 'min_child_weight': 1, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:01] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:01] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:01,897]\u001b[0m Trial 39 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 276, 'max_depth': 11, 'min_child_weight': 11, 'subsample': 0.9, 'colsample_bytree': 0.5}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:01] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:01] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:03,506]\u001b[0m Trial 40 finished with value: 0.2526475037821483 and parameters: {'n_estimators': 953, 'max_depth': 13, 'min_child_weight': 19, 'subsample': 0.9, 'colsample_bytree': 0.5}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:03] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:03] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:04,120]\u001b[0m Trial 41 finished with value: 0.25567322239031776 and parameters: {'n_estimators': 351, 'max_depth': 15, 'min_child_weight': 17, 'subsample': 0.5, 'colsample_bytree': 0.5}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:04] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:04] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:04,960]\u001b[0m Trial 42 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 401, 'max_depth': 13, 'min_child_weight': 14, 'subsample': 0.9, 'colsample_bytree': 0.6}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:05] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:05] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:05,696]\u001b[0m Trial 43 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 387, 'max_depth': 15, 'min_child_weight': 17, 'subsample': 0.8, 'colsample_bytree': 0.5}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:05] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:05] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:06,190]\u001b[0m Trial 44 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 210, 'max_depth': 8, 'min_child_weight': 13, 'subsample': 0.9, 'colsample_bytree': 0.5}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:06] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:06] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:07,068]\u001b[0m Trial 45 finished with value: 0.25567322239031776 and parameters: {'n_estimators': 484, 'max_depth': 17, 'min_child_weight': 16, 'subsample': 0.8, 'colsample_bytree': 0.5}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:07] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:07] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:08,494]\u001b[0m Trial 46 finished with value: 0.24810892586989408 and parameters: {'n_estimators': 857, 'max_depth': 11, 'min_child_weight': 14, 'subsample': 0.7, 'colsample_bytree': 0.5}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:08] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:08] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:10,071]\u001b[0m Trial 47 finished with value: 0.254160363086233 and parameters: {'n_estimators': 892, 'max_depth': 10, 'min_child_weight': 11, 'subsample': 0.7, 'colsample_bytree': 0.5}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","\u001b[32m[I 2022-12-11 00:20:10,269]\u001b[0m Trial 48 finished with value: 0.25567322239031776 and parameters: {'n_estimators': 132, 'max_depth': 1, 'min_child_weight': 3, 'subsample': 0.5, 'colsample_bytree': 0.6}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:10] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:10] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n","[00:20:10] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:10] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","\u001b[32m[I 2022-12-11 00:20:11,167]\u001b[0m Trial 49 finished with value: 0.254160363086233 and parameters: {'n_estimators': 526, 'max_depth': 17, 'min_child_weight': 20, 'subsample': 0.8, 'colsample_bytree': 0.6}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","\u001b[32m[I 2022-12-11 00:20:11,296]\u001b[0m Trial 50 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 41, 'max_depth': 16, 'min_child_weight': 18, 'subsample': 0.8, 'colsample_bytree': 0.6}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:11] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:11] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n","[00:20:11] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:11] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:11,848]\u001b[0m Trial 51 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 295, 'max_depth': 12, 'min_child_weight': 14, 'subsample': 0.6, 'colsample_bytree': 0.5}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:11] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:11] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:12,358]\u001b[0m Trial 52 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 250, 'max_depth': 14, 'min_child_weight': 15, 'subsample': 0.9, 'colsample_bytree': 0.5}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:12] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:12] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:12,762]\u001b[0m Trial 53 finished with value: 0.2526475037821483 and parameters: {'n_estimators': 196, 'max_depth': 10, 'min_child_weight': 16, 'subsample': 0.9, 'colsample_bytree': 0.5}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:12] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:12] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:13,991]\u001b[0m Trial 54 finished with value: 0.25567322239031776 and parameters: {'n_estimators': 832, 'max_depth': 3, 'min_child_weight': 17, 'subsample': 0.7, 'colsample_bytree': 0.5}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:14] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:14] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:14,718]\u001b[0m Trial 55 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 371, 'max_depth': 9, 'min_child_weight': 12, 'subsample': 0.7, 'colsample_bytree': 0.5}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:14] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:14] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:15,840]\u001b[0m Trial 56 finished with value: 0.2526475037821483 and parameters: {'n_estimators': 635, 'max_depth': 15, 'min_child_weight': 18, 'subsample': 0.9, 'colsample_bytree': 0.6}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:15] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:15] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:16,474]\u001b[0m Trial 57 finished with value: 0.2526475037821483 and parameters: {'n_estimators': 305, 'max_depth': 11, 'min_child_weight': 13, 'subsample': 0.8, 'colsample_bytree': 0.5}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:16] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:16] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:17,366]\u001b[0m Trial 58 finished with value: 0.2526475037821483 and parameters: {'n_estimators': 543, 'max_depth': 8, 'min_child_weight': 15, 'subsample': 0.5, 'colsample_bytree': 0.5}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:17] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:17] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:18,132]\u001b[0m Trial 59 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 420, 'max_depth': 13, 'min_child_weight': 19, 'subsample': 0.9, 'colsample_bytree': 0.6}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:18] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:18] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:18,647]\u001b[0m Trial 60 finished with value: 0.24810892586989408 and parameters: {'n_estimators': 228, 'max_depth': 14, 'min_child_weight': 15, 'subsample': 0.9, 'colsample_bytree': 0.9}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:18] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:18] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:19,223]\u001b[0m Trial 61 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 256, 'max_depth': 14, 'min_child_weight': 15, 'subsample': 0.9, 'colsample_bytree': 0.9}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:19] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:19] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:19,917]\u001b[0m Trial 62 finished with value: 0.254160363086233 and parameters: {'n_estimators': 310, 'max_depth': 12, 'min_child_weight': 14, 'subsample': 0.9, 'colsample_bytree': 0.8}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:19] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:19] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:20,668]\u001b[0m Trial 63 finished with value: 0.2526475037821483 and parameters: {'n_estimators': 458, 'max_depth': 17, 'min_child_weight': 20, 'subsample': 0.6, 'colsample_bytree': 0.5}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:20] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:20] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:21,335]\u001b[0m Trial 64 finished with value: 0.24659606656580935 and parameters: {'n_estimators': 333, 'max_depth': 16, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:21] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:21] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:21,666]\u001b[0m Trial 65 finished with value: 0.2526475037821483 and parameters: {'n_estimators': 130, 'max_depth': 16, 'min_child_weight': 16, 'subsample': 0.9, 'colsample_bytree': 0.8}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:21] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:21] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:22,225]\u001b[0m Trial 66 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 237, 'max_depth': 11, 'min_child_weight': 13, 'subsample': 0.9, 'colsample_bytree': 0.9}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","\u001b[32m[I 2022-12-11 00:20:22,462]\u001b[0m Trial 67 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 81, 'max_depth': 13, 'min_child_weight': 12, 'subsample': 0.9, 'colsample_bytree': 0.8}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:22] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:22] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:22] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:22] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:23,283]\u001b[0m Trial 68 finished with value: 0.2526475037821483 and parameters: {'n_estimators': 443, 'max_depth': 19, 'min_child_weight': 19, 'subsample': 0.8, 'colsample_bytree': 0.7}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:23] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:23] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:23,938]\u001b[0m Trial 69 finished with value: 0.24659606656580935 and parameters: {'n_estimators': 332, 'max_depth': 16, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:23] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:23] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:24,653]\u001b[0m Trial 70 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 363, 'max_depth': 15, 'min_child_weight': 18, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:24] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:24] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:26,300]\u001b[0m Trial 71 finished with value: 0.25567322239031776 and parameters: {'n_estimators': 991, 'max_depth': 14, 'min_child_weight': 17, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:26] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:26] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:27,004]\u001b[0m Trial 72 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 329, 'max_depth': 18, 'min_child_weight': 15, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:27] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:27] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:27,812]\u001b[0m Trial 73 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 420, 'max_depth': 15, 'min_child_weight': 18, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:27] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:27] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:28,729]\u001b[0m Trial 74 finished with value: 0.24810892586989408 and parameters: {'n_estimators': 479, 'max_depth': 16, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:28] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:28] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:29,405]\u001b[0m Trial 75 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 331, 'max_depth': 16, 'min_child_weight': 16, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:29] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:29] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:30,148]\u001b[0m Trial 76 finished with value: 0.2526475037821483 and parameters: {'n_estimators': 368, 'max_depth': 17, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.8}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:30] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:30] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:30,576]\u001b[0m Trial 77 finished with value: 0.24810892586989408 and parameters: {'n_estimators': 184, 'max_depth': 14, 'min_child_weight': 16, 'subsample': 0.8, 'colsample_bytree': 0.9}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:30] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:30] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:30,997]\u001b[0m Trial 78 finished with value: 0.2526475037821483 and parameters: {'n_estimators': 189, 'max_depth': 12, 'min_child_weight': 14, 'subsample': 0.6, 'colsample_bytree': 0.9}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:31] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:31] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:32,375]\u001b[0m Trial 79 finished with value: 0.25567322239031776 and parameters: {'n_estimators': 787, 'max_depth': 18, 'min_child_weight': 17, 'subsample': 0.8, 'colsample_bytree': 0.6}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:32] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:32] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:33,011]\u001b[0m Trial 80 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 271, 'max_depth': 13, 'min_child_weight': 19, 'subsample': 0.9, 'colsample_bytree': 0.8}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:33] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:33] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:33,818]\u001b[0m Trial 81 finished with value: 0.24659606656580935 and parameters: {'n_estimators': 389, 'max_depth': 15, 'min_child_weight': 18, 'subsample': 0.9, 'colsample_bytree': 0.6}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:33] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:33] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:35,190]\u001b[0m Trial 82 finished with value: 0.254160363086233 and parameters: {'n_estimators': 485, 'max_depth': 16, 'min_child_weight': 6, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:35] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:35] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:35,913]\u001b[0m Trial 83 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 382, 'max_depth': 15, 'min_child_weight': 19, 'subsample': 0.9, 'colsample_bytree': 0.5}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:35] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:35] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:36,243]\u001b[0m Trial 84 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 150, 'max_depth': 17, 'min_child_weight': 16, 'subsample': 0.9, 'colsample_bytree': 0.5}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:36] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:36] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:36,900]\u001b[0m Trial 85 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 312, 'max_depth': 14, 'min_child_weight': 15, 'subsample': 0.9, 'colsample_bytree': 0.5}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:36] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:36] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:37,496]\u001b[0m Trial 86 finished with value: 0.2571860816944024 and parameters: {'n_estimators': 233, 'max_depth': 13, 'min_child_weight': 9, 'subsample': 0.9, 'colsample_bytree': 0.6}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:37] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:37] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:38,151]\u001b[0m Trial 87 finished with value: 0.24659606656580935 and parameters: {'n_estimators': 340, 'max_depth': 16, 'min_child_weight': 18, 'subsample': 0.9, 'colsample_bytree': 0.6}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:38] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:38] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:38,992]\u001b[0m Trial 88 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 433, 'max_depth': 9, 'min_child_weight': 18, 'subsample': 0.6, 'colsample_bytree': 0.6}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:39] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:39] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:39,899]\u001b[0m Trial 89 finished with value: 0.2526475037821483 and parameters: {'n_estimators': 398, 'max_depth': 15, 'min_child_weight': 20, 'subsample': 0.9, 'colsample_bytree': 0.6}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:39] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:39] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:40,701]\u001b[0m Trial 90 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 349, 'max_depth': 17, 'min_child_weight': 18, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:40] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:40] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:41,335]\u001b[0m Trial 91 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 328, 'max_depth': 16, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.6}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:41] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:41] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:41,918]\u001b[0m Trial 92 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 298, 'max_depth': 6, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.6}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:41] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:41] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:42,624]\u001b[0m Trial 93 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 384, 'max_depth': 19, 'min_child_weight': 19, 'subsample': 0.9, 'colsample_bytree': 0.5}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:42] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:42] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:43,159]\u001b[0m Trial 94 finished with value: 0.24810892586989408 and parameters: {'n_estimators': 276, 'max_depth': 15, 'min_child_weight': 18, 'subsample': 0.9, 'colsample_bytree': 0.5}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:43] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:43] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:43,675]\u001b[0m Trial 95 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 261, 'max_depth': 15, 'min_child_weight': 19, 'subsample': 0.9, 'colsample_bytree': 0.6}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:43] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:43] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:44,547]\u001b[0m Trial 96 finished with value: 0.2526475037821483 and parameters: {'n_estimators': 476, 'max_depth': 16, 'min_child_weight': 18, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:44] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:44] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:45,186]\u001b[0m Trial 97 finished with value: 0.24810892586989408 and parameters: {'n_estimators': 349, 'max_depth': 18, 'min_child_weight': 18, 'subsample': 0.8, 'colsample_bytree': 0.5}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:45] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:45] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:46,168]\u001b[0m Trial 98 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 522, 'max_depth': 18, 'min_child_weight': 16, 'subsample': 0.8, 'colsample_bytree': 0.7}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:46] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:46] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:46,876]\u001b[0m Trial 99 finished with value: 0.24810892586989408 and parameters: {'n_estimators': 353, 'max_depth': 18, 'min_child_weight': 17, 'subsample': 0.8, 'colsample_bytree': 0.7}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:46] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:46] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:47,335]\u001b[0m Trial 100 finished with value: 0.24810892586989408 and parameters: {'n_estimators': 209, 'max_depth': 16, 'min_child_weight': 16, 'subsample': 0.8, 'colsample_bytree': 0.7}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:47] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:47] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:47,696]\u001b[0m Trial 101 finished with value: 0.24810892586989408 and parameters: {'n_estimators': 161, 'max_depth': 10, 'min_child_weight': 14, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:47] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:47] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:48,140]\u001b[0m Trial 102 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 209, 'max_depth': 10, 'min_child_weight': 14, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:48] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:48] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:48,583]\u001b[0m Trial 103 finished with value: 0.24810892586989408 and parameters: {'n_estimators': 185, 'max_depth': 14, 'min_child_weight': 15, 'subsample': 0.9, 'colsample_bytree': 0.9}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:48] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:48] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:48,947]\u001b[0m Trial 104 finished with value: 0.2526475037821483 and parameters: {'n_estimators': 132, 'max_depth': 11, 'min_child_weight': 13, 'subsample': 0.9, 'colsample_bytree': 0.9}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:48] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:48] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:49,801]\u001b[0m Trial 105 finished with value: 0.25567322239031776 and parameters: {'n_estimators': 404, 'max_depth': 16, 'min_child_weight': 11, 'subsample': 0.9, 'colsample_bytree': 0.6}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:49] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:49] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:50,600]\u001b[0m Trial 106 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 426, 'max_depth': 15, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.6}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:50] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:50] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:51,598]\u001b[0m Trial 107 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 587, 'max_depth': 17, 'min_child_weight': 20, 'subsample': 0.9, 'colsample_bytree': 0.5}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:51] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:51] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:52,519]\u001b[0m Trial 108 finished with value: 0.24810892586989408 and parameters: {'n_estimators': 473, 'max_depth': 16, 'min_child_weight': 16, 'subsample': 0.7, 'colsample_bytree': 0.8}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:52] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:52] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:53,446]\u001b[0m Trial 109 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 463, 'max_depth': 14, 'min_child_weight': 16, 'subsample': 0.9, 'colsample_bytree': 0.8}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:53] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:53] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:54,099]\u001b[0m Trial 110 finished with value: 0.2526475037821483 and parameters: {'n_estimators': 317, 'max_depth': 12, 'min_child_weight': 15, 'subsample': 0.7, 'colsample_bytree': 0.8}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:54] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:54] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:54,739]\u001b[0m Trial 111 finished with value: 0.24810892586989408 and parameters: {'n_estimators': 285, 'max_depth': 14, 'min_child_weight': 15, 'subsample': 0.9, 'colsample_bytree': 0.9}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:54] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:54] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:55,276]\u001b[0m Trial 112 finished with value: 0.24659606656580935 and parameters: {'n_estimators': 241, 'max_depth': 14, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.9}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:55] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:55] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:55,916]\u001b[0m Trial 113 finished with value: 0.2450832072617246 and parameters: {'n_estimators': 297, 'max_depth': 13, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.9}. Best is trial 113 with value: 0.2450832072617246.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:55] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:55] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:56,460]\u001b[0m Trial 114 finished with value: 0.24810892586989408 and parameters: {'n_estimators': 248, 'max_depth': 13, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.9}. Best is trial 113 with value: 0.2450832072617246.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:56] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:56] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:57,011]\u001b[0m Trial 115 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 252, 'max_depth': 13, 'min_child_weight': 18, 'subsample': 0.9, 'colsample_bytree': 0.9}. Best is trial 113 with value: 0.2450832072617246.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:57] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:57] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:57,288]\u001b[0m Trial 116 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 107, 'max_depth': 14, 'min_child_weight': 16, 'subsample': 0.9, 'colsample_bytree': 0.9}. Best is trial 113 with value: 0.2450832072617246.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:57] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:57] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:57,753]\u001b[0m Trial 117 finished with value: 0.254160363086233 and parameters: {'n_estimators': 224, 'max_depth': 14, 'min_child_weight': 17, 'subsample': 0.5, 'colsample_bytree': 0.9}. Best is trial 113 with value: 0.2450832072617246.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:57] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:57] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:58,113]\u001b[0m Trial 118 finished with value: 0.254160363086233 and parameters: {'n_estimators': 166, 'max_depth': 19, 'min_child_weight': 18, 'subsample': 0.8, 'colsample_bytree': 0.7}. Best is trial 113 with value: 0.2450832072617246.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:58] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:58] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:58,628]\u001b[0m Trial 119 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 267, 'max_depth': 12, 'min_child_weight': 18, 'subsample': 0.8, 'colsample_bytree': 0.5}. Best is trial 113 with value: 0.2450832072617246.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:58] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:58] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:59,160]\u001b[0m Trial 120 finished with value: 0.24810892586989408 and parameters: {'n_estimators': 291, 'max_depth': 11, 'min_child_weight': 19, 'subsample': 0.6, 'colsample_bytree': 0.5}. Best is trial 113 with value: 0.2450832072617246.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:59] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:59] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:20:59,905]\u001b[0m Trial 121 finished with value: 0.24810892586989408 and parameters: {'n_estimators': 343, 'max_depth': 15, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.9}. Best is trial 113 with value: 0.2450832072617246.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:20:59] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:20:59] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:00,526]\u001b[0m Trial 122 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 293, 'max_depth': 15, 'min_child_weight': 19, 'subsample': 0.9, 'colsample_bytree': 0.9}. Best is trial 113 with value: 0.2450832072617246.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:00] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:00] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:00,983]\u001b[0m Trial 123 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 210, 'max_depth': 13, 'min_child_weight': 16, 'subsample': 0.8, 'colsample_bytree': 0.7}. Best is trial 113 with value: 0.2450832072617246.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:01] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:01] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:01,626]\u001b[0m Trial 124 finished with value: 0.2526475037821483 and parameters: {'n_estimators': 318, 'max_depth': 12, 'min_child_weight': 16, 'subsample': 0.8, 'colsample_bytree': 0.7}. Best is trial 113 with value: 0.2450832072617246.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:01] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:01] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:02,340]\u001b[0m Trial 125 finished with value: 0.2450832072617246 and parameters: {'n_estimators': 362, 'max_depth': 8, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 113 with value: 0.2450832072617246.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:02] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:02] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:03,086]\u001b[0m Trial 126 finished with value: 0.2526475037821483 and parameters: {'n_estimators': 367, 'max_depth': 6, 'min_child_weight': 17, 'subsample': 0.8, 'colsample_bytree': 0.7}. Best is trial 113 with value: 0.2450832072617246.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:03] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:03] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:03,802]\u001b[0m Trial 127 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 355, 'max_depth': 7, 'min_child_weight': 18, 'subsample': 0.9, 'colsample_bytree': 0.8}. Best is trial 113 with value: 0.2450832072617246.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:03] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:03] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:04,551]\u001b[0m Trial 128 finished with value: 0.24810892586989408 and parameters: {'n_estimators': 391, 'max_depth': 15, 'min_child_weight': 18, 'subsample': 0.9, 'colsample_bytree': 0.5}. Best is trial 113 with value: 0.2450832072617246.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:04] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:04] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:05,864]\u001b[0m Trial 129 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 687, 'max_depth': 15, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.9}. Best is trial 113 with value: 0.2450832072617246.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:05] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:05] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:06,694]\u001b[0m Trial 130 finished with value: 0.24810892586989408 and parameters: {'n_estimators': 383, 'max_depth': 16, 'min_child_weight': 18, 'subsample': 0.9, 'colsample_bytree': 0.9}. Best is trial 113 with value: 0.2450832072617246.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:06] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:06] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:07,087]\u001b[0m Trial 131 finished with value: 0.2526475037821483 and parameters: {'n_estimators': 152, 'max_depth': 8, 'min_child_weight': 13, 'subsample': 0.9, 'colsample_bytree': 0.9}. Best is trial 113 with value: 0.2450832072617246.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:07] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:07] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:07,552]\u001b[0m Trial 132 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 238, 'max_depth': 5, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.5}. Best is trial 113 with value: 0.2450832072617246.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:07] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:07] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:07,993]\u001b[0m Trial 133 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 198, 'max_depth': 7, 'min_child_weight': 14, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 113 with value: 0.2450832072617246.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:08] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:08] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:08,270]\u001b[0m Trial 134 finished with value: 0.25567322239031776 and parameters: {'n_estimators': 109, 'max_depth': 8, 'min_child_weight': 15, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 113 with value: 0.2450832072617246.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:08] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:08] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:08,971]\u001b[0m Trial 135 finished with value: 0.24810892586989408 and parameters: {'n_estimators': 333, 'max_depth': 20, 'min_child_weight': 15, 'subsample': 0.7, 'colsample_bytree': 0.9}. Best is trial 113 with value: 0.2450832072617246.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","\u001b[32m[I 2022-12-11 00:21:09,132]\u001b[0m Trial 136 finished with value: 0.24357034795764 and parameters: {'n_estimators': 49, 'max_depth': 9, 'min_child_weight': 16, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:09] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:09] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n","[00:21:09] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:09] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:09,236]\u001b[0m Trial 137 finished with value: 0.24810892586989408 and parameters: {'n_estimators': 17, 'max_depth': 9, 'min_child_weight': 15, 'subsample': 0.9, 'colsample_bytree': 0.9}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","\u001b[32m[I 2022-12-11 00:21:09,461]\u001b[0m Trial 138 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 79, 'max_depth': 8, 'min_child_weight': 16, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:09] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:09] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","\u001b[32m[I 2022-12-11 00:21:09,588]\u001b[0m Trial 139 finished with value: 0.24659606656580935 and parameters: {'n_estimators': 33, 'max_depth': 10, 'min_child_weight': 16, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:09] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:09] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n","[00:21:09] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:09] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:10,343]\u001b[0m Trial 140 finished with value: 0.24659606656580935 and parameters: {'n_estimators': 348, 'max_depth': 18, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","\u001b[32m[I 2022-12-11 00:21:10,508]\u001b[0m Trial 141 finished with value: 0.24659606656580935 and parameters: {'n_estimators': 50, 'max_depth': 17, 'min_child_weight': 16, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:10] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:10] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n","[00:21:10] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:10] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:10,648]\u001b[0m Trial 142 finished with value: 0.24659606656580935 and parameters: {'n_estimators': 36, 'max_depth': 17, 'min_child_weight': 16, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","\u001b[32m[I 2022-12-11 00:21:10,800]\u001b[0m Trial 143 finished with value: 0.24810892586989408 and parameters: {'n_estimators': 43, 'max_depth': 17, 'min_child_weight': 16, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:10] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:10] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n","[00:21:10] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:10] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:10,923]\u001b[0m Trial 144 finished with value: 0.254160363086233 and parameters: {'n_estimators': 30, 'max_depth': 10, 'min_child_weight': 17, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","\u001b[32m[I 2022-12-11 00:21:11,070]\u001b[0m Trial 145 finished with value: 0.2526475037821483 and parameters: {'n_estimators': 46, 'max_depth': 10, 'min_child_weight': 19, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:10] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:10] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n","[00:21:11] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:11] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:11,265]\u001b[0m Trial 146 finished with value: 0.24659606656580935 and parameters: {'n_estimators': 69, 'max_depth': 9, 'min_child_weight': 16, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","\u001b[32m[I 2022-12-11 00:21:11,426]\u001b[0m Trial 147 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 51, 'max_depth': 9, 'min_child_weight': 16, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:11] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:11] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n","[00:21:11] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:11] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:11,608]\u001b[0m Trial 148 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 62, 'max_depth': 17, 'min_child_weight': 16, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","\u001b[32m[I 2022-12-11 00:21:11,683]\u001b[0m Trial 149 finished with value: 0.2677760968229954 and parameters: {'n_estimators': 6, 'max_depth': 9, 'min_child_weight': 17, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","\u001b[32m[I 2022-12-11 00:21:11,870]\u001b[0m Trial 150 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 71, 'max_depth': 17, 'min_child_weight': 17, 'subsample': 0.6, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:11] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:11] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n","[00:21:11] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:11] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","\u001b[32m[I 2022-12-11 00:21:11,984]\u001b[0m Trial 151 finished with value: 0.24659606656580935 and parameters: {'n_estimators': 25, 'max_depth': 18, 'min_child_weight': 16, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","\u001b[32m[I 2022-12-11 00:21:12,092]\u001b[0m Trial 152 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 24, 'max_depth': 18, 'min_child_weight': 17, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:11] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:11] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n","[00:21:12] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:12] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:12] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:12] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:12,339]\u001b[0m Trial 153 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 101, 'max_depth': 8, 'min_child_weight': 16, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","\u001b[32m[I 2022-12-11 00:21:12,406]\u001b[0m Trial 154 finished with value: 0.2708018154311649 and parameters: {'n_estimators': 3, 'max_depth': 16, 'min_child_weight': 17, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:12] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:12] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n","[00:21:12] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:12] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:12,581]\u001b[0m Trial 155 finished with value: 0.24659606656580935 and parameters: {'n_estimators': 57, 'max_depth': 17, 'min_child_weight': 15, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","\u001b[32m[I 2022-12-11 00:21:12,689]\u001b[0m Trial 156 finished with value: 0.24659606656580935 and parameters: {'n_estimators': 24, 'max_depth': 18, 'min_child_weight': 15, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","\u001b[32m[I 2022-12-11 00:21:12,817]\u001b[0m Trial 157 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 34, 'max_depth': 19, 'min_child_weight': 16, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:12] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:12] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n","[00:21:12] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:12] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","\u001b[32m[I 2022-12-11 00:21:13,045]\u001b[0m Trial 158 finished with value: 0.2526475037821483 and parameters: {'n_estimators': 82, 'max_depth': 18, 'min_child_weight': 16, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:12] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:12] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","\u001b[32m[I 2022-12-11 00:21:13,235]\u001b[0m Trial 159 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 67, 'max_depth': 18, 'min_child_weight': 15, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:13] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:13] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n","[00:21:13] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:13] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","\u001b[32m[I 2022-12-11 00:21:13,419]\u001b[0m Trial 160 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 53, 'max_depth': 17, 'min_child_weight': 16, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","\u001b[32m[I 2022-12-11 00:21:13,484]\u001b[0m Trial 161 finished with value: 0.26475037821482605 and parameters: {'n_estimators': 1, 'max_depth': 19, 'min_child_weight': 17, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","\u001b[32m[I 2022-12-11 00:21:13,591]\u001b[0m Trial 162 finished with value: 0.24659606656580935 and parameters: {'n_estimators': 24, 'max_depth': 20, 'min_child_weight': 15, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:13] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:13] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n","[00:21:13] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:13] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n","[00:21:13] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:13] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:13,696]\u001b[0m Trial 163 finished with value: 0.24810892586989408 and parameters: {'n_estimators': 20, 'max_depth': 20, 'min_child_weight': 15, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:13] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:13] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:13,933]\u001b[0m Trial 164 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 90, 'max_depth': 17, 'min_child_weight': 15, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","\u001b[32m[I 2022-12-11 00:21:14,112]\u001b[0m Trial 165 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 62, 'max_depth': 18, 'min_child_weight': 16, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:13] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:13] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n","[00:21:14] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:14] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:14,272]\u001b[0m Trial 166 finished with value: 0.25567322239031776 and parameters: {'n_estimators': 48, 'max_depth': 17, 'min_child_weight': 17, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","\u001b[32m[I 2022-12-11 00:21:14,395]\u001b[0m Trial 167 finished with value: 0.24357034795764 and parameters: {'n_estimators': 29, 'max_depth': 19, 'min_child_weight': 16, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:14] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:14] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n","[00:21:14] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:14] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:14,679]\u001b[0m Trial 168 finished with value: 0.24659606656580935 and parameters: {'n_estimators': 116, 'max_depth': 20, 'min_child_weight': 15, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:14] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:14] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:14,936]\u001b[0m Trial 169 finished with value: 0.24810892586989408 and parameters: {'n_estimators': 101, 'max_depth': 20, 'min_child_weight': 15, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","\u001b[32m[I 2022-12-11 00:21:15,042]\u001b[0m Trial 170 finished with value: 0.2526475037821483 and parameters: {'n_estimators': 20, 'max_depth': 19, 'min_child_weight': 18, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:14] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:14] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n","[00:21:15] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:15] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:15,321]\u001b[0m Trial 171 finished with value: 0.254160363086233 and parameters: {'n_estimators': 117, 'max_depth': 18, 'min_child_weight': 16, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","\u001b[32m[I 2022-12-11 00:21:15,459]\u001b[0m Trial 172 finished with value: 0.24810892586989408 and parameters: {'n_estimators': 37, 'max_depth': 19, 'min_child_weight': 16, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:15] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:15] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n","[00:21:15] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:15] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:15,655]\u001b[0m Trial 173 finished with value: 0.24659606656580935 and parameters: {'n_estimators': 71, 'max_depth': 17, 'min_child_weight': 17, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","\u001b[32m[I 2022-12-11 00:21:15,859]\u001b[0m Trial 174 finished with value: 0.24810892586989408 and parameters: {'n_estimators': 73, 'max_depth': 17, 'min_child_weight': 17, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:15] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:15] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:15] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:15] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:16,147]\u001b[0m Trial 175 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 123, 'max_depth': 16, 'min_child_weight': 17, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","\u001b[32m[I 2022-12-11 00:21:16,329]\u001b[0m Trial 176 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 57, 'max_depth': 17, 'min_child_weight': 17, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","\u001b[32m[I 2022-12-11 00:21:16,391]\u001b[0m Trial 177 finished with value: 0.5037821482602118 and parameters: {'n_estimators': 0, 'max_depth': 18, 'min_child_weight': 16, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:16] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:16] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n","[00:21:16] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","\u001b[32m[I 2022-12-11 00:21:16,506]\u001b[0m Trial 178 finished with value: 0.24810892586989408 and parameters: {'n_estimators': 26, 'max_depth': 19, 'min_child_weight': 16, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:16] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:16] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n","[00:21:16] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:16] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:16,750]\u001b[0m Trial 179 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 90, 'max_depth': 20, 'min_child_weight': 15, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:16] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:16] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:17,595]\u001b[0m Trial 180 finished with value: 0.254160363086233 and parameters: {'n_estimators': 315, 'max_depth': 20, 'min_child_weight': 4, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","\u001b[32m[I 2022-12-11 00:21:17,733]\u001b[0m Trial 181 finished with value: 0.24810892586989408 and parameters: {'n_estimators': 36, 'max_depth': 19, 'min_child_weight': 15, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","\u001b[32m[I 2022-12-11 00:21:17,838]\u001b[0m Trial 182 finished with value: 0.2526475037821483 and parameters: {'n_estimators': 22, 'max_depth': 18, 'min_child_weight': 18, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:17] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:17] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n","[00:21:17] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:17] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","\u001b[32m[I 2022-12-11 00:21:17,989]\u001b[0m Trial 183 finished with value: 0.24810892586989408 and parameters: {'n_estimators': 43, 'max_depth': 9, 'min_child_weight': 16, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:17] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:17] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n","[00:21:18] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:18] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:18,214]\u001b[0m Trial 184 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 82, 'max_depth': 20, 'min_child_weight': 17, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:18] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:18] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:18,891]\u001b[0m Trial 185 finished with value: 0.254160363086233 and parameters: {'n_estimators': 337, 'max_depth': 16, 'min_child_weight': 14, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","\u001b[32m[I 2022-12-11 00:21:19,073]\u001b[0m Trial 186 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 64, 'max_depth': 17, 'min_child_weight': 17, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:18] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:18] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n","[00:21:19] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:19] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","\u001b[32m[I 2022-12-11 00:21:19,172]\u001b[0m Trial 187 finished with value: 0.2571860816944024 and parameters: {'n_estimators': 15, 'max_depth': 19, 'min_child_weight': 15, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:19] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:19] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:19,883]\u001b[0m Trial 188 finished with value: 0.25567322239031776 and parameters: {'n_estimators': 372, 'max_depth': 20, 'min_child_weight': 16, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","\u001b[32m[I 2022-12-11 00:21:20,063]\u001b[0m Trial 189 finished with value: 0.24810892586989408 and parameters: {'n_estimators': 47, 'max_depth': 10, 'min_child_weight': 16, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:19] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:19] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n","[00:21:20] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:20] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:20,336]\u001b[0m Trial 190 finished with value: 0.2526475037821483 and parameters: {'n_estimators': 95, 'max_depth': 9, 'min_child_weight': 18, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","\u001b[32m[I 2022-12-11 00:21:20,552]\u001b[0m Trial 191 finished with value: 0.24659606656580935 and parameters: {'n_estimators': 69, 'max_depth': 16, 'min_child_weight': 17, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:20] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:20] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","\u001b[32m[I 2022-12-11 00:21:20,697]\u001b[0m Trial 192 finished with value: 0.24810892586989408 and parameters: {'n_estimators': 35, 'max_depth': 16, 'min_child_weight': 16, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:20] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:20] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n","[00:21:20] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:20] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:20,920]\u001b[0m Trial 193 finished with value: 0.24810892586989408 and parameters: {'n_estimators': 68, 'max_depth': 17, 'min_child_weight': 17, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","\u001b[32m[I 2022-12-11 00:21:21,089]\u001b[0m Trial 194 finished with value: 0.24357034795764 and parameters: {'n_estimators': 51, 'max_depth': 16, 'min_child_weight': 15, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:20] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:20] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n","[00:21:21] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:21] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","\u001b[32m[I 2022-12-11 00:21:21,306]\u001b[0m Trial 195 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 61, 'max_depth': 16, 'min_child_weight': 7, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","\u001b[32m[I 2022-12-11 00:21:21,454]\u001b[0m Trial 196 finished with value: 0.2450832072617246 and parameters: {'n_estimators': 40, 'max_depth': 18, 'min_child_weight': 14, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","\u001b[32m[I 2022-12-11 00:21:21,516]\u001b[0m Trial 197 finished with value: 0.26475037821482605 and parameters: {'n_estimators': 1, 'max_depth': 18, 'min_child_weight': 14, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:21] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:21] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n","[00:21:21] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:21] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","\u001b[32m[I 2022-12-11 00:21:21,745]\u001b[0m Trial 198 finished with value: 0.2526475037821483 and parameters: {'n_estimators': 84, 'max_depth': 17, 'min_child_weight': 15, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:21] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:21] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","\u001b[32m[I 2022-12-11 00:21:21,902]\u001b[0m Trial 199 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 47, 'max_depth': 19, 'min_child_weight': 14, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:21] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:21] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n","[00:21:21] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:21] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:22,037]\u001b[0m Trial 200 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 31, 'max_depth': 18, 'min_child_weight': 15, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","\u001b[32m[I 2022-12-11 00:21:22,164]\u001b[0m Trial 201 finished with value: 0.254160363086233 and parameters: {'n_estimators': 28, 'max_depth': 16, 'min_child_weight': 17, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:22] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:22] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n","[00:21:22] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:22] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:22,846]\u001b[0m Trial 202 finished with value: 0.2526475037821483 and parameters: {'n_estimators': 359, 'max_depth': 16, 'min_child_weight': 17, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","\u001b[32m[I 2022-12-11 00:21:23,054]\u001b[0m Trial 203 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 73, 'max_depth': 8, 'min_child_weight': 16, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:22] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:22] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","\u001b[32m[I 2022-12-11 00:21:23,158]\u001b[0m Trial 204 finished with value: 0.254160363086233 and parameters: {'n_estimators': 18, 'max_depth': 18, 'min_child_weight': 15, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:23] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:23] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n","[00:21:23] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:23] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:23,810]\u001b[0m Trial 205 finished with value: 0.254160363086233 and parameters: {'n_estimators': 312, 'max_depth': 17, 'min_child_weight': 14, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:23] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:23] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:24,078]\u001b[0m Trial 206 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 107, 'max_depth': 17, 'min_child_weight': 16, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:24] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:24] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:24,774]\u001b[0m Trial 207 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 338, 'max_depth': 19, 'min_child_weight': 15, 'subsample': 0.9, 'colsample_bytree': 0.6}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","\u001b[32m[I 2022-12-11 00:21:24,945]\u001b[0m Trial 208 finished with value: 0.24659606656580935 and parameters: {'n_estimators': 54, 'max_depth': 18, 'min_child_weight': 15, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:24] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:24] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n","[00:21:25] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:25] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:25,116]\u001b[0m Trial 209 finished with value: 0.24357034795764 and parameters: {'n_estimators': 51, 'max_depth': 18, 'min_child_weight': 15, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","\u001b[32m[I 2022-12-11 00:21:25,287]\u001b[0m Trial 210 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 50, 'max_depth': 18, 'min_child_weight': 14, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:25] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:25] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n","[00:21:25] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:25] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:25,464]\u001b[0m Trial 211 finished with value: 0.24205748865355525 and parameters: {'n_estimators': 58, 'max_depth': 18, 'min_child_weight': 15, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","\u001b[32m[I 2022-12-11 00:21:25,531]\u001b[0m Trial 212 finished with value: 0.5037821482602118 and parameters: {'n_estimators': 0, 'max_depth': 19, 'min_child_weight': 16, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:25] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:25] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:25] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:25,743]\u001b[0m Trial 213 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 75, 'max_depth': 18, 'min_child_weight': 15, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","\u001b[32m[I 2022-12-11 00:21:25,865]\u001b[0m Trial 214 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 27, 'max_depth': 18, 'min_child_weight': 15, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:25] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:25] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n","[00:21:25] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:25] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:26,178]\u001b[0m Trial 215 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 132, 'max_depth': 15, 'min_child_weight': 18, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","\u001b[32m[I 2022-12-11 00:21:26,312]\u001b[0m Trial 216 finished with value: 0.24810892586989408 and parameters: {'n_estimators': 35, 'max_depth': 19, 'min_child_weight': 15, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:26] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:26] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n","[00:21:26] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:26] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:26,493]\u001b[0m Trial 217 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 58, 'max_depth': 18, 'min_child_weight': 16, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","\u001b[32m[I 2022-12-11 00:21:26,689]\u001b[0m Trial 218 finished with value: 0.24659606656580935 and parameters: {'n_estimators': 69, 'max_depth': 17, 'min_child_weight': 16, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:26] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:26] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:26] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:26] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:26,962]\u001b[0m Trial 219 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 98, 'max_depth': 17, 'min_child_weight': 15, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","\u001b[32m[I 2022-12-11 00:21:27,204]\u001b[0m Trial 220 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 87, 'max_depth': 17, 'min_child_weight': 16, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:27] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:27] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:27] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:27] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:27,827]\u001b[0m Trial 221 finished with value: 0.2450832072617246 and parameters: {'n_estimators': 303, 'max_depth': 16, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:27] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:27] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:28,453]\u001b[0m Trial 222 finished with value: 0.2450832072617246 and parameters: {'n_estimators': 304, 'max_depth': 16, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:28] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:28] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:29,086]\u001b[0m Trial 223 finished with value: 0.2450832072617246 and parameters: {'n_estimators': 306, 'max_depth': 16, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:29] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:29] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:29,699]\u001b[0m Trial 224 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 300, 'max_depth': 16, 'min_child_weight': 18, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:29] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:29] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:30,329]\u001b[0m Trial 225 finished with value: 0.2450832072617246 and parameters: {'n_estimators': 310, 'max_depth': 20, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:30] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:30] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:30,761]\u001b[0m Trial 226 finished with value: 0.2526475037821483 and parameters: {'n_estimators': 286, 'max_depth': 2, 'min_child_weight': 18, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:30] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:30] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:31,394]\u001b[0m Trial 227 finished with value: 0.2450832072617246 and parameters: {'n_estimators': 304, 'max_depth': 20, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:31] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:31] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:32,005]\u001b[0m Trial 228 finished with value: 0.2526475037821483 and parameters: {'n_estimators': 306, 'max_depth': 15, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.6}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:32] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:32] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:32,575]\u001b[0m Trial 229 finished with value: 0.2450832072617246 and parameters: {'n_estimators': 271, 'max_depth': 20, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:32] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:32] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:33,139]\u001b[0m Trial 230 finished with value: 0.24659606656580935 and parameters: {'n_estimators': 269, 'max_depth': 20, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:33] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:33] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:33,801]\u001b[0m Trial 231 finished with value: 0.2450832072617246 and parameters: {'n_estimators': 295, 'max_depth': 20, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:33] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:33] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:34,409]\u001b[0m Trial 232 finished with value: 0.2450832072617246 and parameters: {'n_estimators': 288, 'max_depth': 20, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:34] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:34] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:34,994]\u001b[0m Trial 233 finished with value: 0.2450832072617246 and parameters: {'n_estimators': 280, 'max_depth': 20, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:35] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:35] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:35,622]\u001b[0m Trial 234 finished with value: 0.2450832072617246 and parameters: {'n_estimators': 282, 'max_depth': 20, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:35] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:35] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:36,236]\u001b[0m Trial 235 finished with value: 0.24659606656580935 and parameters: {'n_estimators': 262, 'max_depth': 20, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:36] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:36] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:36,899]\u001b[0m Trial 236 finished with value: 0.2450832072617246 and parameters: {'n_estimators': 283, 'max_depth': 20, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:36] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:36] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:37,550]\u001b[0m Trial 237 finished with value: 0.2450832072617246 and parameters: {'n_estimators': 283, 'max_depth': 20, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:37] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:37] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:38,212]\u001b[0m Trial 238 finished with value: 0.24659606656580935 and parameters: {'n_estimators': 286, 'max_depth': 20, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:38] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:38] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:38,825]\u001b[0m Trial 239 finished with value: 0.24659606656580935 and parameters: {'n_estimators': 279, 'max_depth': 20, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:38] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:38] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:39,382]\u001b[0m Trial 240 finished with value: 0.24659606656580935 and parameters: {'n_estimators': 257, 'max_depth': 20, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:39] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:39] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:40,013]\u001b[0m Trial 241 finished with value: 0.2450832072617246 and parameters: {'n_estimators': 305, 'max_depth': 20, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:40] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:40] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:40,612]\u001b[0m Trial 242 finished with value: 0.2450832072617246 and parameters: {'n_estimators': 290, 'max_depth': 20, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:40] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:40] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:41,243]\u001b[0m Trial 243 finished with value: 0.2450832072617246 and parameters: {'n_estimators': 302, 'max_depth': 20, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:41] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:41] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:41,837]\u001b[0m Trial 244 finished with value: 0.24659606656580935 and parameters: {'n_estimators': 285, 'max_depth': 20, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:41] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:41] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:42,455]\u001b[0m Trial 245 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 304, 'max_depth': 20, 'min_child_weight': 18, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:42] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:42] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:43,079]\u001b[0m Trial 246 finished with value: 0.2450832072617246 and parameters: {'n_estimators': 298, 'max_depth': 20, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:43] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:43] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:43,728]\u001b[0m Trial 247 finished with value: 0.2450832072617246 and parameters: {'n_estimators': 316, 'max_depth': 20, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:43] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:43] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:44,390]\u001b[0m Trial 248 finished with value: 0.2450832072617246 and parameters: {'n_estimators': 319, 'max_depth': 20, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:44] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:44] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:45,041]\u001b[0m Trial 249 finished with value: 0.2450832072617246 and parameters: {'n_estimators': 318, 'max_depth': 20, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:45] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:45] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:45,688]\u001b[0m Trial 250 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 318, 'max_depth': 20, 'min_child_weight': 18, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:45] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:45] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:46,351]\u001b[0m Trial 251 finished with value: 0.24659606656580935 and parameters: {'n_estimators': 322, 'max_depth': 20, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:46] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:46] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:46,932]\u001b[0m Trial 252 finished with value: 0.2450832072617246 and parameters: {'n_estimators': 267, 'max_depth': 20, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:46] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:46] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:47,514]\u001b[0m Trial 253 finished with value: 0.24659606656580935 and parameters: {'n_estimators': 266, 'max_depth': 20, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:47] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:47] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:48,047]\u001b[0m Trial 254 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 247, 'max_depth': 20, 'min_child_weight': 18, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:48] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:48] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:48,639]\u001b[0m Trial 255 finished with value: 0.2450832072617246 and parameters: {'n_estimators': 281, 'max_depth': 19, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:48] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:48] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:49,216]\u001b[0m Trial 256 finished with value: 0.2450832072617246 and parameters: {'n_estimators': 268, 'max_depth': 19, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:49] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:49] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:49,825]\u001b[0m Trial 257 finished with value: 0.2450832072617246 and parameters: {'n_estimators': 296, 'max_depth': 20, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:49] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:49] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:50,499]\u001b[0m Trial 258 finished with value: 0.24659606656580935 and parameters: {'n_estimators': 329, 'max_depth': 20, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:50] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:50] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:51,009]\u001b[0m Trial 259 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 230, 'max_depth': 19, 'min_child_weight': 18, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:51] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:51] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:51,614]\u001b[0m Trial 260 finished with value: 0.24659606656580935 and parameters: {'n_estimators': 286, 'max_depth': 19, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:51] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:51] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:52,176]\u001b[0m Trial 261 finished with value: 0.24659606656580935 and parameters: {'n_estimators': 266, 'max_depth': 19, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:52] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:52] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:52,825]\u001b[0m Trial 262 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 318, 'max_depth': 19, 'min_child_weight': 18, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:52] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:52] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:53,455]\u001b[0m Trial 263 finished with value: 0.2450832072617246 and parameters: {'n_estimators': 305, 'max_depth': 20, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:53] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:53] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:54,073]\u001b[0m Trial 264 finished with value: 0.2450832072617246 and parameters: {'n_estimators': 293, 'max_depth': 20, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:54] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:54] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:54,685]\u001b[0m Trial 265 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 292, 'max_depth': 20, 'min_child_weight': 18, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:54] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:54] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:55,408]\u001b[0m Trial 266 finished with value: 0.2526475037821483 and parameters: {'n_estimators': 302, 'max_depth': 20, 'min_child_weight': 10, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:55] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:55] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:55,944]\u001b[0m Trial 267 finished with value: 0.2450832072617246 and parameters: {'n_estimators': 246, 'max_depth': 19, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:56] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:56] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:56,485]\u001b[0m Trial 268 finished with value: 0.24659606656580935 and parameters: {'n_estimators': 251, 'max_depth': 20, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:56] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:56] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:57,122]\u001b[0m Trial 269 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 306, 'max_depth': 20, 'min_child_weight': 18, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:57] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:57] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:57,789]\u001b[0m Trial 270 finished with value: 0.24659606656580935 and parameters: {'n_estimators': 325, 'max_depth': 20, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:57] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:57] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:58,404]\u001b[0m Trial 271 finished with value: 0.2450832072617246 and parameters: {'n_estimators': 293, 'max_depth': 20, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:58] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:58] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:58,993]\u001b[0m Trial 272 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 282, 'max_depth': 19, 'min_child_weight': 18, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:59] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:59] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:21:59,835]\u001b[0m Trial 273 finished with value: 0.25567322239031776 and parameters: {'n_estimators': 241, 'max_depth': 20, 'min_child_weight': 2, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:21:59] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:21:59] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:22:00,597]\u001b[0m Trial 274 finished with value: 0.24659606656580935 and parameters: {'n_estimators': 331, 'max_depth': 19, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:22:00] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:22:00] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:22:01,256]\u001b[0m Trial 275 finished with value: 0.2450832072617246 and parameters: {'n_estimators': 282, 'max_depth': 20, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:22:01] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:22:01] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:22:01,825]\u001b[0m Trial 276 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 265, 'max_depth': 19, 'min_child_weight': 16, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:22:01] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:22:01] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:22:02,305]\u001b[0m Trial 277 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 222, 'max_depth': 19, 'min_child_weight': 18, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:22:02] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:22:02] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:22:02,959]\u001b[0m Trial 278 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 317, 'max_depth': 20, 'min_child_weight': 16, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:22:03] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:22:03] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:22:03,652]\u001b[0m Trial 279 finished with value: 0.24659606656580935 and parameters: {'n_estimators': 341, 'max_depth': 19, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:22:03] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:22:03] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:22:04,277]\u001b[0m Trial 280 finished with value: 0.2450832072617246 and parameters: {'n_estimators': 298, 'max_depth': 20, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:22:04] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:22:04] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:22:04,917]\u001b[0m Trial 281 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 297, 'max_depth': 20, 'min_child_weight': 16, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:22:04] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:22:04] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:22:05,572]\u001b[0m Trial 282 finished with value: 0.2450832072617246 and parameters: {'n_estimators': 317, 'max_depth': 20, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:22:05] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:22:05] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:22:06,105]\u001b[0m Trial 283 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 250, 'max_depth': 19, 'min_child_weight': 18, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:22:06] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:22:06] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:22:06,774]\u001b[0m Trial 284 finished with value: 0.24659606656580935 and parameters: {'n_estimators': 325, 'max_depth': 19, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:22:06] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:22:06] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:22:07,350]\u001b[0m Trial 285 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 274, 'max_depth': 20, 'min_child_weight': 18, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:22:07] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:22:07] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:22:07,929]\u001b[0m Trial 286 finished with value: 0.2450832072617246 and parameters: {'n_estimators': 271, 'max_depth': 20, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:22:07] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:22:07] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:22:08,521]\u001b[0m Trial 287 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 272, 'max_depth': 20, 'min_child_weight': 16, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:22:08] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:22:08] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:22:09,198]\u001b[0m Trial 288 finished with value: 0.2450832072617246 and parameters: {'n_estimators': 335, 'max_depth': 19, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:22:09] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:22:09] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:22:09,956]\u001b[0m Trial 289 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 362, 'max_depth': 19, 'min_child_weight': 16, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:22:10] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:22:10] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:22:10,538]\u001b[0m Trial 290 finished with value: 0.24659606656580935 and parameters: {'n_estimators': 265, 'max_depth': 20, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:22:10] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:22:10] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:22:11,077]\u001b[0m Trial 291 finished with value: 0.2450832072617246 and parameters: {'n_estimators': 246, 'max_depth': 19, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:22:11] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:22:11] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:22:11,717]\u001b[0m Trial 292 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 308, 'max_depth': 20, 'min_child_weight': 18, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:22:11] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:22:11] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:22:12,450]\u001b[0m Trial 293 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 351, 'max_depth': 19, 'min_child_weight': 16, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:22:12] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:22:12] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:22:12,959]\u001b[0m Trial 294 finished with value: 0.24357034795764 and parameters: {'n_estimators': 234, 'max_depth': 19, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:22:13] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:22:13] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:22:13,585]\u001b[0m Trial 295 finished with value: 0.2450832072617246 and parameters: {'n_estimators': 299, 'max_depth': 19, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:22:13] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:22:13] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:22:14,071]\u001b[0m Trial 296 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 224, 'max_depth': 19, 'min_child_weight': 18, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:22:14] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:22:14] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:22:14,607]\u001b[0m Trial 297 finished with value: 0.2450832072617246 and parameters: {'n_estimators': 246, 'max_depth': 19, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:22:14] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:22:14] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:22:15,143]\u001b[0m Trial 298 finished with value: 0.24357034795764 and parameters: {'n_estimators': 241, 'max_depth': 19, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  if sys.path[0] == \"\":\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n","  del sys.path[0]\n","c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:22:15] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"verbose\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:22:15] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-12-11 00:22:15,613]\u001b[0m Trial 299 finished with value: 0.24659606656580935 and parameters: {'n_estimators': 202, 'max_depth': 19, 'min_child_weight': 16, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n"]}],"source":["study = XGBoostClassifier_opt(X_res, X_test, y_res, y_test)"]},{"cell_type":"code","execution_count":280,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[00:22:59] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n","Parameters: { \"dtrain\", \"early_stopping_rounds\", \"evals\", \"num_boost_round\", \"params\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","[00:22:59] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n","accuracy scores 0.75642965204236\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 2 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["XGBoostClassifier(X_train, X_test, y_train, y_test, study)"]},{"cell_type":"code","execution_count":233,"metadata":{},"outputs":[{"data":{"text/plain":["0.7569535999414048"]},"execution_count":233,"metadata":{},"output_type":"execute_result"}],"source":[]},{"cell_type":"code","execution_count":253,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":258,"metadata":{},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 2 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":[]},{"cell_type":"code","execution_count":260,"metadata":{},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":122419,"status":"ok","timestamp":1668677379418,"user":{"displayName":"hiroto iwasaki","userId":"11312076570438010727"},"user_tz":-540},"id":"0SsPPYpZ0GCV","outputId":"e8fcba4c-2df6-4b2e-fb24-0dfc067c3806"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  estimator.fit(X_train, y_train, **fit_params)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:926: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  self.best_estimator_.fit(X, y, **fit_params)\n"]},{"data":{"text/plain":["GridSearchCV(estimator=RandomForestClassifier(),\n","             param_grid={'max_depth': (10, 20, 30, 40, 50, None),\n","                         'max_features': ('sqrt', 'log2', 'auto', None),\n","                         'n_estimators': [10, 20, 30, 50, 100, 300]},\n","             scoring='accuracy')"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["X_resampled = pd.read_csv(\"X_resampled.csv\", index_col=0)\n","y_resampled = pd.read_csv(\"y_resampled.csv\", index_col=0)\n","\n","X_resampled = (X_resampled.mean()-X_resampled)/X_resampled.std()\n","\n","model = RandomForestClassifier()\n","parameters = {  \n","    'n_estimators': [10, 20, 30, 50, 100, 300],     # 用意する決定木モデルの数\n","    'max_features': ('sqrt', 'log2','auto', None),  # ランダムに指定する特徴量の数\n","    'max_depth':    (10, 20, 30, 40, 50, None),     # 決定木のノード深さの制限値\n","}\n","gridsearch = GridSearchCV(estimator=model, param_grid=parameters, scoring=\"accuracy\")\n","gridsearch.fit(X_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":821,"status":"ok","timestamp":1668677441240,"user":{"displayName":"hiroto iwasaki","userId":"11312076570438010727"},"user_tz":-540},"id":"ADPN0Rv5uuCG","outputId":"90a85999-ed0a-4ae5-d8b7-a45a84207989"},"outputs":[{"name":"stdout","output_type":"stream","text":["best.params_: {'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 10}\n","best.score_: 1.0\n"]}],"source":["def gridsearch_info():\n","  print(f\"best.params_: {gridsearch.best_params_}\")\n","  print(f\"best.score_: {gridsearch.best_score_}\")\n","\n","def "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1668677442554,"user":{"displayName":"hiroto iwasaki","userId":"11312076570438010727"},"user_tz":-540},"id":"5b11KnRtMfum","outputId":"c963570c-2ac2-47eb-b812-7c2767dabb80"},"outputs":[{"name":"stdout","output_type":"stream","text":["1.0\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  \n"]}],"source":["model = RandomForestClassifier(n_estimators = gridsearch.best_params_['n_estimators'], # 用意する決定木モデルの数\n","                               max_features = gridsearch.best_params_['max_features'], # ランダムに指定する特徴量の数\n","                               max_depth    = gridsearch.best_params_['max_depth'],    # 決定木のノード深さの制限値\n","                               criterion='gini',                                       # 不純度評価指標の種類(ジニ係数）\n","                               min_samples_leaf = 1,                                   # 1ノードの深さの最小値\n","                               random_state = 0,                                       # 乱数シード\n","                              )\n","y_pred = model.fit(X_test, y_test)\n","predicted = model.predict(X_test)\n","print(metrics.accuracy_score(predicted,y_test))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":265},"executionInfo":{"elapsed":918,"status":"ok","timestamp":1668681462037,"user":{"displayName":"hiroto iwasaki","userId":"11312076570438010727"},"user_tz":-540},"id":"_qyVENkgKfIl","outputId":"e117390c-fe66-4a35-b9c8-04ba19f15a39"},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["from sklearn.datasets import make_blobs\n","from sklearn.neural_network import MLPClassifier\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","if __name__ == \"__main__\":\n","\n","    # サンプルデータを生成する\n","    x, y = make_blobs(n_samples=100, n_features=2, centers=2)\n","\n","    # 分類モデル（※今回はニューラルネットワーク）を作成する\n","    estimator = MLPClassifier()\n","    estimator.fit(x, y)\n","\n","    # サンプルデータの値域を求める\n","    f1_min = x[:, 0].min() - 0.5\n","    f1_max = x[:, 0].max() + 0.5\n","    f2_min = x[:, 1].min() - 0.5\n","    f2_max = x[:, 1].max() + 0.5\n","\n","    step = 0.02\n","    f1_range = np.arange(f1_min, f1_max, step)\n","    f2_range = np.arange(f2_min, f2_max, step)\n","    f1, f2 = np.meshgrid(f1_range, f2_range)\n","\n","    # 決定境界を描画する\n","    Z = estimator.predict_proba(np.c_[f1.ravel(), f2.ravel()])[:, 1]\n","    Z = Z.reshape(f1.shape)\n","\n","    plt.contourf(f1, f2, Z, cmap=plt.cm.RdBu, alpha=0.8)\n","    plt.scatter(x[:,0], x[:,1], c=y, cmap=plt.cm.RdBu)\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9422,"status":"ok","timestamp":1669107629239,"user":{"displayName":"hiroto iwasaki","userId":"11312076570438010727"},"user_tz":-540},"id":"CeoCJPpx2aoW","outputId":"350e691b-73c5-4250-f690-eb023b3ed5f7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting optuna\n","  Downloading optuna-3.0.3-py3-none-any.whl (348 kB)\n","\u001b[K     |████████████████████████████████| 348 kB 30.1 MB/s \n","\u001b[?25hCollecting cmaes>=0.8.2\n","  Downloading cmaes-0.9.0-py3-none-any.whl (23 kB)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from optuna) (6.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from optuna) (1.21.6)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (21.3)\n","Requirement already satisfied: importlib-metadata<5.0.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (4.13.0)\n","Requirement already satisfied: scipy<1.9.0,>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.7.3)\n","Collecting cliff\n","  Downloading cliff-3.10.1-py3-none-any.whl (81 kB)\n","\u001b[K     |████████████████████████████████| 81 kB 11.2 MB/s \n","\u001b[?25hRequirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.44)\n","Collecting colorlog\n","  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n","Collecting alembic>=1.5.0\n","  Downloading alembic-1.8.1-py3-none-any.whl (209 kB)\n","\u001b[K     |████████████████████████████████| 209 kB 73.0 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from optuna) (4.64.1)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from alembic>=1.5.0->optuna) (5.10.0)\n","Collecting Mako\n","  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n","\u001b[K     |████████████████████████████████| 78 kB 8.0 MB/s \n","\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<5.0.0->optuna) (3.10.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<5.0.0->optuna) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->optuna) (3.0.9)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.3.0->optuna) (2.0.1)\n","Requirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (3.5.0)\n","Collecting stevedore>=2.0.1\n","  Downloading stevedore-3.5.2-py3-none-any.whl (50 kB)\n","\u001b[K     |████████████████████████████████| 50 kB 8.2 MB/s \n","\u001b[?25hCollecting cmd2>=1.0.0\n","  Downloading cmd2-2.4.2-py3-none-any.whl (147 kB)\n","\u001b[K     |████████████████████████████████| 147 kB 66.7 MB/s \n","\u001b[?25hCollecting autopage>=0.4.0\n","  Downloading autopage-0.5.1-py3-none-any.whl (29 kB)\n","Collecting pbr!=2.1.0,>=2.0.0\n","  Downloading pbr-5.11.0-py2.py3-none-any.whl (112 kB)\n","\u001b[K     |████████████████████████████████| 112 kB 66.5 MB/s \n","\u001b[?25hRequirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\n","Collecting pyperclip>=1.6\n","  Downloading pyperclip-1.8.2.tar.gz (20 kB)\n","Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (22.1.0)\n","Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.0.1)\n","Building wheels for collected packages: pyperclip\n","  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11136 sha256=4d5e3142f3a778ee0d757f9091e2a5d44b87cbee0c4deb5b2a2f8d56575fdba5\n","  Stored in directory: /root/.cache/pip/wheels/9f/18/84/8f69f8b08169c7bae2dde6bd7daf0c19fca8c8e500ee620a28\n","Successfully built pyperclip\n","Installing collected packages: pyperclip, pbr, stevedore, Mako, cmd2, autopage, colorlog, cmaes, cliff, alembic, optuna\n","Successfully installed Mako-1.2.4 alembic-1.8.1 autopage-0.5.1 cliff-3.10.1 cmaes-0.9.0 cmd2-2.4.2 colorlog-6.7.0 optuna-3.0.3 pbr-5.11.0 pyperclip-1.8.2 stevedore-3.5.2\n"]}],"source":["!pip install optuna\n","import xgboost as xgb\n","import optuna"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5884,"status":"ok","timestamp":1669073595010,"user":{"displayName":"hiroto iwasaki","userId":"11312076570438010727"},"user_tz":-540},"id":"s-6ezRbsiWsE","outputId":"d98e4457-3d56-4f4a-8872-d2620f10ef80"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting seaborn_analyzer\n","  Downloading seaborn_analyzer-0.2.13-py3-none-any.whl (59 kB)\n","\u001b[K     |████████████████████████████████| 59 kB 2.8 MB/s \n","\u001b[?25hRequirement already satisfied: scikit-learn>=0.24.2 in /usr/local/lib/python3.7/dist-packages (from seaborn_analyzer) (1.0.2)\n","Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.7/dist-packages (from seaborn_analyzer) (0.11.2)\n","Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.7/dist-packages (from seaborn_analyzer) (1.21.6)\n","Requirement already satisfied: pandas>=1.2.4 in /usr/local/lib/python3.7/dist-packages (from seaborn_analyzer) (1.3.5)\n","Requirement already satisfied: scipy>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from seaborn_analyzer) (1.7.3)\n","Requirement already satisfied: matplotlib>=3.1.3 in /usr/local/lib/python3.7/dist-packages (from seaborn_analyzer) (3.2.2)\n","Collecting lightgbm>=3.3.2\n","  Downloading lightgbm-3.3.3-py3-none-manylinux1_x86_64.whl (2.0 MB)\n","\u001b[K     |████████████████████████████████| 2.0 MB 9.2 MB/s \n","\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from lightgbm>=3.3.2->seaborn_analyzer) (0.38.4)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.3->seaborn_analyzer) (0.11.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.3->seaborn_analyzer) (3.0.9)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.3->seaborn_analyzer) (2.8.2)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.3->seaborn_analyzer) (1.4.4)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=3.1.3->seaborn_analyzer) (4.1.1)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.4->seaborn_analyzer) (2022.6)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=3.1.3->seaborn_analyzer) (1.15.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24.2->seaborn_analyzer) (3.1.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24.2->seaborn_analyzer) (1.2.0)\n","Installing collected packages: lightgbm, seaborn-analyzer\n","  Attempting uninstall: lightgbm\n","    Found existing installation: lightgbm 2.2.3\n","    Uninstalling lightgbm-2.2.3:\n","      Successfully uninstalled lightgbm-2.2.3\n","Successfully installed lightgbm-3.3.3 seaborn-analyzer-0.2.13\n"]}],"source":["!pip install seaborn_analyzer"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":245},"executionInfo":{"elapsed":3625,"status":"error","timestamp":1669073600344,"user":{"displayName":"hiroto iwasaki","userId":"11312076570438010727"},"user_tz":-540},"id":"rSAsBFZdzzpY","outputId":"e83fe754-71b5-49e5-b297-7d03b9a6e6c9"},"outputs":[{"ename":"NameError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-499470768259>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m               \u001b[0;34m'early_stopping_rounds'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# 学習時、評価指標がこの回数連続で改善しなくなった時点でストップ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m               \u001b[0;34m'eval_metric'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'rmse'\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# early_stopping_roundsの評価指標\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m               \u001b[0;34m'eval_set'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# early_stopping_roundsの評価指標算出用データ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m               }\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# クロスバリデーションして決定境界を可視化\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"]}],"source":["from seaborn_analyzer import regplot\n","from xgboost import XGBRegressor\n","from sklearn.model_selection import KFold\n","\n","X_resampled = pd.read_csv(\"X_resampled.csv\", index_col=0)\n","y_resampled = pd.read_csv(\"y_resampled.csv\", index_col=0)\n","\n","X_resampled = (X_resampled.mean()-X_resampled)/X_resampled.std()\n","\n","X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.3, stratify=y_resampled)\n","\n","# 乱数シード\n","seed = 42\n","# モデル作成\n","model = XGBRegressor(booster='gbtree', objective='reg:squarederror',\n","                     random_state=seed, n_estimators=10000)  # チューニング前のモデル\n","# 学習時fitパラメータ指定\n","fit_params = {'verbose': 0,  # 学習中のコマンドライン出力\n","              'early_stopping_rounds': 10,  # 学習時、評価指標がこの回数連続で改善しなくなった時点でストップ\n","              'eval_metric': 'rmse',  # early_stopping_roundsの評価指標\n","              'eval_set': [(X, y)]  # early_stopping_roundsの評価指標算出用データ\n","              }\n","# クロスバリデーションして決定境界を可視化\n","seed = 42  # 乱数シード\n","cv = KFold(n_splits=3, shuffle=True, random_state=seed)  # KFoldでクロスバリデーション分割指定\n","regplot.regression_heat_plot(model, USE_EXPLANATORY, OBJECTIVE_VARIALBLE, df_osaka,\n","                             pair_sigmarange = 0.5, rounddigit_x1=3, rounddigit_x2=3,\n","                             cv=cv, display_cv_indices=0,\n","                             fit_params=fit_params)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":503},"executionInfo":{"elapsed":566,"status":"error","timestamp":1669107666310,"user":{"displayName":"hiroto iwasaki","userId":"11312076570438010727"},"user_tz":-540},"id":"yH_GLGJxfWyu","outputId":"8a05bb5e-031f-4ab7-e0cd-1281b7e4d877"},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-11-22 09:01:03,169]\u001b[0m A new study created in memory with name: no-name-ba2cc7f5-a24c-45b2-8edb-2577caba2e50\u001b[0m\n","\u001b[33m[W 2022-11-22 09:01:03,171]\u001b[0m Trial 0 failed because of the following error: TypeError(\"objective() missing 2 required positional arguments: 'y' and 'trial'\")\u001b[0m\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/optuna/study/_optimize.py\", line 196, in _run_trial\n","    value_or_values = func(trial)\n","TypeError: objective() missing 2 required positional arguments: 'y' and 'trial'\n"]},{"ename":"TypeError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-96be2de41a23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    426\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgc_after_trial\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m             \u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m         )\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0mreseed_sampler_rng\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0mtime_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                 \u001b[0mprogress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprogress_bar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             )\n\u001b[1;32m     78\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     ):\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: objective() missing 2 required positional arguments: 'y' and 'trial'"]}],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import KFold\n","from sklearn.metrics import roc_auc_score, accuracy_score\n","from sklearn.model_selection import train_test_split\n","import xgboost as xgb\n","import optuna\n","\n","#Objective関数の設定\n","def objective(x, y, trial):\n","    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)\n","\n","    params = {\n","        'objective': 'binary:logistic',\n","        'max_depth': trial.suggest_int('max_depth', 1, 9),\n","        'n_estimators': trial.suggest_int('n_estimators', 10, 1000),\n","        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-8, 1.0)\n","    }\n","\n","    model = xgb.XGBClassifier(**params)\n","    model.fit(x_train, y_train)\n","\n","    pred = model.predict(x_test)\n","\n","    accuracy = accuracy_score(y_test, pred)\n","    return (1-accuracy)\n","\n","if __name__ == '__main__':\n","\n","    study = optuna.create_study()\n","    study.optimize(objective, n_trials=300)\n","\n","    print(study.best_params)\n","    print(study.best_value)\n","    print(study.best_trial)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":139,"status":"ok","timestamp":1668594428941,"user":{"displayName":"hiroto iwasaki","userId":"11312076570438010727"},"user_tz":-540},"id":"A32HeO6P8kxk","outputId":"a29950a9-0120-4d87-e35e-77b89de35f14"},"outputs":[{"name":"stdout","output_type":"stream","text":["y_train: Counter({0: 2283, 1: 2282})\n","y_test Counter({1: 979, 0: 978})\n"]}],"source":["from collections import Counter\n","print(\"y_train:\", Counter(y_train[\"Transported\"]))\n","print(\"y_test\", Counter(y_test[\"Transported\"]))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1668639106261,"user":{"displayName":"hiroto iwasaki","userId":"11312076570438010727"},"user_tz":-540},"id":"ge2TrAvQ-Nmi","outputId":"a4fdddbb-6c26-4333-8ef6-a5ae1d253fe1"},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","Int64Index: 6522 entries, 0 to 6521\n","Data columns (total 21 columns):\n"," #   Column                     Non-Null Count  Dtype  \n","---  ------                     --------------  -----  \n"," 0   CabinNum                   6522 non-null   float64\n"," 1   HomePlanet_Earth           6522 non-null   float64\n"," 2   HomePlanet_Europa          6522 non-null   float64\n"," 3   HomePlanet_Mars            6522 non-null   float64\n"," 4   CryoSleep_False            6522 non-null   float64\n"," 5   CryoSleep_True             6522 non-null   float64\n"," 6   Destination_55 Cancri e    6522 non-null   float64\n"," 7   Destination_PSO J318.5-22  6522 non-null   float64\n"," 8   Destination_TRAPPIST-1e    6522 non-null   float64\n"," 9   VIP_False                  6522 non-null   float64\n"," 10  VIP_True                   6522 non-null   float64\n"," 11  CabinDeck_A                6522 non-null   float64\n"," 12  CabinDeck_B                6522 non-null   float64\n"," 13  CabinDeck_C                6522 non-null   float64\n"," 14  CabinDeck_D                6522 non-null   float64\n"," 15  CabinDeck_E                6522 non-null   float64\n"," 16  CabinDeck_F                6522 non-null   float64\n"," 17  CabinDeck_G                6522 non-null   float64\n"," 18  CabinDeck_T                6522 non-null   float64\n"," 19  CabinSide_P                6522 non-null   float64\n"," 20  CabinSide_S                6522 non-null   float64\n","dtypes: float64(21)\n","memory usage: 1.1 MB\n"]}],"source":["X_resampled.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ILoD1VYgo4V7"},"outputs":[],"source":["from lightgbm import LGBMClassifier"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gT4RQOzVrGXT"},"outputs":[],"source":["model = LGBMClassifier()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q2OJOnacrwwY"},"outputs":[],"source":["parameters = {'max_depth': [4,6], \n","              'n_estimators': [50,100,200]\n","              }\n","\n","gridsearch = GridSearchCV(model, param_grid=parameters , verbose=0)\n","gridsearch.fit(X_train, y_train)\n","print(gridsearch.best_params_, gridsearch.best_score_)\n","\n","model = LGBMClassifier(**gridsearch.best_params_).fit(X_train, y_train)\n","y_pred = model.predict(X_test)\n","print(confusion_matrix(y_test, y_pred))\n","print(classification_report(y_test, y_pred))\n","\n","roc = roc_curve(y_test, y_pred)\n","fpr, tpr, proba = roc_curve(y_test, y_pred)\n","auc = auc(fpr, tpr)\n","fig, ax = plt.subplots(figsize = (8,6))\n","ax.plot(fpr, tpr)\n","\n","ax.set_xlabel('FPR')\n","ax.set_ylabel('TPR')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CDFUvK-QwV79"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMsudEsI14TsaoWhgCXjays","name":"","version":""},"kernelspec":{"display_name":"Python 3.7.15 ('py37')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.15 (default, Nov 24 2022, 18:44:54) [MSC v.1916 64 bit (AMD64)]"},"vscode":{"interpreter":{"hash":"13829c6cee335f7d4f78a61190ede447d35a3772f027e7e85d688cdb17698982"}}},"nbformat":4,"nbformat_minor":0}
