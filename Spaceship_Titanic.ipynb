{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zakihir0/first-commit/blob/main/Spaceship_Titanic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fyLJluZxh37",
        "outputId": "ad06b79a-632e-4fd6-d4c9-e227308f126a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'first-commit'...\n",
            "remote: Enumerating objects: 34, done.\u001b[K\n",
            "remote: Counting objects: 100% (34/34), done.\u001b[K\n",
            "remote: Compressing objects: 100% (31/31), done.\u001b[K\n",
            "remote: Total 34 (delta 13), reused 6 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (34/34), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/zakihir0/first-commit.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7KmccXO_g46",
        "outputId": "4a5d7534-ffcd-4675-9694-f2631c897e98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting optuna\n",
            "  Downloading optuna-3.0.4-py3-none-any.whl (348 kB)\n",
            "\u001b[K     |████████████████████████████████| 348 kB 13.5 MB/s \n",
            "\u001b[?25hCollecting alembic>=1.5.0\n",
            "  Downloading alembic-1.8.1-py3-none-any.whl (209 kB)\n",
            "\u001b[K     |████████████████████████████████| 209 kB 61.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.8/dist-packages (from optuna) (1.4.44)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from optuna) (21.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from optuna) (1.21.6)\n",
            "Collecting cliff\n",
            "  Downloading cliff-4.1.0-py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 4.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata<5.0.0 in /usr/local/lib/python3.8/dist-packages (from optuna) (4.13.0)\n",
            "Collecting colorlog\n",
            "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: scipy<1.9.0,>=1.7.0 in /usr/local/lib/python3.8/dist-packages (from optuna) (1.7.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from optuna) (6.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from optuna) (4.64.1)\n",
            "Collecting cmaes>=0.8.2\n",
            "  Downloading cmaes-0.9.0-py3-none-any.whl (23 kB)\n",
            "Collecting Mako\n",
            "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 2.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.8/dist-packages (from alembic>=1.5.0->optuna) (5.10.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata<5.0.0->optuna) (3.11.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->optuna) (3.0.9)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.8/dist-packages (from sqlalchemy>=1.3.0->optuna) (2.0.1)\n",
            "Collecting stevedore>=2.0.1\n",
            "  Downloading stevedore-4.1.1-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 3.5 MB/s \n",
            "\u001b[?25hCollecting autopage>=0.4.0\n",
            "  Downloading autopage-0.5.1-py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.8/dist-packages (from cliff->optuna) (3.5.0)\n",
            "Collecting cmd2>=1.0.0\n",
            "  Downloading cmd2-2.4.2-py3-none-any.whl (147 kB)\n",
            "\u001b[K     |████████████████████████████████| 147 kB 60.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.8/dist-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\n",
            "Collecting pyperclip>=1.6\n",
            "  Downloading pyperclip-1.8.2.tar.gz (20 kB)\n",
            "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.8/dist-packages (from cmd2>=1.0.0->cliff->optuna) (22.1.0)\n",
            "Collecting pbr!=2.1.0,>=2.0.0\n",
            "  Downloading pbr-5.11.0-py2.py3-none-any.whl (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 6.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.8/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.0.1)\n",
            "Building wheels for collected packages: pyperclip\n",
            "  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11136 sha256=f31dc03999369ec666b90192dcef14c0c295bb3c2320da4043dd5063766e6706\n",
            "  Stored in directory: /root/.cache/pip/wheels/7f/1a/65/84ff8c386bec21fca6d220ea1f5498a0367883a78dd5ba6122\n",
            "Successfully built pyperclip\n",
            "Installing collected packages: pyperclip, pbr, stevedore, Mako, cmd2, autopage, colorlog, cmaes, cliff, alembic, optuna\n",
            "Successfully installed Mako-1.2.4 alembic-1.8.1 autopage-0.5.1 cliff-4.1.0 cmaes-0.9.0 cmd2-2.4.2 colorlog-6.7.0 optuna-3.0.4 pbr-5.11.0 pyperclip-1.8.2 stevedore-4.1.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.8/dist-packages (0.90)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from xgboost) (1.7.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from xgboost) (1.21.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install optuna\n",
        "!pip install xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8F1DxAOKhMg8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import (roc_curve, auc)\n",
        "import matplotlib.pyplot as plt\n",
        "from imblearn.under_sampling import CondensedNearestNeighbour\n",
        "from imblearn.under_sampling import EditedNearestNeighbours\n",
        "from collections import Counter\n",
        "import glob\n",
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import optuna\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from imblearn.under_sampling import ClusterCentroids\n",
        "from optuna.integration import lightgbm as lgb\n",
        "from sklearn import metrics\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (PrecisionRecallDisplay, RocCurveDisplay,\n",
        "                             accuracy_score, adjusted_mutual_info_score,\n",
        "                             adjusted_rand_score, auc, average_precision_score,\n",
        "                             balanced_accuracy_score, brier_score_loss,\n",
        "                             calinski_harabasz_score, check_scoring,\n",
        "                             classification_report, cluster, cohen_kappa_score,\n",
        "                             completeness_score, confusion_matrix,\n",
        "                             consensus_score, coverage_error, d2_tweedie_score,\n",
        "                             davies_bouldin_score, dcg_score, det_curve,\n",
        "                             euclidean_distances, explained_variance_score,\n",
        "                             f1_score, fbeta_score, fowlkes_mallows_score,\n",
        "                             get_scorer, hamming_loss, hinge_loss,\n",
        "                             homogeneity_completeness_v_measure,\n",
        "                             homogeneity_score, jaccard_score,\n",
        "                             label_ranking_average_precision_score,\n",
        "                             label_ranking_loss, log_loss, make_scorer,\n",
        "                             matthews_corrcoef, max_error, mean_absolute_error,\n",
        "                             mean_absolute_percentage_error,\n",
        "                             mean_gamma_deviance, mean_pinball_loss,\n",
        "                             mean_poisson_deviance, mean_squared_error,\n",
        "                             mean_squared_log_error, mean_tweedie_deviance,\n",
        "                             median_absolute_error,\n",
        "                             multilabel_confusion_matrix, mutual_info_score,\n",
        "                             nan_euclidean_distances, ndcg_score,\n",
        "                             normalized_mutual_info_score,\n",
        "                             pair_confusion_matrix, pairwise_distances,\n",
        "                             pairwise_distances_argmin,\n",
        "                             pairwise_distances_argmin_min,\n",
        "                             pairwise_distances_chunked, pairwise_kernels,\n",
        "                             plot_confusion_matrix, plot_det_curve,\n",
        "                             plot_precision_recall_curve, plot_roc_curve,\n",
        "                             precision_recall_curve,\n",
        "                             precision_recall_fscore_support, precision_score,\n",
        "                             r2_score, rand_score, recall_score, roc_auc_score,\n",
        "                             roc_curve, silhouette_samples, silhouette_score,\n",
        "                             top_k_accuracy_score, v_measure_score,\n",
        "                             zero_one_loss)\n",
        "from sklearn.model_selection import (BaseCrossValidator, BaseShuffleSplit,\n",
        "                                     GridSearchCV, GroupKFold,\n",
        "                                     GroupShuffleSplit, KFold,\n",
        "                                     LeaveOneGroupOut, LeaveOneOut,\n",
        "                                     LeavePGroupsOut, LeavePOut,\n",
        "                                     PredefinedSplit, RepeatedKFold,\n",
        "                                     RepeatedStratifiedKFold, ShuffleSplit,\n",
        "                                     StratifiedGroupKFold, StratifiedKFold,\n",
        "                                     StratifiedShuffleSplit,\n",
        "                                     check_cv, cross_val_predict,\n",
        "                                     cross_val_score, cross_validate,\n",
        "                                     learning_curve, permutation_test_score,\n",
        "                                     train_test_split, validation_curve)\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.manifold import TSNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rQCSsW1LxXo6"
      },
      "outputs": [],
      "source": [
        "test = pd.read_csv(\"https://raw.githubusercontent.com/zakihir0/first-commit/main/test.csv\")\n",
        "train = pd.read_csv(\"https://raw.githubusercontent.com/zakihir0/first-commit/main/train.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "EDS52sgYi3Ny"
      },
      "outputs": [],
      "source": [
        "colname = ['HomePlanet', 'CryoSleep', 'Cabin', 'Destination', 'Age', 'VIP', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'Transported']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "dBioqMV0xXo7"
      },
      "outputs": [],
      "source": [
        "df = train[colname]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 679
        },
        "id": "wqctvFzsxXo7",
        "outputId": "0fa48647-8471-419b-c873-cef754cefff4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "df_obj_colname: Index(['HomePlanet', 'CryoSleep', 'Cabin', 'Destination', 'VIP'], dtype='object')\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-c19429c6-c941-4b4f-ac66-2e5a12740cf2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PassengerId</th>\n",
              "      <th>HomePlanet</th>\n",
              "      <th>CryoSleep</th>\n",
              "      <th>Cabin</th>\n",
              "      <th>Destination</th>\n",
              "      <th>Age</th>\n",
              "      <th>VIP</th>\n",
              "      <th>RoomService</th>\n",
              "      <th>FoodCourt</th>\n",
              "      <th>ShoppingMall</th>\n",
              "      <th>Spa</th>\n",
              "      <th>VRDeck</th>\n",
              "      <th>Name</th>\n",
              "      <th>Transported</th>\n",
              "      <th>CabinDeck</th>\n",
              "      <th>CabinNum</th>\n",
              "      <th>CabinSide</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0001_01</td>\n",
              "      <td>Europa</td>\n",
              "      <td>False</td>\n",
              "      <td>B/0/P</td>\n",
              "      <td>TRAPPIST-1e</td>\n",
              "      <td>39.0</td>\n",
              "      <td>False</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Maham Ofracculy</td>\n",
              "      <td>False</td>\n",
              "      <td>B</td>\n",
              "      <td>0</td>\n",
              "      <td>P</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0002_01</td>\n",
              "      <td>Earth</td>\n",
              "      <td>False</td>\n",
              "      <td>F/0/S</td>\n",
              "      <td>TRAPPIST-1e</td>\n",
              "      <td>24.0</td>\n",
              "      <td>False</td>\n",
              "      <td>109.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>549.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>Juanna Vines</td>\n",
              "      <td>True</td>\n",
              "      <td>F</td>\n",
              "      <td>0</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0003_01</td>\n",
              "      <td>Europa</td>\n",
              "      <td>False</td>\n",
              "      <td>A/0/S</td>\n",
              "      <td>TRAPPIST-1e</td>\n",
              "      <td>58.0</td>\n",
              "      <td>True</td>\n",
              "      <td>43.0</td>\n",
              "      <td>3576.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6715.0</td>\n",
              "      <td>49.0</td>\n",
              "      <td>Altark Susent</td>\n",
              "      <td>False</td>\n",
              "      <td>A</td>\n",
              "      <td>0</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0003_02</td>\n",
              "      <td>Europa</td>\n",
              "      <td>False</td>\n",
              "      <td>A/0/S</td>\n",
              "      <td>TRAPPIST-1e</td>\n",
              "      <td>33.0</td>\n",
              "      <td>False</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1283.0</td>\n",
              "      <td>371.0</td>\n",
              "      <td>3329.0</td>\n",
              "      <td>193.0</td>\n",
              "      <td>Solam Susent</td>\n",
              "      <td>False</td>\n",
              "      <td>A</td>\n",
              "      <td>0</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0004_01</td>\n",
              "      <td>Earth</td>\n",
              "      <td>False</td>\n",
              "      <td>F/1/S</td>\n",
              "      <td>TRAPPIST-1e</td>\n",
              "      <td>16.0</td>\n",
              "      <td>False</td>\n",
              "      <td>303.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>151.0</td>\n",
              "      <td>565.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>Willy Santantines</td>\n",
              "      <td>True</td>\n",
              "      <td>F</td>\n",
              "      <td>1</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8688</th>\n",
              "      <td>9276_01</td>\n",
              "      <td>Europa</td>\n",
              "      <td>False</td>\n",
              "      <td>A/98/P</td>\n",
              "      <td>55 Cancri e</td>\n",
              "      <td>41.0</td>\n",
              "      <td>True</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6819.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1643.0</td>\n",
              "      <td>74.0</td>\n",
              "      <td>Gravior Noxnuther</td>\n",
              "      <td>False</td>\n",
              "      <td>A</td>\n",
              "      <td>98</td>\n",
              "      <td>P</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8689</th>\n",
              "      <td>9278_01</td>\n",
              "      <td>Earth</td>\n",
              "      <td>True</td>\n",
              "      <td>G/1499/S</td>\n",
              "      <td>PSO J318.5-22</td>\n",
              "      <td>18.0</td>\n",
              "      <td>False</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Kurta Mondalley</td>\n",
              "      <td>False</td>\n",
              "      <td>G</td>\n",
              "      <td>1499</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8690</th>\n",
              "      <td>9279_01</td>\n",
              "      <td>Earth</td>\n",
              "      <td>False</td>\n",
              "      <td>G/1500/S</td>\n",
              "      <td>TRAPPIST-1e</td>\n",
              "      <td>26.0</td>\n",
              "      <td>False</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1872.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Fayey Connon</td>\n",
              "      <td>True</td>\n",
              "      <td>G</td>\n",
              "      <td>1500</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8691</th>\n",
              "      <td>9280_01</td>\n",
              "      <td>Europa</td>\n",
              "      <td>False</td>\n",
              "      <td>E/608/S</td>\n",
              "      <td>55 Cancri e</td>\n",
              "      <td>32.0</td>\n",
              "      <td>False</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1049.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>353.0</td>\n",
              "      <td>3235.0</td>\n",
              "      <td>Celeon Hontichre</td>\n",
              "      <td>False</td>\n",
              "      <td>E</td>\n",
              "      <td>608</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8692</th>\n",
              "      <td>9280_02</td>\n",
              "      <td>Europa</td>\n",
              "      <td>False</td>\n",
              "      <td>E/608/S</td>\n",
              "      <td>TRAPPIST-1e</td>\n",
              "      <td>44.0</td>\n",
              "      <td>False</td>\n",
              "      <td>126.0</td>\n",
              "      <td>4688.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>Propsh Hontichre</td>\n",
              "      <td>True</td>\n",
              "      <td>E</td>\n",
              "      <td>608</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8693 rows × 17 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c19429c6-c941-4b4f-ac66-2e5a12740cf2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c19429c6-c941-4b4f-ac66-2e5a12740cf2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c19429c6-c941-4b4f-ac66-2e5a12740cf2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     PassengerId HomePlanet CryoSleep     Cabin    Destination   Age    VIP  \\\n",
              "0        0001_01     Europa     False     B/0/P    TRAPPIST-1e  39.0  False   \n",
              "1        0002_01      Earth     False     F/0/S    TRAPPIST-1e  24.0  False   \n",
              "2        0003_01     Europa     False     A/0/S    TRAPPIST-1e  58.0   True   \n",
              "3        0003_02     Europa     False     A/0/S    TRAPPIST-1e  33.0  False   \n",
              "4        0004_01      Earth     False     F/1/S    TRAPPIST-1e  16.0  False   \n",
              "...          ...        ...       ...       ...            ...   ...    ...   \n",
              "8688     9276_01     Europa     False    A/98/P    55 Cancri e  41.0   True   \n",
              "8689     9278_01      Earth      True  G/1499/S  PSO J318.5-22  18.0  False   \n",
              "8690     9279_01      Earth     False  G/1500/S    TRAPPIST-1e  26.0  False   \n",
              "8691     9280_01     Europa     False   E/608/S    55 Cancri e  32.0  False   \n",
              "8692     9280_02     Europa     False   E/608/S    TRAPPIST-1e  44.0  False   \n",
              "\n",
              "      RoomService  FoodCourt  ShoppingMall     Spa  VRDeck               Name  \\\n",
              "0             0.0        0.0           0.0     0.0     0.0    Maham Ofracculy   \n",
              "1           109.0        9.0          25.0   549.0    44.0       Juanna Vines   \n",
              "2            43.0     3576.0           0.0  6715.0    49.0      Altark Susent   \n",
              "3             0.0     1283.0         371.0  3329.0   193.0       Solam Susent   \n",
              "4           303.0       70.0         151.0   565.0     2.0  Willy Santantines   \n",
              "...           ...        ...           ...     ...     ...                ...   \n",
              "8688          0.0     6819.0           0.0  1643.0    74.0  Gravior Noxnuther   \n",
              "8689          0.0        0.0           0.0     0.0     0.0    Kurta Mondalley   \n",
              "8690          0.0        0.0        1872.0     1.0     0.0       Fayey Connon   \n",
              "8691          0.0     1049.0           0.0   353.0  3235.0   Celeon Hontichre   \n",
              "8692        126.0     4688.0           0.0     0.0    12.0   Propsh Hontichre   \n",
              "\n",
              "      Transported CabinDeck CabinNum CabinSide  \n",
              "0           False         B        0         P  \n",
              "1            True         F        0         S  \n",
              "2           False         A        0         S  \n",
              "3           False         A        0         S  \n",
              "4            True         F        1         S  \n",
              "...           ...       ...      ...       ...  \n",
              "8688        False         A       98         P  \n",
              "8689        False         G     1499         S  \n",
              "8690         True         G     1500         S  \n",
              "8691        False         E      608         S  \n",
              "8692         True         E      608         S  \n",
              "\n",
              "[8693 rows x 17 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "df_obj_colname = df.columns[df.dtypes == \"object\"]\n",
        "print(f\"df_obj_colname: {df_obj_colname}\")\n",
        "\n",
        "train[['CabinDeck','CabinNum','CabinSide']] = train['Cabin'].str.split('/', expand=True)\n",
        "test[['CabinDeck','CabinNum','CabinSide']] = test['Cabin'].str.split('/', expand=True)\n",
        "display(train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        },
        "id": "LxpeYI4GZJef",
        "outputId": "f41aeaa2-ed5b-4bbc-fcbc-4fe0d5b1f341"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CabinDeck\n",
            "A     256\n",
            "B     779\n",
            "C     747\n",
            "D     478\n",
            "E     876\n",
            "F    2794\n",
            "G    2559\n",
            "T       5\n",
            "Name: PassengerId, dtype: int64\n",
            "CabinSide\n",
            "P    4206\n",
            "S    4288\n",
            "Name: PassengerId, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "plt.hist(train.groupby(by=[\"CabinNum\"])[\"PassengerId\"].count())\n",
        "plt.title(\"CabinNum-PassengerID\")\n",
        "plt.show()\n",
        "\n",
        "train[\"CabinNum\"] = pd.to_numeric(train[\"CabinNum\"])\n",
        "test[\"CabinNum\"] = pd.to_numeric(test[\"CabinNum\"])\n",
        "\n",
        "print(train.groupby(by=[\"CabinDeck\"])[\"PassengerId\"].count())\n",
        "\n",
        "print(train.groupby(by=[\"CabinSide\"])[\"PassengerId\"].count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        },
        "id": "3yRhDjM6ZFvG",
        "outputId": "ed75845a-e57d-4c9a-c7f1-68e5024fff68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_dummy_colname: Index(['HomePlanet', 'CryoSleep', 'Destination', 'VIP', 'CabinDeck',\n",
            "       'CabinNum', 'CabinSide'],\n",
            "      dtype='object')\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-3c8c2b73-ecc2-4c66-a756-87c07d761d6f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>HomePlanet</th>\n",
              "      <th>CryoSleep</th>\n",
              "      <th>Destination</th>\n",
              "      <th>Age</th>\n",
              "      <th>VIP</th>\n",
              "      <th>RoomService</th>\n",
              "      <th>FoodCourt</th>\n",
              "      <th>ShoppingMall</th>\n",
              "      <th>Spa</th>\n",
              "      <th>VRDeck</th>\n",
              "      <th>Name</th>\n",
              "      <th>Transported</th>\n",
              "      <th>CabinDeck</th>\n",
              "      <th>CabinNum</th>\n",
              "      <th>CabinSide</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Europa</td>\n",
              "      <td>False</td>\n",
              "      <td>TRAPPIST-1e</td>\n",
              "      <td>39.0</td>\n",
              "      <td>False</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Maham Ofracculy</td>\n",
              "      <td>False</td>\n",
              "      <td>B</td>\n",
              "      <td>0.0</td>\n",
              "      <td>P</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Earth</td>\n",
              "      <td>False</td>\n",
              "      <td>TRAPPIST-1e</td>\n",
              "      <td>24.0</td>\n",
              "      <td>False</td>\n",
              "      <td>109.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>549.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>Juanna Vines</td>\n",
              "      <td>True</td>\n",
              "      <td>F</td>\n",
              "      <td>0.0</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Europa</td>\n",
              "      <td>False</td>\n",
              "      <td>TRAPPIST-1e</td>\n",
              "      <td>58.0</td>\n",
              "      <td>True</td>\n",
              "      <td>43.0</td>\n",
              "      <td>3576.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6715.0</td>\n",
              "      <td>49.0</td>\n",
              "      <td>Altark Susent</td>\n",
              "      <td>False</td>\n",
              "      <td>A</td>\n",
              "      <td>0.0</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Europa</td>\n",
              "      <td>False</td>\n",
              "      <td>TRAPPIST-1e</td>\n",
              "      <td>33.0</td>\n",
              "      <td>False</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1283.0</td>\n",
              "      <td>371.0</td>\n",
              "      <td>3329.0</td>\n",
              "      <td>193.0</td>\n",
              "      <td>Solam Susent</td>\n",
              "      <td>False</td>\n",
              "      <td>A</td>\n",
              "      <td>0.0</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Earth</td>\n",
              "      <td>False</td>\n",
              "      <td>TRAPPIST-1e</td>\n",
              "      <td>16.0</td>\n",
              "      <td>False</td>\n",
              "      <td>303.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>151.0</td>\n",
              "      <td>565.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>Willy Santantines</td>\n",
              "      <td>True</td>\n",
              "      <td>F</td>\n",
              "      <td>1.0</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8688</th>\n",
              "      <td>Europa</td>\n",
              "      <td>False</td>\n",
              "      <td>55 Cancri e</td>\n",
              "      <td>41.0</td>\n",
              "      <td>True</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6819.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1643.0</td>\n",
              "      <td>74.0</td>\n",
              "      <td>Gravior Noxnuther</td>\n",
              "      <td>False</td>\n",
              "      <td>A</td>\n",
              "      <td>98.0</td>\n",
              "      <td>P</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8689</th>\n",
              "      <td>Earth</td>\n",
              "      <td>True</td>\n",
              "      <td>PSO J318.5-22</td>\n",
              "      <td>18.0</td>\n",
              "      <td>False</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Kurta Mondalley</td>\n",
              "      <td>False</td>\n",
              "      <td>G</td>\n",
              "      <td>1499.0</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8690</th>\n",
              "      <td>Earth</td>\n",
              "      <td>False</td>\n",
              "      <td>TRAPPIST-1e</td>\n",
              "      <td>26.0</td>\n",
              "      <td>False</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1872.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Fayey Connon</td>\n",
              "      <td>True</td>\n",
              "      <td>G</td>\n",
              "      <td>1500.0</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8691</th>\n",
              "      <td>Europa</td>\n",
              "      <td>False</td>\n",
              "      <td>55 Cancri e</td>\n",
              "      <td>32.0</td>\n",
              "      <td>False</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1049.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>353.0</td>\n",
              "      <td>3235.0</td>\n",
              "      <td>Celeon Hontichre</td>\n",
              "      <td>False</td>\n",
              "      <td>E</td>\n",
              "      <td>608.0</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8692</th>\n",
              "      <td>Europa</td>\n",
              "      <td>False</td>\n",
              "      <td>TRAPPIST-1e</td>\n",
              "      <td>44.0</td>\n",
              "      <td>False</td>\n",
              "      <td>126.0</td>\n",
              "      <td>4688.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>Propsh Hontichre</td>\n",
              "      <td>True</td>\n",
              "      <td>E</td>\n",
              "      <td>608.0</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6606 rows × 15 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3c8c2b73-ecc2-4c66-a756-87c07d761d6f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3c8c2b73-ecc2-4c66-a756-87c07d761d6f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3c8c2b73-ecc2-4c66-a756-87c07d761d6f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     HomePlanet CryoSleep    Destination   Age    VIP  RoomService  FoodCourt  \\\n",
              "0        Europa     False    TRAPPIST-1e  39.0  False          0.0        0.0   \n",
              "1         Earth     False    TRAPPIST-1e  24.0  False        109.0        9.0   \n",
              "2        Europa     False    TRAPPIST-1e  58.0   True         43.0     3576.0   \n",
              "3        Europa     False    TRAPPIST-1e  33.0  False          0.0     1283.0   \n",
              "4         Earth     False    TRAPPIST-1e  16.0  False        303.0       70.0   \n",
              "...         ...       ...            ...   ...    ...          ...        ...   \n",
              "8688     Europa     False    55 Cancri e  41.0   True          0.0     6819.0   \n",
              "8689      Earth      True  PSO J318.5-22  18.0  False          0.0        0.0   \n",
              "8690      Earth     False    TRAPPIST-1e  26.0  False          0.0        0.0   \n",
              "8691     Europa     False    55 Cancri e  32.0  False          0.0     1049.0   \n",
              "8692     Europa     False    TRAPPIST-1e  44.0  False        126.0     4688.0   \n",
              "\n",
              "      ShoppingMall     Spa  VRDeck               Name  Transported CabinDeck  \\\n",
              "0              0.0     0.0     0.0    Maham Ofracculy        False         B   \n",
              "1             25.0   549.0    44.0       Juanna Vines         True         F   \n",
              "2              0.0  6715.0    49.0      Altark Susent        False         A   \n",
              "3            371.0  3329.0   193.0       Solam Susent        False         A   \n",
              "4            151.0   565.0     2.0  Willy Santantines         True         F   \n",
              "...            ...     ...     ...                ...          ...       ...   \n",
              "8688           0.0  1643.0    74.0  Gravior Noxnuther        False         A   \n",
              "8689           0.0     0.0     0.0    Kurta Mondalley        False         G   \n",
              "8690        1872.0     1.0     0.0       Fayey Connon         True         G   \n",
              "8691           0.0   353.0  3235.0   Celeon Hontichre        False         E   \n",
              "8692           0.0     0.0    12.0   Propsh Hontichre         True         E   \n",
              "\n",
              "      CabinNum CabinSide  \n",
              "0          0.0         P  \n",
              "1          0.0         S  \n",
              "2          0.0         S  \n",
              "3          0.0         S  \n",
              "4          1.0         S  \n",
              "...        ...       ...  \n",
              "8688      98.0         P  \n",
              "8689    1499.0         S  \n",
              "8690    1500.0         S  \n",
              "8691     608.0         S  \n",
              "8692     608.0         S  \n",
              "\n",
              "[6606 rows x 15 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "train = train.drop(\"Cabin\", axis=1)\n",
        "test = test.drop(\"Cabin\", axis=1)\n",
        "\n",
        "train_dummy_colname = train[['HomePlanet', 'CryoSleep', 'Destination', 'VIP', \"CabinDeck\", \"CabinNum\", \"CabinSide\"]].columns\n",
        "print(f\"train_dummy_colname: {train_dummy_colname}\")\n",
        "\n",
        "train_drop = train.drop(\"PassengerId\", axis=1)\n",
        "train_drop.dropna(inplace=True)\n",
        "\n",
        "display(train_drop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHta1Upvtp89",
        "outputId": "9651ff7a-dfb7-4e55-91b7-7a43f246973b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[array(['Europa', 'Earth', 'Mars', nan], dtype=object),\n",
              " array([False, True, nan], dtype=object),\n",
              " array(['B/0/P', 'F/0/S', 'A/0/S', ..., 'G/1499/S', 'G/1500/S', 'E/608/S'],\n",
              "       dtype=object),\n",
              " array(['TRAPPIST-1e', 'PSO J318.5-22', '55 Cancri e', nan], dtype=object),\n",
              " array([39., 24., 58., 33., 16., 44., 26., 28., 35., 14., 34., 45., 32.,\n",
              "        48., 31., 27.,  0.,  1., 49., 29., 10.,  7., 21., 62., 15., 43.,\n",
              "        47.,  2., 20., 23., 30., 17., 55.,  4., 19., 56., nan, 25., 38.,\n",
              "        36., 22., 18., 42., 37., 13.,  8., 40.,  3., 54.,  9.,  6., 64.,\n",
              "        67., 61., 50., 41., 57., 11., 52., 51., 46., 60., 63., 59.,  5.,\n",
              "        79., 68., 74., 12., 53., 65., 71., 75., 70., 76., 78., 73., 66.,\n",
              "        69., 72., 77.])]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_data_list = []\n",
        "for i in range(len(df_obj_colname)):\n",
        "  df_colname = df.columns[i]\n",
        "  df_data = pd.unique(df[df_colname])\n",
        "  df_data_list.append(df_data)\n",
        "df_data_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "sDYlrNn96iCB"
      },
      "outputs": [],
      "source": [
        "dataset = pd.get_dummies(train_drop[train_dummy_colname])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "lt9R4FDTxXo8"
      },
      "outputs": [],
      "source": [
        "y = train[\"Transported\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPSmjbhDxXo8",
        "outputId": "1d35c976-0733-45d9-aab4-c094e4ba5bf7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Int64Index([   0,    1,    2,    3,    4,    5,    6,    8,    9,   11,\n",
              "            ...\n",
              "            8681, 8682, 8683, 8685, 8686, 8688, 8689, 8690, 8691, 8692],\n",
              "           dtype='int64', length=6606)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset.index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "id": "I69NVGyTxXo9",
        "outputId": "20288688-3030-4137-9227-2a6a412c7589"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-adafa42d-199d-432e-8b50-7025ccf5b1a1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CabinNum</th>\n",
              "      <th>HomePlanet_Earth</th>\n",
              "      <th>HomePlanet_Europa</th>\n",
              "      <th>HomePlanet_Mars</th>\n",
              "      <th>CryoSleep_False</th>\n",
              "      <th>CryoSleep_True</th>\n",
              "      <th>Destination_55 Cancri e</th>\n",
              "      <th>Destination_PSO J318.5-22</th>\n",
              "      <th>Destination_TRAPPIST-1e</th>\n",
              "      <th>VIP_False</th>\n",
              "      <th>...</th>\n",
              "      <th>CabinDeck_A</th>\n",
              "      <th>CabinDeck_B</th>\n",
              "      <th>CabinDeck_C</th>\n",
              "      <th>CabinDeck_D</th>\n",
              "      <th>CabinDeck_E</th>\n",
              "      <th>CabinDeck_F</th>\n",
              "      <th>CabinDeck_G</th>\n",
              "      <th>CabinDeck_T</th>\n",
              "      <th>CabinSide_P</th>\n",
              "      <th>CabinSide_S</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8688</th>\n",
              "      <td>98.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8689</th>\n",
              "      <td>1499.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8690</th>\n",
              "      <td>1500.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8691</th>\n",
              "      <td>608.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8692</th>\n",
              "      <td>608.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6606 rows × 21 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-adafa42d-199d-432e-8b50-7025ccf5b1a1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-adafa42d-199d-432e-8b50-7025ccf5b1a1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-adafa42d-199d-432e-8b50-7025ccf5b1a1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      CabinNum  HomePlanet_Earth  HomePlanet_Europa  HomePlanet_Mars  \\\n",
              "0          0.0                 0                  1                0   \n",
              "1          0.0                 1                  0                0   \n",
              "2          0.0                 0                  1                0   \n",
              "3          0.0                 0                  1                0   \n",
              "4          1.0                 1                  0                0   \n",
              "...        ...               ...                ...              ...   \n",
              "8688      98.0                 0                  1                0   \n",
              "8689    1499.0                 1                  0                0   \n",
              "8690    1500.0                 1                  0                0   \n",
              "8691     608.0                 0                  1                0   \n",
              "8692     608.0                 0                  1                0   \n",
              "\n",
              "      CryoSleep_False  CryoSleep_True  Destination_55 Cancri e  \\\n",
              "0                   1               0                        0   \n",
              "1                   1               0                        0   \n",
              "2                   1               0                        0   \n",
              "3                   1               0                        0   \n",
              "4                   1               0                        0   \n",
              "...               ...             ...                      ...   \n",
              "8688                1               0                        1   \n",
              "8689                0               1                        0   \n",
              "8690                1               0                        0   \n",
              "8691                1               0                        1   \n",
              "8692                1               0                        0   \n",
              "\n",
              "      Destination_PSO J318.5-22  Destination_TRAPPIST-1e  VIP_False  ...  \\\n",
              "0                             0                        1          1  ...   \n",
              "1                             0                        1          1  ...   \n",
              "2                             0                        1          0  ...   \n",
              "3                             0                        1          1  ...   \n",
              "4                             0                        1          1  ...   \n",
              "...                         ...                      ...        ...  ...   \n",
              "8688                          0                        0          0  ...   \n",
              "8689                          1                        0          1  ...   \n",
              "8690                          0                        1          1  ...   \n",
              "8691                          0                        0          1  ...   \n",
              "8692                          0                        1          1  ...   \n",
              "\n",
              "      CabinDeck_A  CabinDeck_B  CabinDeck_C  CabinDeck_D  CabinDeck_E  \\\n",
              "0               0            1            0            0            0   \n",
              "1               0            0            0            0            0   \n",
              "2               1            0            0            0            0   \n",
              "3               1            0            0            0            0   \n",
              "4               0            0            0            0            0   \n",
              "...           ...          ...          ...          ...          ...   \n",
              "8688            1            0            0            0            0   \n",
              "8689            0            0            0            0            0   \n",
              "8690            0            0            0            0            0   \n",
              "8691            0            0            0            0            1   \n",
              "8692            0            0            0            0            1   \n",
              "\n",
              "      CabinDeck_F  CabinDeck_G  CabinDeck_T  CabinSide_P  CabinSide_S  \n",
              "0               0            0            0            1            0  \n",
              "1               1            0            0            0            1  \n",
              "2               0            0            0            0            1  \n",
              "3               0            0            0            0            1  \n",
              "4               1            0            0            0            1  \n",
              "...           ...          ...          ...          ...          ...  \n",
              "8688            0            0            0            1            0  \n",
              "8689            0            1            0            0            1  \n",
              "8690            0            1            0            0            1  \n",
              "8691            0            0            0            0            1  \n",
              "8692            0            0            0            0            1  \n",
              "\n",
              "[6606 rows x 21 columns]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uxfr8HttxXo9"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mカーネルを起動できませんでした。 \n",
            "\u001b[1;31mTraceback (most recent call last):\n",
            "\u001b[1;31m  File \"/Users/hiroto/Library/Python/3.9/lib/python/site-packages/traitlets/config/application.py\", line 113, in inner\n",
            "\u001b[1;31m    return method(app, *args, **kwargs)\n",
            "\u001b[1;31m  File \"/Users/hiroto/Library/Python/3.9/lib/python/site-packages/traitlets/config/application.py\", line 834, in parse_command_line\n",
            "\u001b[1;31m    self.update_config(self.cli_config)\n",
            "\u001b[1;31m  File \"/Users/hiroto/Library/Python/3.9/lib/python/site-packages/traitlets/config/configurable.py\", line 231, in update_config\n",
            "\u001b[1;31m    self._load_config(config)\n",
            "\u001b[1;31m  File \"/Users/hiroto/Library/Python/3.9/lib/python/site-packages/traitlets/config/configurable.py\", line 201, in _load_config\n",
            "\u001b[1;31m    warn(msg)\n",
            "\u001b[1;31m  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/contextlib.py\", line 124, in __exit__\n",
            "\u001b[1;31m    next(self.gen)\n",
            "\u001b[1;31m  File \"/Users/hiroto/Library/Python/3.9/lib/python/site-packages/traitlets/traitlets.py\", line 1483, in hold_trait_notifications\n",
            "\u001b[1;31m    raise e\n",
            "\u001b[1;31m  File \"/Users/hiroto/Library/Python/3.9/lib/python/site-packages/traitlets/traitlets.py\", line 1469, in hold_trait_notifications\n",
            "\u001b[1;31m    value = trait._cross_validate(self, getattr(self, name))\n",
            "\u001b[1;31m  File \"/Users/hiroto/Library/Python/3.9/lib/python/site-packages/traitlets/traitlets.py\", line 743, in _cross_validate\n",
            "\u001b[1;31m    value = obj._trait_validators<a href='obj, proposal'>self.name</a>\n",
            "\u001b[1;31m  File \"/Users/hiroto/Library/Python/3.9/lib/python/site-packages/traitlets/traitlets.py\", line 1223, in __call__\n",
            "\u001b[1;31m    return self.func(*args, **kwargs)\n",
            "\u001b[1;31m  File \"/Users/hiroto/Library/Python/3.9/lib/python/site-packages/notebook/notebookapp.py\", line 1523, in _notebook_dir_validate\n",
            "\u001b[1;31m    raise TraitError(trans.gettext(\"No such notebook dir: '%r'\") % value)\n",
            "\u001b[1;31mtraitlets.traitlets.TraitError: No such notebook dir: ''/zakihir0/first-commit''\n",
            "\u001b[1;31m\n",
            "\u001b[1;31mDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31m\n",
            "\u001b[1;31mTraceback (most recent call last):\n",
            "\u001b[1;31m  File \"/Users/hiroto/.vscode/extensions/ms-toolsai.jupyter-2022.9.1303220346/pythonFiles/vscode_datascience_helpers/daemon/daemon_python.py\", line 54, in _decorator\n",
            "\u001b[1;31m    return func(self, *args, **kwargs)\n",
            "\u001b[1;31m  File \"/Users/hiroto/.vscode/extensions/ms-toolsai.jupyter-2022.9.1303220346/pythonFiles/vscode_datascience_helpers/jupyter_daemon.py\", line 111, in m_exec_module_observable\n",
            "\u001b[1;31m    self._start_notebook(args, cwd, env)\n",
            "\u001b[1;31m  File \"/Users/hiroto/.vscode/extensions/ms-toolsai.jupyter-2022.9.1303220346/pythonFiles/vscode_datascience_helpers/jupyter_daemon.py\", line 168, in _start_notebook\n",
            "\u001b[1;31m    app.launch_new_instance()\n",
            "\u001b[1;31m  File \"/Users/hiroto/Library/Python/3.9/lib/python/site-packages/jupyter_core/application.py\", line 276, in launch_instance\n",
            "\u001b[1;31m    return super().launch_instance(argv=argv, **kwargs)\n",
            "\u001b[1;31m  File \"/Users/hiroto/Library/Python/3.9/lib/python/site-packages/traitlets/config/application.py\", line 991, in launch_instance\n",
            "\u001b[1;31m    app.initialize(argv)\n",
            "\u001b[1;31m  File \"/Users/hiroto/Library/Python/3.9/lib/python/site-packages/traitlets/config/application.py\", line 113, in inner\n",
            "\u001b[1;31m    return method(app, *args, **kwargs)\n",
            "\u001b[1;31m  File \"/Users/hiroto/Library/Python/3.9/lib/python/site-packages/notebook/notebookapp.py\", line 2142, in initialize\n",
            "\u001b[1;31m    super().initialize(argv)\n",
            "\u001b[1;31m  File \"/Users/hiroto/Library/Python/3.9/lib/python/site-packages/traitlets/config/application.py\", line 113, in inner\n",
            "\u001b[1;31m    return method(app, *args, **kwargs)\n",
            "\u001b[1;31m  File \"/Users/hiroto/Library/Python/3.9/lib/python/site-packages/jupyter_core/application.py\", line 247, in initialize\n",
            "\u001b[1;31m    self.parse_command_line(argv)\n",
            "\u001b[1;31m  File \"/Users/hiroto/Library/Python/3.9/lib/python/site-packages/notebook/notebookapp.py\", line 1612, in parse_command_line\n",
            "\u001b[1;31m    super().parse_command_line(argv)\n",
            "\u001b[1;31m  File \"/Users/hiroto/Library/Python/3.9/lib/python/site-packages/traitlets/config/application.py\", line 117, in inner\n",
            "\u001b[1;31m    app.exit(1)\n",
            "\u001b[1;31m  File \"/Users/hiroto/Library/Python/3.9/lib/python/site-packages/traitlets/config/application.py\", line 979, in exit\n",
            "\u001b[1;31m    sys.exit(exit_status)\n",
            "\u001b[1;31mSystemExit: 1\n",
            "\u001b[1;31m\n",
            "\u001b[1;31m[C 12:15:42.065 NotebookApp] Bad config encountered during initialization: No such notebook dir: ''/zakihir0/first-commit''\n",
            "\u001b[1;31m\n",
            "\u001b[1;31mFailed to run jupyter as observable with args notebook --no-browser --notebook-dir=\"/zakihir0/first-commit\" --config=/var/folders/gf/vx91cl6n2zz1lp38lfq9ds4c0000gn/T/d59c3383-36e1-4d89-b057-56755d0e5c80/jupyter_notebook_config.py --NotebookApp.iopub_data_rate_limit=10000000000.0"
          ]
        }
      ],
      "source": [
        "y = y.iloc[dataset.index, ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mカーネルを起動できませんでした。 \n",
            "\u001b[1;31mTraceback (most recent call last):\n",
            "\u001b[1;31m  File \"/Users/hiroto/Library/Python/3.9/lib/python/site-packages/traitlets/config/application.py\", line 113, in inner\n",
            "\u001b[1;31m    return method(app, *args, **kwargs)\n",
            "\u001b[1;31m  File \"/Users/hiroto/Library/Python/3.9/lib/python/site-packages/traitlets/config/application.py\", line 834, in parse_command_line\n",
            "\u001b[1;31m    self.update_config(self.cli_config)\n",
            "\u001b[1;31m  File \"/Users/hiroto/Library/Python/3.9/lib/python/site-packages/traitlets/config/configurable.py\", line 231, in update_config\n",
            "\u001b[1;31m    self._load_config(config)\n",
            "\u001b[1;31m  File \"/Users/hiroto/Library/Python/3.9/lib/python/site-packages/traitlets/config/configurable.py\", line 201, in _load_config\n",
            "\u001b[1;31m    warn(msg)\n",
            "\u001b[1;31m  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/contextlib.py\", line 124, in __exit__\n",
            "\u001b[1;31m    next(self.gen)\n",
            "\u001b[1;31m  File \"/Users/hiroto/Library/Python/3.9/lib/python/site-packages/traitlets/traitlets.py\", line 1483, in hold_trait_notifications\n",
            "\u001b[1;31m    raise e\n",
            "\u001b[1;31m  File \"/Users/hiroto/Library/Python/3.9/lib/python/site-packages/traitlets/traitlets.py\", line 1469, in hold_trait_notifications\n",
            "\u001b[1;31m    value = trait._cross_validate(self, getattr(self, name))\n",
            "\u001b[1;31m  File \"/Users/hiroto/Library/Python/3.9/lib/python/site-packages/traitlets/traitlets.py\", line 743, in _cross_validate\n",
            "\u001b[1;31m    value = obj._trait_validators<a href='obj, proposal'>self.name</a>\n",
            "\u001b[1;31m  File \"/Users/hiroto/Library/Python/3.9/lib/python/site-packages/traitlets/traitlets.py\", line 1223, in __call__\n",
            "\u001b[1;31m    return self.func(*args, **kwargs)\n",
            "\u001b[1;31m  File \"/Users/hiroto/Library/Python/3.9/lib/python/site-packages/notebook/notebookapp.py\", line 1523, in _notebook_dir_validate\n",
            "\u001b[1;31m    raise TraitError(trans.gettext(\"No such notebook dir: '%r'\") % value)\n",
            "\u001b[1;31mtraitlets.traitlets.TraitError: No such notebook dir: ''/zakihir0/first-commit''\n",
            "\u001b[1;31m\n",
            "\u001b[1;31mDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31m\n",
            "\u001b[1;31mTraceback (most recent call last):\n",
            "\u001b[1;31m  File \"/Users/hiroto/.vscode/extensions/ms-toolsai.jupyter-2022.9.1303220346/pythonFiles/vscode_datascience_helpers/daemon/daemon_python.py\", line 54, in _decorator\n",
            "\u001b[1;31m    return func(self, *args, **kwargs)\n",
            "\u001b[1;31m  File \"/Users/hiroto/.vscode/extensions/ms-toolsai.jupyter-2022.9.1303220346/pythonFiles/vscode_datascience_helpers/jupyter_daemon.py\", line 111, in m_exec_module_observable\n",
            "\u001b[1;31m    self._start_notebook(args, cwd, env)\n",
            "\u001b[1;31m  File \"/Users/hiroto/.vscode/extensions/ms-toolsai.jupyter-2022.9.1303220346/pythonFiles/vscode_datascience_helpers/jupyter_daemon.py\", line 168, in _start_notebook\n",
            "\u001b[1;31m    app.launch_new_instance()\n",
            "\u001b[1;31m  File \"/Users/hiroto/Library/Python/3.9/lib/python/site-packages/jupyter_core/application.py\", line 276, in launch_instance\n",
            "\u001b[1;31m    return super().launch_instance(argv=argv, **kwargs)\n",
            "\u001b[1;31m  File \"/Users/hiroto/Library/Python/3.9/lib/python/site-packages/traitlets/config/application.py\", line 991, in launch_instance\n",
            "\u001b[1;31m    app.initialize(argv)\n",
            "\u001b[1;31m  File \"/Users/hiroto/Library/Python/3.9/lib/python/site-packages/traitlets/config/application.py\", line 113, in inner\n",
            "\u001b[1;31m    return method(app, *args, **kwargs)\n",
            "\u001b[1;31m  File \"/Users/hiroto/Library/Python/3.9/lib/python/site-packages/notebook/notebookapp.py\", line 2142, in initialize\n",
            "\u001b[1;31m    super().initialize(argv)\n",
            "\u001b[1;31m  File \"/Users/hiroto/Library/Python/3.9/lib/python/site-packages/traitlets/config/application.py\", line 113, in inner\n",
            "\u001b[1;31m    return method(app, *args, **kwargs)\n",
            "\u001b[1;31m  File \"/Users/hiroto/Library/Python/3.9/lib/python/site-packages/jupyter_core/application.py\", line 247, in initialize\n",
            "\u001b[1;31m    self.parse_command_line(argv)\n",
            "\u001b[1;31m  File \"/Users/hiroto/Library/Python/3.9/lib/python/site-packages/notebook/notebookapp.py\", line 1612, in parse_command_line\n",
            "\u001b[1;31m    super().parse_command_line(argv)\n",
            "\u001b[1;31m  File \"/Users/hiroto/Library/Python/3.9/lib/python/site-packages/traitlets/config/application.py\", line 117, in inner\n",
            "\u001b[1;31m    app.exit(1)\n",
            "\u001b[1;31m  File \"/Users/hiroto/Library/Python/3.9/lib/python/site-packages/traitlets/config/application.py\", line 979, in exit\n",
            "\u001b[1;31m    sys.exit(exit_status)\n",
            "\u001b[1;31mSystemExit: 1\n",
            "\u001b[1;31m\n",
            "\u001b[1;31m[C 12:18:16.448 NotebookApp] Bad config encountered during initialization: No such notebook dir: ''/zakihir0/first-commit''\n",
            "\u001b[1;31m\n",
            "\u001b[1;31mFailed to run jupyter as observable with args notebook --no-browser --notebook-dir=\"/zakihir0/first-commit\" --config=/var/folders/gf/vx91cl6n2zz1lp38lfq9ds4c0000gn/T/73db2cf8-7161-46cb-91df-f29a92196559/jupyter_notebook_config.py --NotebookApp.iopub_data_rate_limit=10000000000.0"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGqwKGfxxXo9"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mカーネルを起動できませんでした。 \n",
            "\u001b[1;31mTraceback (most recent call last):\n",
            "\u001b[1;31m  File \"/Users/hiroto/Library/Python/3.9/lib/python/site-packages/traitlets/config/application.py\", line 113, in inner\n",
            "\u001b[1;31m    return method(app, *args, **kwargs)\n",
            "\u001b[1;31m  File \"/Users/hiroto/Library/Python/3.9/lib/python/site-packages/traitlets/config/application.py\", line 834, in parse_command_line\n",
            "\u001b[1;31m    self.update_config(self.cli_config)\n",
            "\u001b[1;31m  File \"/Users/hiroto/Library/Python/3.9/lib/python/site-packages/traitlets/config/configurable.py\", line 231, in update_config\n",
            "\u001b[1;31m    self._load_config(config)\n",
            "\u001b[1;31m  File \"/Users/hiroto/Library/Python/3.9/lib/python/site-packages/traitlets/config/configurable.py\", line 201, in _load_config\n",
            "\u001b[1;31m    warn(msg)\n",
            "\u001b[1;31m  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/contextlib.py\", line 124, in __exit__\n",
            "\u001b[1;31m    next(self.gen)\n",
            "\u001b[1;31m  File \"/Users/hiroto/Library/Python/3.9/lib/python/site-packages/traitlets/traitlets.py\", line 1483, in hold_trait_notifications\n",
            "\u001b[1;31m    raise e\n",
            "\u001b[1;31m  File \"/Users/hiroto/Library/Python/3.9/lib/python/site-packages/traitlets/traitlets.py\", line 1469, in hold_trait_notifications\n",
            "\u001b[1;31m    value = trait._cross_validate(self, getattr(self, name))\n",
            "\u001b[1;31m  File \"/Users/hiroto/Library/Python/3.9/lib/python/site-packages/traitlets/traitlets.py\", line 743, in _cross_validate\n",
            "\u001b[1;31m    value = obj._trait_validators<a href='obj, proposal'>self.name</a>\n",
            "\u001b[1;31m  File \"/Users/hiroto/Library/Python/3.9/lib/python/site-packages/traitlets/traitlets.py\", line 1223, in __call__\n",
            "\u001b[1;31m    return self.func(*args, **kwargs)\n",
            "\u001b[1;31m  File \"/Users/hiroto/Library/Python/3.9/lib/python/site-packages/notebook/notebookapp.py\", line 1523, in _notebook_dir_validate\n",
            "\u001b[1;31m    raise TraitError(trans.gettext(\"No such notebook dir: '%r'\") % value)\n",
            "\u001b[1;31mtraitlets.traitlets.TraitError: No such notebook dir: ''/zakihir0/first-commit''\n",
            "\u001b[1;31m\n",
            "\u001b[1;31mDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31m\n",
            "\u001b[1;31mTraceback (most recent call last):\n",
            "\u001b[1;31m  File \"/Users/hiroto/.vscode/extensions/ms-toolsai.jupyter-2022.9.1303220346/pythonFiles/vscode_datascience_helpers/daemon/daemon_python.py\", line 54, in _decorator\n",
            "\u001b[1;31m    return func(self, *args, **kwargs)\n",
            "\u001b[1;31m  File \"/Users/hiroto/.vscode/extensions/ms-toolsai.jupyter-2022.9.1303220346/pythonFiles/vscode_datascience_helpers/jupyter_daemon.py\", line 111, in m_exec_module_observable\n",
            "\u001b[1;31m    self._start_notebook(args, cwd, env)\n",
            "\u001b[1;31m  File \"/Users/hiroto/.vscode/extensions/ms-toolsai.jupyter-2022.9.1303220346/pythonFiles/vscode_datascience_helpers/jupyter_daemon.py\", line 168, in _start_notebook\n",
            "\u001b[1;31m    app.launch_new_instance()\n",
            "\u001b[1;31m  File \"/Users/hiroto/Library/Python/3.9/lib/python/site-packages/jupyter_core/application.py\", line 276, in launch_instance\n",
            "\u001b[1;31m    return super().launch_instance(argv=argv, **kwargs)\n",
            "\u001b[1;31m  File \"/Users/hiroto/Library/Python/3.9/lib/python/site-packages/traitlets/config/application.py\", line 991, in launch_instance\n",
            "\u001b[1;31m    app.initialize(argv)\n",
            "\u001b[1;31m  File \"/Users/hiroto/Library/Python/3.9/lib/python/site-packages/traitlets/config/application.py\", line 113, in inner\n",
            "\u001b[1;31m    return method(app, *args, **kwargs)\n",
            "\u001b[1;31m  File \"/Users/hiroto/Library/Python/3.9/lib/python/site-packages/notebook/notebookapp.py\", line 2142, in initialize\n",
            "\u001b[1;31m    super().initialize(argv)\n",
            "\u001b[1;31m  File \"/Users/hiroto/Library/Python/3.9/lib/python/site-packages/traitlets/config/application.py\", line 113, in inner\n",
            "\u001b[1;31m    return method(app, *args, **kwargs)\n",
            "\u001b[1;31m  File \"/Users/hiroto/Library/Python/3.9/lib/python/site-packages/jupyter_core/application.py\", line 247, in initialize\n",
            "\u001b[1;31m    self.parse_command_line(argv)\n",
            "\u001b[1;31m  File \"/Users/hiroto/Library/Python/3.9/lib/python/site-packages/notebook/notebookapp.py\", line 1612, in parse_command_line\n",
            "\u001b[1;31m    super().parse_command_line(argv)\n",
            "\u001b[1;31m  File \"/Users/hiroto/Library/Python/3.9/lib/python/site-packages/traitlets/config/application.py\", line 117, in inner\n",
            "\u001b[1;31m    app.exit(1)\n",
            "\u001b[1;31m  File \"/Users/hiroto/Library/Python/3.9/lib/python/site-packages/traitlets/config/application.py\", line 979, in exit\n",
            "\u001b[1;31m    sys.exit(exit_status)\n",
            "\u001b[1;31mSystemExit: 1\n",
            "\u001b[1;31m\n",
            "\u001b[1;31m[C 12:18:24.053 NotebookApp] Bad config encountered during initialization: No such notebook dir: ''/zakihir0/first-commit''\n",
            "\u001b[1;31m\n",
            "\u001b[1;31mFailed to run jupyter as observable with args notebook --no-browser --notebook-dir=\"/zakihir0/first-commit\" --config=/var/folders/gf/vx91cl6n2zz1lp38lfq9ds4c0000gn/T/438906e0-e089-4f2d-81b9-f8681db483fa/jupyter_notebook_config.py --NotebookApp.iopub_data_rate_limit=10000000000.0"
          ]
        }
      ],
      "source": [
        "df = pd.concat([dataset, y], axis=1)\n",
        "df = pd.concat([dataset, y], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "id": "K8WGO38dxXo9",
        "outputId": "73fb3552-28bc-4b00-cfb1-d2b859ae61c9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-c956a4ba-449f-45a8-ac89-3c71753c3375\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CabinNum</th>\n",
              "      <th>HomePlanet_Earth</th>\n",
              "      <th>HomePlanet_Europa</th>\n",
              "      <th>HomePlanet_Mars</th>\n",
              "      <th>CryoSleep_False</th>\n",
              "      <th>CryoSleep_True</th>\n",
              "      <th>Destination_55 Cancri e</th>\n",
              "      <th>Destination_PSO J318.5-22</th>\n",
              "      <th>Destination_TRAPPIST-1e</th>\n",
              "      <th>VIP_False</th>\n",
              "      <th>...</th>\n",
              "      <th>CabinDeck_B</th>\n",
              "      <th>CabinDeck_C</th>\n",
              "      <th>CabinDeck_D</th>\n",
              "      <th>CabinDeck_E</th>\n",
              "      <th>CabinDeck_F</th>\n",
              "      <th>CabinDeck_G</th>\n",
              "      <th>CabinDeck_T</th>\n",
              "      <th>CabinSide_P</th>\n",
              "      <th>CabinSide_S</th>\n",
              "      <th>Transported</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8688</th>\n",
              "      <td>98.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8689</th>\n",
              "      <td>1499.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8690</th>\n",
              "      <td>1500.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8691</th>\n",
              "      <td>608.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8692</th>\n",
              "      <td>608.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6606 rows × 22 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c956a4ba-449f-45a8-ac89-3c71753c3375')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c956a4ba-449f-45a8-ac89-3c71753c3375 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c956a4ba-449f-45a8-ac89-3c71753c3375');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      CabinNum  HomePlanet_Earth  HomePlanet_Europa  HomePlanet_Mars  \\\n",
              "0          0.0                 0                  1                0   \n",
              "1          0.0                 1                  0                0   \n",
              "2          0.0                 0                  1                0   \n",
              "3          0.0                 0                  1                0   \n",
              "4          1.0                 1                  0                0   \n",
              "...        ...               ...                ...              ...   \n",
              "8688      98.0                 0                  1                0   \n",
              "8689    1499.0                 1                  0                0   \n",
              "8690    1500.0                 1                  0                0   \n",
              "8691     608.0                 0                  1                0   \n",
              "8692     608.0                 0                  1                0   \n",
              "\n",
              "      CryoSleep_False  CryoSleep_True  Destination_55 Cancri e  \\\n",
              "0                   1               0                        0   \n",
              "1                   1               0                        0   \n",
              "2                   1               0                        0   \n",
              "3                   1               0                        0   \n",
              "4                   1               0                        0   \n",
              "...               ...             ...                      ...   \n",
              "8688                1               0                        1   \n",
              "8689                0               1                        0   \n",
              "8690                1               0                        0   \n",
              "8691                1               0                        1   \n",
              "8692                1               0                        0   \n",
              "\n",
              "      Destination_PSO J318.5-22  Destination_TRAPPIST-1e  VIP_False  ...  \\\n",
              "0                             0                        1          1  ...   \n",
              "1                             0                        1          1  ...   \n",
              "2                             0                        1          0  ...   \n",
              "3                             0                        1          1  ...   \n",
              "4                             0                        1          1  ...   \n",
              "...                         ...                      ...        ...  ...   \n",
              "8688                          0                        0          0  ...   \n",
              "8689                          1                        0          1  ...   \n",
              "8690                          0                        1          1  ...   \n",
              "8691                          0                        0          1  ...   \n",
              "8692                          0                        1          1  ...   \n",
              "\n",
              "      CabinDeck_B  CabinDeck_C  CabinDeck_D  CabinDeck_E  CabinDeck_F  \\\n",
              "0               1            0            0            0            0   \n",
              "1               0            0            0            0            1   \n",
              "2               0            0            0            0            0   \n",
              "3               0            0            0            0            0   \n",
              "4               0            0            0            0            1   \n",
              "...           ...          ...          ...          ...          ...   \n",
              "8688            0            0            0            0            0   \n",
              "8689            0            0            0            0            0   \n",
              "8690            0            0            0            0            0   \n",
              "8691            0            0            0            1            0   \n",
              "8692            0            0            0            1            0   \n",
              "\n",
              "      CabinDeck_G  CabinDeck_T  CabinSide_P  CabinSide_S  Transported  \n",
              "0               0            0            1            0        False  \n",
              "1               0            0            0            1         True  \n",
              "2               0            0            0            1        False  \n",
              "3               0            0            0            1        False  \n",
              "4               0            0            0            1         True  \n",
              "...           ...          ...          ...          ...          ...  \n",
              "8688            0            0            1            0        False  \n",
              "8689            1            0            0            1        False  \n",
              "8690            1            0            0            1         True  \n",
              "8691            0            0            0            1        False  \n",
              "8692            0            0            0            1         True  \n",
              "\n",
              "[6606 rows x 22 columns]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1yzRvnqggOBe"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mカーネルを起動できませんでした。 \n",
            "\u001b[1;31mTraceback (most recent call last):\n",
            "\u001b[1;31m  File \"/Users/hiroto/Library/Python/3.9/lib/python/site-packages/traitlets/config/application.py\", line 113, in inner\n",
            "\u001b[1;31m    return method(app, *args, **kwargs)\n",
            "\u001b[1;31m  File \"/Users/hiroto/Library/Python/3.9/lib/python/site-packages/traitlets/config/application.py\", line 834, in parse_command_line\n",
            "\u001b[1;31m    self.update_config(self.cli_config)\n",
            "\u001b[1;31m  File \"/Users/hiroto/Library/Python/3.9/lib/python/site-packages/traitlets/config/configurable.py\", line 231, in update_config\n",
            "\u001b[1;31m    self._load_config(config)\n",
            "\u001b[1;31m  File \"/Users/hiroto/Library/Python/3.9/lib/python/site-packages/traitlets/config/configurable.py\", line 201, in _load_config\n",
            "\u001b[1;31m    warn(msg)\n",
            "\u001b[1;31m  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/contextlib.py\", line 124, in __exit__\n",
            "\u001b[1;31m    next(self.gen)\n",
            "\u001b[1;31m  File \"/Users/hiroto/Library/Python/3.9/lib/python/site-packages/traitlets/traitlets.py\", line 1483, in hold_trait_notifications\n",
            "\u001b[1;31m    raise e\n",
            "\u001b[1;31m  File \"/Users/hiroto/Library/Python/3.9/lib/python/site-packages/traitlets/traitlets.py\", line 1469, in hold_trait_notifications\n",
            "\u001b[1;31m    value = trait._cross_validate(self, getattr(self, name))\n",
            "\u001b[1;31m  File \"/Users/hiroto/Library/Python/3.9/lib/python/site-packages/traitlets/traitlets.py\", line 743, in _cross_validate\n",
            "\u001b[1;31m    value = obj._trait_validators<a href='obj, proposal'>self.name</a>\n",
            "\u001b[1;31m  File \"/Users/hiroto/Library/Python/3.9/lib/python/site-packages/traitlets/traitlets.py\", line 1223, in __call__\n",
            "\u001b[1;31m    return self.func(*args, **kwargs)\n",
            "\u001b[1;31m  File \"/Users/hiroto/Library/Python/3.9/lib/python/site-packages/notebook/notebookapp.py\", line 1523, in _notebook_dir_validate\n",
            "\u001b[1;31m    raise TraitError(trans.gettext(\"No such notebook dir: '%r'\") % value)\n",
            "\u001b[1;31mtraitlets.traitlets.TraitError: No such notebook dir: ''/zakihir0/first-commit''\n",
            "\u001b[1;31m\n",
            "\u001b[1;31mDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31m\n",
            "\u001b[1;31mTraceback (most recent call last):\n",
            "\u001b[1;31m  File \"/Users/hiroto/.vscode/extensions/ms-toolsai.jupyter-2022.9.1303220346/pythonFiles/vscode_datascience_helpers/daemon/daemon_python.py\", line 54, in _decorator\n",
            "\u001b[1;31m    return func(self, *args, **kwargs)\n",
            "\u001b[1;31m  File \"/Users/hiroto/.vscode/extensions/ms-toolsai.jupyter-2022.9.1303220346/pythonFiles/vscode_datascience_helpers/jupyter_daemon.py\", line 111, in m_exec_module_observable\n",
            "\u001b[1;31m    self._start_notebook(args, cwd, env)\n",
            "\u001b[1;31m  File \"/Users/hiroto/.vscode/extensions/ms-toolsai.jupyter-2022.9.1303220346/pythonFiles/vscode_datascience_helpers/jupyter_daemon.py\", line 168, in _start_notebook\n",
            "\u001b[1;31m    app.launch_new_instance()\n",
            "\u001b[1;31m  File \"/Users/hiroto/Library/Python/3.9/lib/python/site-packages/jupyter_core/application.py\", line 276, in launch_instance\n",
            "\u001b[1;31m    return super().launch_instance(argv=argv, **kwargs)\n",
            "\u001b[1;31m  File \"/Users/hiroto/Library/Python/3.9/lib/python/site-packages/traitlets/config/application.py\", line 991, in launch_instance\n",
            "\u001b[1;31m    app.initialize(argv)\n",
            "\u001b[1;31m  File \"/Users/hiroto/Library/Python/3.9/lib/python/site-packages/traitlets/config/application.py\", line 113, in inner\n",
            "\u001b[1;31m    return method(app, *args, **kwargs)\n",
            "\u001b[1;31m  File \"/Users/hiroto/Library/Python/3.9/lib/python/site-packages/notebook/notebookapp.py\", line 2142, in initialize\n",
            "\u001b[1;31m    super().initialize(argv)\n",
            "\u001b[1;31m  File \"/Users/hiroto/Library/Python/3.9/lib/python/site-packages/traitlets/config/application.py\", line 113, in inner\n",
            "\u001b[1;31m    return method(app, *args, **kwargs)\n",
            "\u001b[1;31m  File \"/Users/hiroto/Library/Python/3.9/lib/python/site-packages/jupyter_core/application.py\", line 247, in initialize\n",
            "\u001b[1;31m    self.parse_command_line(argv)\n",
            "\u001b[1;31m  File \"/Users/hiroto/Library/Python/3.9/lib/python/site-packages/notebook/notebookapp.py\", line 1612, in parse_command_line\n",
            "\u001b[1;31m    super().parse_command_line(argv)\n",
            "\u001b[1;31m  File \"/Users/hiroto/Library/Python/3.9/lib/python/site-packages/traitlets/config/application.py\", line 117, in inner\n",
            "\u001b[1;31m    app.exit(1)\n",
            "\u001b[1;31m  File \"/Users/hiroto/Library/Python/3.9/lib/python/site-packages/traitlets/config/application.py\", line 979, in exit\n",
            "\u001b[1;31m    sys.exit(exit_status)\n",
            "\u001b[1;31mSystemExit: 1\n",
            "\u001b[1;31m\n",
            "\u001b[1;31m[C 12:19:02.481 NotebookApp] Bad config encountered during initialization: No such notebook dir: ''/zakihir0/first-commit''\n",
            "\u001b[1;31m\n",
            "\u001b[1;31mFailed to run jupyter as observable with args notebook --no-browser --notebook-dir=\"/zakihir0/first-commit\" --config=/var/folders/gf/vx91cl6n2zz1lp38lfq9ds4c0000gn/T/90852f13-f382-4a50-a5fe-faec1c1b5994/jupyter_notebook_config.py --NotebookApp.iopub_data_rate_limit=10000000000.0"
          ]
        }
      ],
      "source": [
        "X = df.iloc[:, 0:-1]\n",
        "y = df.iloc[:, -1] "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "DHhfUsD-lpcL"
      },
      "outputs": [],
      "source": [
        "X.to_csv(\"X.csv\")\n",
        "y.to_csv(\"y.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ynGUFq9DxXo9"
      },
      "outputs": [],
      "source": [
        "X_scaled = (X-X.mean())/X.std()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "id": "4yLgD1boxXo9",
        "outputId": "a2307f97-39c1-459e-b1da-09f47c9e3ce9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-40a69da7-3346-4765-9da5-1945a862ae3c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CabinNum</th>\n",
              "      <th>HomePlanet_Earth</th>\n",
              "      <th>HomePlanet_Europa</th>\n",
              "      <th>HomePlanet_Mars</th>\n",
              "      <th>CryoSleep_False</th>\n",
              "      <th>CryoSleep_True</th>\n",
              "      <th>Destination_55 Cancri e</th>\n",
              "      <th>Destination_PSO J318.5-22</th>\n",
              "      <th>Destination_TRAPPIST-1e</th>\n",
              "      <th>VIP_False</th>\n",
              "      <th>...</th>\n",
              "      <th>CabinDeck_A</th>\n",
              "      <th>CabinDeck_B</th>\n",
              "      <th>CabinDeck_C</th>\n",
              "      <th>CabinDeck_D</th>\n",
              "      <th>CabinDeck_E</th>\n",
              "      <th>CabinDeck_F</th>\n",
              "      <th>CabinDeck_G</th>\n",
              "      <th>CabinDeck_T</th>\n",
              "      <th>CabinSide_P</th>\n",
              "      <th>CabinSide_S</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-1.166962</td>\n",
              "      <td>-1.082981</td>\n",
              "      <td>1.717017</td>\n",
              "      <td>-0.510772</td>\n",
              "      <td>0.738608</td>\n",
              "      <td>-0.738608</td>\n",
              "      <td>-0.520181</td>\n",
              "      <td>-0.322665</td>\n",
              "      <td>0.665997</td>\n",
              "      <td>0.158543</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.179844</td>\n",
              "      <td>3.085072</td>\n",
              "      <td>-0.312266</td>\n",
              "      <td>-0.244957</td>\n",
              "      <td>-0.339552</td>\n",
              "      <td>-0.695045</td>\n",
              "      <td>-0.652529</td>\n",
              "      <td>-0.017401</td>\n",
              "      <td>1.012721</td>\n",
              "      <td>-1.012721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-1.166962</td>\n",
              "      <td>0.923237</td>\n",
              "      <td>-0.582317</td>\n",
              "      <td>-0.510772</td>\n",
              "      <td>0.738608</td>\n",
              "      <td>-0.738608</td>\n",
              "      <td>-0.520181</td>\n",
              "      <td>-0.322665</td>\n",
              "      <td>0.665997</td>\n",
              "      <td>0.158543</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.179844</td>\n",
              "      <td>-0.324093</td>\n",
              "      <td>-0.312266</td>\n",
              "      <td>-0.244957</td>\n",
              "      <td>-0.339552</td>\n",
              "      <td>1.438537</td>\n",
              "      <td>-0.652529</td>\n",
              "      <td>-0.017401</td>\n",
              "      <td>-0.987289</td>\n",
              "      <td>0.987289</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-1.166962</td>\n",
              "      <td>-1.082981</td>\n",
              "      <td>1.717017</td>\n",
              "      <td>-0.510772</td>\n",
              "      <td>0.738608</td>\n",
              "      <td>-0.738608</td>\n",
              "      <td>-0.520181</td>\n",
              "      <td>-0.322665</td>\n",
              "      <td>0.665997</td>\n",
              "      <td>-6.306485</td>\n",
              "      <td>...</td>\n",
              "      <td>5.559529</td>\n",
              "      <td>-0.324093</td>\n",
              "      <td>-0.312266</td>\n",
              "      <td>-0.244957</td>\n",
              "      <td>-0.339552</td>\n",
              "      <td>-0.695045</td>\n",
              "      <td>-0.652529</td>\n",
              "      <td>-0.017401</td>\n",
              "      <td>-0.987289</td>\n",
              "      <td>0.987289</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-1.166962</td>\n",
              "      <td>-1.082981</td>\n",
              "      <td>1.717017</td>\n",
              "      <td>-0.510772</td>\n",
              "      <td>0.738608</td>\n",
              "      <td>-0.738608</td>\n",
              "      <td>-0.520181</td>\n",
              "      <td>-0.322665</td>\n",
              "      <td>0.665997</td>\n",
              "      <td>0.158543</td>\n",
              "      <td>...</td>\n",
              "      <td>5.559529</td>\n",
              "      <td>-0.324093</td>\n",
              "      <td>-0.312266</td>\n",
              "      <td>-0.244957</td>\n",
              "      <td>-0.339552</td>\n",
              "      <td>-0.695045</td>\n",
              "      <td>-0.652529</td>\n",
              "      <td>-0.017401</td>\n",
              "      <td>-0.987289</td>\n",
              "      <td>0.987289</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-1.165015</td>\n",
              "      <td>0.923237</td>\n",
              "      <td>-0.582317</td>\n",
              "      <td>-0.510772</td>\n",
              "      <td>0.738608</td>\n",
              "      <td>-0.738608</td>\n",
              "      <td>-0.520181</td>\n",
              "      <td>-0.322665</td>\n",
              "      <td>0.665997</td>\n",
              "      <td>0.158543</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.179844</td>\n",
              "      <td>-0.324093</td>\n",
              "      <td>-0.312266</td>\n",
              "      <td>-0.244957</td>\n",
              "      <td>-0.339552</td>\n",
              "      <td>1.438537</td>\n",
              "      <td>-0.652529</td>\n",
              "      <td>-0.017401</td>\n",
              "      <td>-0.987289</td>\n",
              "      <td>0.987289</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8688</th>\n",
              "      <td>-0.976097</td>\n",
              "      <td>-1.082981</td>\n",
              "      <td>1.717017</td>\n",
              "      <td>-0.510772</td>\n",
              "      <td>0.738608</td>\n",
              "      <td>-0.738608</td>\n",
              "      <td>1.922118</td>\n",
              "      <td>-0.322665</td>\n",
              "      <td>-1.501281</td>\n",
              "      <td>-6.306485</td>\n",
              "      <td>...</td>\n",
              "      <td>5.559529</td>\n",
              "      <td>-0.324093</td>\n",
              "      <td>-0.312266</td>\n",
              "      <td>-0.244957</td>\n",
              "      <td>-0.339552</td>\n",
              "      <td>-0.695045</td>\n",
              "      <td>-0.652529</td>\n",
              "      <td>-0.017401</td>\n",
              "      <td>1.012721</td>\n",
              "      <td>-1.012721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8689</th>\n",
              "      <td>1.752501</td>\n",
              "      <td>0.923237</td>\n",
              "      <td>-0.582317</td>\n",
              "      <td>-0.510772</td>\n",
              "      <td>-1.353693</td>\n",
              "      <td>1.353693</td>\n",
              "      <td>-0.520181</td>\n",
              "      <td>3.098722</td>\n",
              "      <td>-1.501281</td>\n",
              "      <td>0.158543</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.179844</td>\n",
              "      <td>-0.324093</td>\n",
              "      <td>-0.312266</td>\n",
              "      <td>-0.244957</td>\n",
              "      <td>-0.339552</td>\n",
              "      <td>-0.695045</td>\n",
              "      <td>1.532268</td>\n",
              "      <td>-0.017401</td>\n",
              "      <td>-0.987289</td>\n",
              "      <td>0.987289</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8690</th>\n",
              "      <td>1.754448</td>\n",
              "      <td>0.923237</td>\n",
              "      <td>-0.582317</td>\n",
              "      <td>-0.510772</td>\n",
              "      <td>0.738608</td>\n",
              "      <td>-0.738608</td>\n",
              "      <td>-0.520181</td>\n",
              "      <td>-0.322665</td>\n",
              "      <td>0.665997</td>\n",
              "      <td>0.158543</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.179844</td>\n",
              "      <td>-0.324093</td>\n",
              "      <td>-0.312266</td>\n",
              "      <td>-0.244957</td>\n",
              "      <td>-0.339552</td>\n",
              "      <td>-0.695045</td>\n",
              "      <td>1.532268</td>\n",
              "      <td>-0.017401</td>\n",
              "      <td>-0.987289</td>\n",
              "      <td>0.987289</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8691</th>\n",
              "      <td>0.017183</td>\n",
              "      <td>-1.082981</td>\n",
              "      <td>1.717017</td>\n",
              "      <td>-0.510772</td>\n",
              "      <td>0.738608</td>\n",
              "      <td>-0.738608</td>\n",
              "      <td>1.922118</td>\n",
              "      <td>-0.322665</td>\n",
              "      <td>-1.501281</td>\n",
              "      <td>0.158543</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.179844</td>\n",
              "      <td>-0.324093</td>\n",
              "      <td>-0.312266</td>\n",
              "      <td>-0.244957</td>\n",
              "      <td>2.944609</td>\n",
              "      <td>-0.695045</td>\n",
              "      <td>-0.652529</td>\n",
              "      <td>-0.017401</td>\n",
              "      <td>-0.987289</td>\n",
              "      <td>0.987289</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8692</th>\n",
              "      <td>0.017183</td>\n",
              "      <td>-1.082981</td>\n",
              "      <td>1.717017</td>\n",
              "      <td>-0.510772</td>\n",
              "      <td>0.738608</td>\n",
              "      <td>-0.738608</td>\n",
              "      <td>-0.520181</td>\n",
              "      <td>-0.322665</td>\n",
              "      <td>0.665997</td>\n",
              "      <td>0.158543</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.179844</td>\n",
              "      <td>-0.324093</td>\n",
              "      <td>-0.312266</td>\n",
              "      <td>-0.244957</td>\n",
              "      <td>2.944609</td>\n",
              "      <td>-0.695045</td>\n",
              "      <td>-0.652529</td>\n",
              "      <td>-0.017401</td>\n",
              "      <td>-0.987289</td>\n",
              "      <td>0.987289</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6606 rows × 21 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-40a69da7-3346-4765-9da5-1945a862ae3c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-40a69da7-3346-4765-9da5-1945a862ae3c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-40a69da7-3346-4765-9da5-1945a862ae3c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      CabinNum  HomePlanet_Earth  HomePlanet_Europa  HomePlanet_Mars  \\\n",
              "0    -1.166962         -1.082981           1.717017        -0.510772   \n",
              "1    -1.166962          0.923237          -0.582317        -0.510772   \n",
              "2    -1.166962         -1.082981           1.717017        -0.510772   \n",
              "3    -1.166962         -1.082981           1.717017        -0.510772   \n",
              "4    -1.165015          0.923237          -0.582317        -0.510772   \n",
              "...        ...               ...                ...              ...   \n",
              "8688 -0.976097         -1.082981           1.717017        -0.510772   \n",
              "8689  1.752501          0.923237          -0.582317        -0.510772   \n",
              "8690  1.754448          0.923237          -0.582317        -0.510772   \n",
              "8691  0.017183         -1.082981           1.717017        -0.510772   \n",
              "8692  0.017183         -1.082981           1.717017        -0.510772   \n",
              "\n",
              "      CryoSleep_False  CryoSleep_True  Destination_55 Cancri e  \\\n",
              "0            0.738608       -0.738608                -0.520181   \n",
              "1            0.738608       -0.738608                -0.520181   \n",
              "2            0.738608       -0.738608                -0.520181   \n",
              "3            0.738608       -0.738608                -0.520181   \n",
              "4            0.738608       -0.738608                -0.520181   \n",
              "...               ...             ...                      ...   \n",
              "8688         0.738608       -0.738608                 1.922118   \n",
              "8689        -1.353693        1.353693                -0.520181   \n",
              "8690         0.738608       -0.738608                -0.520181   \n",
              "8691         0.738608       -0.738608                 1.922118   \n",
              "8692         0.738608       -0.738608                -0.520181   \n",
              "\n",
              "      Destination_PSO J318.5-22  Destination_TRAPPIST-1e  VIP_False  ...  \\\n",
              "0                     -0.322665                 0.665997   0.158543  ...   \n",
              "1                     -0.322665                 0.665997   0.158543  ...   \n",
              "2                     -0.322665                 0.665997  -6.306485  ...   \n",
              "3                     -0.322665                 0.665997   0.158543  ...   \n",
              "4                     -0.322665                 0.665997   0.158543  ...   \n",
              "...                         ...                      ...        ...  ...   \n",
              "8688                  -0.322665                -1.501281  -6.306485  ...   \n",
              "8689                   3.098722                -1.501281   0.158543  ...   \n",
              "8690                  -0.322665                 0.665997   0.158543  ...   \n",
              "8691                  -0.322665                -1.501281   0.158543  ...   \n",
              "8692                  -0.322665                 0.665997   0.158543  ...   \n",
              "\n",
              "      CabinDeck_A  CabinDeck_B  CabinDeck_C  CabinDeck_D  CabinDeck_E  \\\n",
              "0       -0.179844     3.085072    -0.312266    -0.244957    -0.339552   \n",
              "1       -0.179844    -0.324093    -0.312266    -0.244957    -0.339552   \n",
              "2        5.559529    -0.324093    -0.312266    -0.244957    -0.339552   \n",
              "3        5.559529    -0.324093    -0.312266    -0.244957    -0.339552   \n",
              "4       -0.179844    -0.324093    -0.312266    -0.244957    -0.339552   \n",
              "...           ...          ...          ...          ...          ...   \n",
              "8688     5.559529    -0.324093    -0.312266    -0.244957    -0.339552   \n",
              "8689    -0.179844    -0.324093    -0.312266    -0.244957    -0.339552   \n",
              "8690    -0.179844    -0.324093    -0.312266    -0.244957    -0.339552   \n",
              "8691    -0.179844    -0.324093    -0.312266    -0.244957     2.944609   \n",
              "8692    -0.179844    -0.324093    -0.312266    -0.244957     2.944609   \n",
              "\n",
              "      CabinDeck_F  CabinDeck_G  CabinDeck_T  CabinSide_P  CabinSide_S  \n",
              "0       -0.695045    -0.652529    -0.017401     1.012721    -1.012721  \n",
              "1        1.438537    -0.652529    -0.017401    -0.987289     0.987289  \n",
              "2       -0.695045    -0.652529    -0.017401    -0.987289     0.987289  \n",
              "3       -0.695045    -0.652529    -0.017401    -0.987289     0.987289  \n",
              "4        1.438537    -0.652529    -0.017401    -0.987289     0.987289  \n",
              "...           ...          ...          ...          ...          ...  \n",
              "8688    -0.695045    -0.652529    -0.017401     1.012721    -1.012721  \n",
              "8689    -0.695045     1.532268    -0.017401    -0.987289     0.987289  \n",
              "8690    -0.695045     1.532268    -0.017401    -0.987289     0.987289  \n",
              "8691    -0.695045    -0.652529    -0.017401    -0.987289     0.987289  \n",
              "8692    -0.695045    -0.652529    -0.017401    -0.987289     0.987289  \n",
              "\n",
              "[6606 rows x 21 columns]"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_scaled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "rBq5n0FFxXo-"
      },
      "outputs": [],
      "source": [
        "class XGBClassifier:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        \n",
        "    def validation(X, y):\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=y)\n",
        "        print('train dataset shape %s' % Counter(y))\n",
        "        print('train dataset shape %s' % Counter(y_train))\n",
        "        \n",
        "    def sampler(X_train, y_train, sampler):\n",
        "        X_res, y_res = sampler.fit_resample(X_train, y_train) \n",
        "        print('Resampled dataset shape %s' % Counter(y_res))\n",
        "        return X_res, y_res\n",
        "\n",
        "    def XGBoostClassifier_opt(X_train, X_test, y_train, y_test):\n",
        "        def objective(trial):\n",
        "          params = {\n",
        "              'n_estimators': trial.suggest_int('n_estimators', 0, 1000), \n",
        "              'max_depth': trial.suggest_int('max_depth', 1, 20), \n",
        "              'min_child_weight': trial.suggest_int('min_child_weight', 1, 20), \n",
        "              'subsample':trial.suggest_discrete_uniform('subsample', 0.5, 0.9, 0.1), \n",
        "              'colsample_bytree': trial.suggest_discrete_uniform('colsample_bytree', 0.5, 0.9, 0.1),\n",
        "              'verbose': 0\n",
        "              }\n",
        "          model = XGBClassifier(**params)\n",
        "          model.fit(X_train, y_train)\n",
        "          pred = model.predict(X_test)\n",
        "          accuracy = accuracy_score(y_test, pred)\n",
        "          return (1-accuracy)\n",
        "\n",
        "        study = optuna.create_study()\n",
        "        study.optimize(objective, n_trials=300)\n",
        "        return study"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "thRgk59xbWBR"
      },
      "outputs": [],
      "source": [
        "from imblearn.ensemble import BalancedBaggingClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "FyIVqPdseHrj"
      },
      "outputs": [],
      "source": [
        "def validation(X, y):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=y)\n",
        "    print('train dataset shape %s' % Counter(y))\n",
        "    print('train dataset shape %s' % Counter(y_train))\n",
        "    return X_train, X_test, y_train, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oo32xf98eMGY",
        "outputId": "4d931d43-b5c9-45b6-c725-1ae83b502220"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train dataset shape Counter({True: 3327, False: 3279})\n",
            "train dataset shape Counter({True: 2994, False: 2951})\n"
          ]
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = validation(X_scaled, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "affAjAvMboiZ",
        "outputId": "a7af65b5-51b0-47ba-8f9f-5a44fdef42a4"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-6aa5f6f5899f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBalancedBaggingClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampling_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/imblearn/ensemble/_bagging.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;31m# RandomUnderSampler is not supporting sample_weight. We need to pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;31m# None.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_more_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/ensemble/_bagging.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, max_samples, max_depth, sample_weight)\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0mmax_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m             \u001b[0mmax_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_features\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features_in_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"max_features must be int or float\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'BalancedBaggingClassifier' object has no attribute 'n_features_in_'"
          ]
        }
      ],
      "source": [
        "estimator = RandomForestClassifier(n_jobs=-1)\n",
        "model = BalancedBaggingClassifier(base_estimator=estimator, n_jobs=-1, n_estimators=10, sampling_strategy='auto')\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ncQiM28xXo-"
      },
      "outputs": [],
      "source": [
        "def XGBoostClassifier(X_train, X_test, y_train, y_test, study):\n",
        "  model = XGBClassifier(params=study.best_params,\n",
        "                  dtrain=X_train,\n",
        "                  num_boost_round=1000,\n",
        "                  early_stopping_rounds=5,\n",
        "                  evals=[(y_train, \"test\")])\n",
        "  model = model.fit(X_train, y_train)\n",
        "  y_pred = model.predict(X_test)\n",
        "  \n",
        "  print(\"accuracy scores\", accuracy_score(y_test, y_pred))\n",
        "  metrics.roc_auc_score(y_test, y_pred)\n",
        "  matrix = metrics.confusion_matrix(y_test, y_pred, normalize=\"all\", labels=[False, True])\n",
        "  ax = sns.heatmap(matrix, annot=True, cmap=\"Blues\")\n",
        "  \n",
        "  y_proba = model.predict_proba(X_test)\n",
        "\n",
        "  fpr, tpr, thresholds = metrics.roc_curve(y_test, y_proba[:, 1])\n",
        "  auc = metrics.auc(fpr, tpr)\n",
        "  \n",
        "  plt.plot(fpr, tpr, label='ROC curve (area = %.2f)'%auc)\n",
        "  plt.legend()\n",
        "  plt.xlabel('FPR: False positive rate')\n",
        "  plt.ylabel('TPR: True positive rate')\n",
        "  plt.grid()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8NXqYk_xXo-",
        "outputId": "52bea0f6-57e7-4a47-f9a9-2c7f15683ee2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Resampled dataset shape Counter({False: 2951, True: 1385})\n"
          ]
        }
      ],
      "source": [
        "X_res, y_res = sampler(X_train, y_train, EditedNearestNeighbours())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGPZ3vKZxXo-",
        "outputId": "9850ce4c-d313-486a-8b53-2f1463bb80a0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:19:31,023]\u001b[0m A new study created in memory with name: no-name-255b01b1-531f-46f1-87f2-9a519fd6b8a2\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:19:31] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:19:31] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:19:31,617]\u001b[0m Trial 0 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 243, 'max_depth': 9, 'min_child_weight': 13, 'subsample': 0.7, 'colsample_bytree': 0.9}. Best is trial 0 with value: 0.25113464447806355.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:19:31] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:19:31] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:19:32,479]\u001b[0m Trial 1 finished with value: 0.2571860816944024 and parameters: {'n_estimators': 228, 'max_depth': 20, 'min_child_weight': 4, 'subsample': 0.9, 'colsample_bytree': 0.9}. Best is trial 0 with value: 0.25113464447806355.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-11 00:19:32,499]\u001b[0m Trial 2 finished with value: 0.5037821482602118 and parameters: {'n_estimators': 0, 'max_depth': 20, 'min_child_weight': 5, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 0 with value: 0.25113464447806355.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:19:32] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:19:32] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:19:32] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:19:33,378]\u001b[0m Trial 3 finished with value: 0.25567322239031776 and parameters: {'n_estimators': 703, 'max_depth': 1, 'min_child_weight': 10, 'subsample': 0.7, 'colsample_bytree': 0.9}. Best is trial 0 with value: 0.25113464447806355.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:19:33] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:19:33] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:19:35,061]\u001b[0m Trial 4 finished with value: 0.2617246596066566 and parameters: {'n_estimators': 931, 'max_depth': 3, 'min_child_weight': 1, 'subsample': 0.5, 'colsample_bytree': 0.7}. Best is trial 0 with value: 0.25113464447806355.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:19:35] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:19:35] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:19:35,288]\u001b[0m Trial 5 finished with value: 0.254160363086233 and parameters: {'n_estimators': 102, 'max_depth': 12, 'min_child_weight': 13, 'subsample': 0.5, 'colsample_bytree': 0.7}. Best is trial 0 with value: 0.25113464447806355.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:19:35] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:19:35] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:19:35,556]\u001b[0m Trial 6 finished with value: 0.26475037821482605 and parameters: {'n_estimators': 175, 'max_depth': 2, 'min_child_weight': 10, 'subsample': 0.5, 'colsample_bytree': 0.5}. Best is trial 0 with value: 0.25113464447806355.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-11 00:19:35,754]\u001b[0m Trial 7 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 75, 'max_depth': 16, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.5}. Best is trial 0 with value: 0.25113464447806355.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:19:35] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:19:35] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:19:35] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:19:35] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:19:36,420]\u001b[0m Trial 8 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 241, 'max_depth': 9, 'min_child_weight': 7, 'subsample': 0.7, 'colsample_bytree': 0.9}. Best is trial 0 with value: 0.25113464447806355.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:19:36] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:19:36] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:19:37,833]\u001b[0m Trial 9 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 767, 'max_depth': 10, 'min_child_weight': 12, 'subsample': 0.6, 'colsample_bytree': 0.6}. Best is trial 0 with value: 0.25113464447806355.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:19:37] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:19:37] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:19:38,764]\u001b[0m Trial 10 finished with value: 0.254160363086233 and parameters: {'n_estimators': 449, 'max_depth': 6, 'min_child_weight': 20, 'subsample': 0.8, 'colsample_bytree': 0.8}. Best is trial 0 with value: 0.25113464447806355.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:19:38] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:19:38] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:19:39,564]\u001b[0m Trial 11 finished with value: 0.24810892586989408 and parameters: {'n_estimators': 393, 'max_depth': 15, 'min_child_weight': 18, 'subsample': 0.9, 'colsample_bytree': 0.5}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:19:39] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:19:39] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:19:40,688]\u001b[0m Trial 12 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 444, 'max_depth': 14, 'min_child_weight': 16, 'subsample': 0.8, 'colsample_bytree': 0.6}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:19:40] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:19:40] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:19:41,665]\u001b[0m Trial 13 finished with value: 0.24810892586989408 and parameters: {'n_estimators': 460, 'max_depth': 15, 'min_child_weight': 17, 'subsample': 0.8, 'colsample_bytree': 0.6}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:19:41] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:19:41] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:19:42,794]\u001b[0m Trial 14 finished with value: 0.2526475037821483 and parameters: {'n_estimators': 582, 'max_depth': 16, 'min_child_weight': 20, 'subsample': 0.8, 'colsample_bytree': 0.6}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:19:42] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:19:42] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:19:43,524]\u001b[0m Trial 15 finished with value: 0.24810892586989408 and parameters: {'n_estimators': 347, 'max_depth': 16, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.5}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:19:43] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:19:43] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:19:44,633]\u001b[0m Trial 16 finished with value: 0.2526475037821483 and parameters: {'n_estimators': 573, 'max_depth': 14, 'min_child_weight': 15, 'subsample': 0.8, 'colsample_bytree': 0.6}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:19:44] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:19:44] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:19:45,302]\u001b[0m Trial 17 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 354, 'max_depth': 18, 'min_child_weight': 19, 'subsample': 0.9, 'colsample_bytree': 0.5}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:19:45] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:19:45] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:19:46,475]\u001b[0m Trial 18 finished with value: 0.254160363086233 and parameters: {'n_estimators': 571, 'max_depth': 18, 'min_child_weight': 15, 'subsample': 0.8, 'colsample_bytree': 0.6}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:19:46] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:19:46] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:19:47,199]\u001b[0m Trial 19 finished with value: 0.254160363086233 and parameters: {'n_estimators': 339, 'max_depth': 12, 'min_child_weight': 17, 'subsample': 0.6, 'colsample_bytree': 0.8}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:19:47] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:19:47] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:19:49,010]\u001b[0m Trial 20 finished with value: 0.254160363086233 and parameters: {'n_estimators': 724, 'max_depth': 7, 'min_child_weight': 8, 'subsample': 0.9, 'colsample_bytree': 0.5}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:19:49] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:19:49] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:19:49,710]\u001b[0m Trial 21 finished with value: 0.24810892586989408 and parameters: {'n_estimators': 359, 'max_depth': 15, 'min_child_weight': 18, 'subsample': 0.9, 'colsample_bytree': 0.5}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:19:49] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:19:49] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:19:50,368]\u001b[0m Trial 22 finished with value: 0.24810892586989408 and parameters: {'n_estimators': 316, 'max_depth': 12, 'min_child_weight': 15, 'subsample': 0.9, 'colsample_bytree': 0.5}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:19:50] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:19:50] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:19:51,254]\u001b[0m Trial 23 finished with value: 0.254160363086233 and parameters: {'n_estimators': 450, 'max_depth': 18, 'min_child_weight': 18, 'subsample': 0.8, 'colsample_bytree': 0.6}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:19:51] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:19:51] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:19:51,897]\u001b[0m Trial 24 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 331, 'max_depth': 17, 'min_child_weight': 19, 'subsample': 0.9, 'colsample_bytree': 0.5}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:19:51] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:19:51] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:19:52,487]\u001b[0m Trial 25 finished with value: 0.24810892586989408 and parameters: {'n_estimators': 276, 'max_depth': 12, 'min_child_weight': 14, 'subsample': 0.9, 'colsample_bytree': 0.5}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:19:52] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:19:52] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:19:52,972]\u001b[0m Trial 26 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 219, 'max_depth': 12, 'min_child_weight': 14, 'subsample': 0.9, 'colsample_bytree': 0.5}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:19:53] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:19:53] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:19:54,002]\u001b[0m Trial 27 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 503, 'max_depth': 14, 'min_child_weight': 12, 'subsample': 0.8, 'colsample_bytree': 0.6}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:19:54] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:19:54] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:19:55,218]\u001b[0m Trial 28 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 642, 'max_depth': 11, 'min_child_weight': 15, 'subsample': 0.8, 'colsample_bytree': 0.6}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:19:55] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:19:55] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:19:55,552]\u001b[0m Trial 29 finished with value: 0.24810892586989408 and parameters: {'n_estimators': 142, 'max_depth': 7, 'min_child_weight': 13, 'subsample': 0.6, 'colsample_bytree': 0.5}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:19:55] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:19:55] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:19:55,996]\u001b[0m Trial 30 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 163, 'max_depth': 7, 'min_child_weight': 12, 'subsample': 0.6, 'colsample_bytree': 0.8}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:19:56] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:19:56] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:19:56,620]\u001b[0m Trial 31 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 283, 'max_depth': 13, 'min_child_weight': 16, 'subsample': 0.9, 'colsample_bytree': 0.5}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:19:56] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:19:56] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:19:57,170]\u001b[0m Trial 32 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 281, 'max_depth': 5, 'min_child_weight': 13, 'subsample': 0.6, 'colsample_bytree': 0.5}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-11 00:19:57,411]\u001b[0m Trial 33 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 100, 'max_depth': 9, 'min_child_weight': 14, 'subsample': 0.6, 'colsample_bytree': 0.5}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:19:57] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:19:57] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-11 00:19:57,485]\u001b[0m Trial 34 finished with value: 0.2586989409984871 and parameters: {'n_estimators': 8, 'max_depth': 20, 'min_child_weight': 18, 'subsample': 0.7, 'colsample_bytree': 0.6}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:19:57] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:19:57] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "[00:19:57] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:19:57] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:19:57,936]\u001b[0m Trial 35 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 173, 'max_depth': 4, 'min_child_weight': 9, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:19:57] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:19:57] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:19:58,836]\u001b[0m Trial 36 finished with value: 0.24810892586989408 and parameters: {'n_estimators': 426, 'max_depth': 15, 'min_child_weight': 18, 'subsample': 0.9, 'colsample_bytree': 0.6}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:19:58] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:19:58] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:00,002]\u001b[0m Trial 37 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 504, 'max_depth': 16, 'min_child_weight': 16, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:00] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:00] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:01,279]\u001b[0m Trial 38 finished with value: 0.25567322239031776 and parameters: {'n_estimators': 408, 'max_depth': 19, 'min_child_weight': 1, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:01] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:01] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:01,897]\u001b[0m Trial 39 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 276, 'max_depth': 11, 'min_child_weight': 11, 'subsample': 0.9, 'colsample_bytree': 0.5}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:01] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:01] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:03,506]\u001b[0m Trial 40 finished with value: 0.2526475037821483 and parameters: {'n_estimators': 953, 'max_depth': 13, 'min_child_weight': 19, 'subsample': 0.9, 'colsample_bytree': 0.5}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:03] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:03] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:04,120]\u001b[0m Trial 41 finished with value: 0.25567322239031776 and parameters: {'n_estimators': 351, 'max_depth': 15, 'min_child_weight': 17, 'subsample': 0.5, 'colsample_bytree': 0.5}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:04] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:04] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:04,960]\u001b[0m Trial 42 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 401, 'max_depth': 13, 'min_child_weight': 14, 'subsample': 0.9, 'colsample_bytree': 0.6}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:05] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:05] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:05,696]\u001b[0m Trial 43 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 387, 'max_depth': 15, 'min_child_weight': 17, 'subsample': 0.8, 'colsample_bytree': 0.5}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:05] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:05] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:06,190]\u001b[0m Trial 44 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 210, 'max_depth': 8, 'min_child_weight': 13, 'subsample': 0.9, 'colsample_bytree': 0.5}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:06] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:06] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:07,068]\u001b[0m Trial 45 finished with value: 0.25567322239031776 and parameters: {'n_estimators': 484, 'max_depth': 17, 'min_child_weight': 16, 'subsample': 0.8, 'colsample_bytree': 0.5}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:07] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:07] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:08,494]\u001b[0m Trial 46 finished with value: 0.24810892586989408 and parameters: {'n_estimators': 857, 'max_depth': 11, 'min_child_weight': 14, 'subsample': 0.7, 'colsample_bytree': 0.5}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:08] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:08] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:10,071]\u001b[0m Trial 47 finished with value: 0.254160363086233 and parameters: {'n_estimators': 892, 'max_depth': 10, 'min_child_weight': 11, 'subsample': 0.7, 'colsample_bytree': 0.5}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-11 00:20:10,269]\u001b[0m Trial 48 finished with value: 0.25567322239031776 and parameters: {'n_estimators': 132, 'max_depth': 1, 'min_child_weight': 3, 'subsample': 0.5, 'colsample_bytree': 0.6}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:10] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:10] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "[00:20:10] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:10] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-11 00:20:11,167]\u001b[0m Trial 49 finished with value: 0.254160363086233 and parameters: {'n_estimators': 526, 'max_depth': 17, 'min_child_weight': 20, 'subsample': 0.8, 'colsample_bytree': 0.6}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-11 00:20:11,296]\u001b[0m Trial 50 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 41, 'max_depth': 16, 'min_child_weight': 18, 'subsample': 0.8, 'colsample_bytree': 0.6}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:11] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:11] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "[00:20:11] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:11] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:11,848]\u001b[0m Trial 51 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 295, 'max_depth': 12, 'min_child_weight': 14, 'subsample': 0.6, 'colsample_bytree': 0.5}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:11] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:11] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:12,358]\u001b[0m Trial 52 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 250, 'max_depth': 14, 'min_child_weight': 15, 'subsample': 0.9, 'colsample_bytree': 0.5}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:12] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:12] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:12,762]\u001b[0m Trial 53 finished with value: 0.2526475037821483 and parameters: {'n_estimators': 196, 'max_depth': 10, 'min_child_weight': 16, 'subsample': 0.9, 'colsample_bytree': 0.5}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:12] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:12] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:13,991]\u001b[0m Trial 54 finished with value: 0.25567322239031776 and parameters: {'n_estimators': 832, 'max_depth': 3, 'min_child_weight': 17, 'subsample': 0.7, 'colsample_bytree': 0.5}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:14] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:14] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:14,718]\u001b[0m Trial 55 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 371, 'max_depth': 9, 'min_child_weight': 12, 'subsample': 0.7, 'colsample_bytree': 0.5}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:14] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:14] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:15,840]\u001b[0m Trial 56 finished with value: 0.2526475037821483 and parameters: {'n_estimators': 635, 'max_depth': 15, 'min_child_weight': 18, 'subsample': 0.9, 'colsample_bytree': 0.6}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:15] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:15] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:16,474]\u001b[0m Trial 57 finished with value: 0.2526475037821483 and parameters: {'n_estimators': 305, 'max_depth': 11, 'min_child_weight': 13, 'subsample': 0.8, 'colsample_bytree': 0.5}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:16] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:16] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:17,366]\u001b[0m Trial 58 finished with value: 0.2526475037821483 and parameters: {'n_estimators': 543, 'max_depth': 8, 'min_child_weight': 15, 'subsample': 0.5, 'colsample_bytree': 0.5}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:17] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:17] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:18,132]\u001b[0m Trial 59 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 420, 'max_depth': 13, 'min_child_weight': 19, 'subsample': 0.9, 'colsample_bytree': 0.6}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:18] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:18] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:18,647]\u001b[0m Trial 60 finished with value: 0.24810892586989408 and parameters: {'n_estimators': 228, 'max_depth': 14, 'min_child_weight': 15, 'subsample': 0.9, 'colsample_bytree': 0.9}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:18] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:18] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:19,223]\u001b[0m Trial 61 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 256, 'max_depth': 14, 'min_child_weight': 15, 'subsample': 0.9, 'colsample_bytree': 0.9}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:19] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:19] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:19,917]\u001b[0m Trial 62 finished with value: 0.254160363086233 and parameters: {'n_estimators': 310, 'max_depth': 12, 'min_child_weight': 14, 'subsample': 0.9, 'colsample_bytree': 0.8}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:19] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:19] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:20,668]\u001b[0m Trial 63 finished with value: 0.2526475037821483 and parameters: {'n_estimators': 458, 'max_depth': 17, 'min_child_weight': 20, 'subsample': 0.6, 'colsample_bytree': 0.5}. Best is trial 11 with value: 0.24810892586989408.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:20] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:20] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:21,335]\u001b[0m Trial 64 finished with value: 0.24659606656580935 and parameters: {'n_estimators': 333, 'max_depth': 16, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:21] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:21] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:21,666]\u001b[0m Trial 65 finished with value: 0.2526475037821483 and parameters: {'n_estimators': 130, 'max_depth': 16, 'min_child_weight': 16, 'subsample': 0.9, 'colsample_bytree': 0.8}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:21] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:21] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:22,225]\u001b[0m Trial 66 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 237, 'max_depth': 11, 'min_child_weight': 13, 'subsample': 0.9, 'colsample_bytree': 0.9}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-11 00:20:22,462]\u001b[0m Trial 67 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 81, 'max_depth': 13, 'min_child_weight': 12, 'subsample': 0.9, 'colsample_bytree': 0.8}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:22] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:22] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:22] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:22] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:23,283]\u001b[0m Trial 68 finished with value: 0.2526475037821483 and parameters: {'n_estimators': 443, 'max_depth': 19, 'min_child_weight': 19, 'subsample': 0.8, 'colsample_bytree': 0.7}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:23] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:23] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:23,938]\u001b[0m Trial 69 finished with value: 0.24659606656580935 and parameters: {'n_estimators': 332, 'max_depth': 16, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:23] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:23] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:24,653]\u001b[0m Trial 70 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 363, 'max_depth': 15, 'min_child_weight': 18, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:24] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:24] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:26,300]\u001b[0m Trial 71 finished with value: 0.25567322239031776 and parameters: {'n_estimators': 991, 'max_depth': 14, 'min_child_weight': 17, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:26] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:26] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:27,004]\u001b[0m Trial 72 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 329, 'max_depth': 18, 'min_child_weight': 15, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:27] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:27] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:27,812]\u001b[0m Trial 73 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 420, 'max_depth': 15, 'min_child_weight': 18, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:27] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:27] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:28,729]\u001b[0m Trial 74 finished with value: 0.24810892586989408 and parameters: {'n_estimators': 479, 'max_depth': 16, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:28] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:28] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:29,405]\u001b[0m Trial 75 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 331, 'max_depth': 16, 'min_child_weight': 16, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:29] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:29] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:30,148]\u001b[0m Trial 76 finished with value: 0.2526475037821483 and parameters: {'n_estimators': 368, 'max_depth': 17, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.8}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:30] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:30] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:30,576]\u001b[0m Trial 77 finished with value: 0.24810892586989408 and parameters: {'n_estimators': 184, 'max_depth': 14, 'min_child_weight': 16, 'subsample': 0.8, 'colsample_bytree': 0.9}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:30] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:30] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:30,997]\u001b[0m Trial 78 finished with value: 0.2526475037821483 and parameters: {'n_estimators': 189, 'max_depth': 12, 'min_child_weight': 14, 'subsample': 0.6, 'colsample_bytree': 0.9}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:31] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:31] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:32,375]\u001b[0m Trial 79 finished with value: 0.25567322239031776 and parameters: {'n_estimators': 787, 'max_depth': 18, 'min_child_weight': 17, 'subsample': 0.8, 'colsample_bytree': 0.6}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:32] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:32] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:33,011]\u001b[0m Trial 80 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 271, 'max_depth': 13, 'min_child_weight': 19, 'subsample': 0.9, 'colsample_bytree': 0.8}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:33] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:33] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:33,818]\u001b[0m Trial 81 finished with value: 0.24659606656580935 and parameters: {'n_estimators': 389, 'max_depth': 15, 'min_child_weight': 18, 'subsample': 0.9, 'colsample_bytree': 0.6}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:33] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:33] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:35,190]\u001b[0m Trial 82 finished with value: 0.254160363086233 and parameters: {'n_estimators': 485, 'max_depth': 16, 'min_child_weight': 6, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:35] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:35] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:35,913]\u001b[0m Trial 83 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 382, 'max_depth': 15, 'min_child_weight': 19, 'subsample': 0.9, 'colsample_bytree': 0.5}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:35] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:35] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:36,243]\u001b[0m Trial 84 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 150, 'max_depth': 17, 'min_child_weight': 16, 'subsample': 0.9, 'colsample_bytree': 0.5}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:36] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:36] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:36,900]\u001b[0m Trial 85 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 312, 'max_depth': 14, 'min_child_weight': 15, 'subsample': 0.9, 'colsample_bytree': 0.5}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:36] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:36] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:37,496]\u001b[0m Trial 86 finished with value: 0.2571860816944024 and parameters: {'n_estimators': 233, 'max_depth': 13, 'min_child_weight': 9, 'subsample': 0.9, 'colsample_bytree': 0.6}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:37] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:37] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:38,151]\u001b[0m Trial 87 finished with value: 0.24659606656580935 and parameters: {'n_estimators': 340, 'max_depth': 16, 'min_child_weight': 18, 'subsample': 0.9, 'colsample_bytree': 0.6}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:38] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:38] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:38,992]\u001b[0m Trial 88 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 433, 'max_depth': 9, 'min_child_weight': 18, 'subsample': 0.6, 'colsample_bytree': 0.6}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:39] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:39] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:39,899]\u001b[0m Trial 89 finished with value: 0.2526475037821483 and parameters: {'n_estimators': 398, 'max_depth': 15, 'min_child_weight': 20, 'subsample': 0.9, 'colsample_bytree': 0.6}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:39] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:39] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:40,701]\u001b[0m Trial 90 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 349, 'max_depth': 17, 'min_child_weight': 18, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:40] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:40] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:41,335]\u001b[0m Trial 91 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 328, 'max_depth': 16, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.6}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:41] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:41] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:41,918]\u001b[0m Trial 92 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 298, 'max_depth': 6, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.6}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:41] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:41] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:42,624]\u001b[0m Trial 93 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 384, 'max_depth': 19, 'min_child_weight': 19, 'subsample': 0.9, 'colsample_bytree': 0.5}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:42] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:42] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:43,159]\u001b[0m Trial 94 finished with value: 0.24810892586989408 and parameters: {'n_estimators': 276, 'max_depth': 15, 'min_child_weight': 18, 'subsample': 0.9, 'colsample_bytree': 0.5}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:43] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:43] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:43,675]\u001b[0m Trial 95 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 261, 'max_depth': 15, 'min_child_weight': 19, 'subsample': 0.9, 'colsample_bytree': 0.6}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:43] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:43] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:44,547]\u001b[0m Trial 96 finished with value: 0.2526475037821483 and parameters: {'n_estimators': 476, 'max_depth': 16, 'min_child_weight': 18, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:44] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:44] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:45,186]\u001b[0m Trial 97 finished with value: 0.24810892586989408 and parameters: {'n_estimators': 349, 'max_depth': 18, 'min_child_weight': 18, 'subsample': 0.8, 'colsample_bytree': 0.5}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:45] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:45] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:46,168]\u001b[0m Trial 98 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 522, 'max_depth': 18, 'min_child_weight': 16, 'subsample': 0.8, 'colsample_bytree': 0.7}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:46] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:46] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:46,876]\u001b[0m Trial 99 finished with value: 0.24810892586989408 and parameters: {'n_estimators': 353, 'max_depth': 18, 'min_child_weight': 17, 'subsample': 0.8, 'colsample_bytree': 0.7}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:46] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:46] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:47,335]\u001b[0m Trial 100 finished with value: 0.24810892586989408 and parameters: {'n_estimators': 209, 'max_depth': 16, 'min_child_weight': 16, 'subsample': 0.8, 'colsample_bytree': 0.7}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:47] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:47] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:47,696]\u001b[0m Trial 101 finished with value: 0.24810892586989408 and parameters: {'n_estimators': 161, 'max_depth': 10, 'min_child_weight': 14, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:47] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:47] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:48,140]\u001b[0m Trial 102 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 209, 'max_depth': 10, 'min_child_weight': 14, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:48] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:48] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:48,583]\u001b[0m Trial 103 finished with value: 0.24810892586989408 and parameters: {'n_estimators': 185, 'max_depth': 14, 'min_child_weight': 15, 'subsample': 0.9, 'colsample_bytree': 0.9}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:48] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:48] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:48,947]\u001b[0m Trial 104 finished with value: 0.2526475037821483 and parameters: {'n_estimators': 132, 'max_depth': 11, 'min_child_weight': 13, 'subsample': 0.9, 'colsample_bytree': 0.9}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:48] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:48] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:49,801]\u001b[0m Trial 105 finished with value: 0.25567322239031776 and parameters: {'n_estimators': 404, 'max_depth': 16, 'min_child_weight': 11, 'subsample': 0.9, 'colsample_bytree': 0.6}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:49] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:49] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:50,600]\u001b[0m Trial 106 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 426, 'max_depth': 15, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.6}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:50] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:50] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:51,598]\u001b[0m Trial 107 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 587, 'max_depth': 17, 'min_child_weight': 20, 'subsample': 0.9, 'colsample_bytree': 0.5}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:51] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:51] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:52,519]\u001b[0m Trial 108 finished with value: 0.24810892586989408 and parameters: {'n_estimators': 473, 'max_depth': 16, 'min_child_weight': 16, 'subsample': 0.7, 'colsample_bytree': 0.8}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:52] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:52] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:53,446]\u001b[0m Trial 109 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 463, 'max_depth': 14, 'min_child_weight': 16, 'subsample': 0.9, 'colsample_bytree': 0.8}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:53] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:53] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:54,099]\u001b[0m Trial 110 finished with value: 0.2526475037821483 and parameters: {'n_estimators': 317, 'max_depth': 12, 'min_child_weight': 15, 'subsample': 0.7, 'colsample_bytree': 0.8}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:54] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:54] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:54,739]\u001b[0m Trial 111 finished with value: 0.24810892586989408 and parameters: {'n_estimators': 285, 'max_depth': 14, 'min_child_weight': 15, 'subsample': 0.9, 'colsample_bytree': 0.9}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:54] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:54] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:55,276]\u001b[0m Trial 112 finished with value: 0.24659606656580935 and parameters: {'n_estimators': 241, 'max_depth': 14, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.9}. Best is trial 64 with value: 0.24659606656580935.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:55] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:55] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:55,916]\u001b[0m Trial 113 finished with value: 0.2450832072617246 and parameters: {'n_estimators': 297, 'max_depth': 13, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.9}. Best is trial 113 with value: 0.2450832072617246.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:55] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:55] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:56,460]\u001b[0m Trial 114 finished with value: 0.24810892586989408 and parameters: {'n_estimators': 248, 'max_depth': 13, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.9}. Best is trial 113 with value: 0.2450832072617246.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:56] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:56] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:57,011]\u001b[0m Trial 115 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 252, 'max_depth': 13, 'min_child_weight': 18, 'subsample': 0.9, 'colsample_bytree': 0.9}. Best is trial 113 with value: 0.2450832072617246.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:57] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:57] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:57,288]\u001b[0m Trial 116 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 107, 'max_depth': 14, 'min_child_weight': 16, 'subsample': 0.9, 'colsample_bytree': 0.9}. Best is trial 113 with value: 0.2450832072617246.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:57] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:57] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:57,753]\u001b[0m Trial 117 finished with value: 0.254160363086233 and parameters: {'n_estimators': 224, 'max_depth': 14, 'min_child_weight': 17, 'subsample': 0.5, 'colsample_bytree': 0.9}. Best is trial 113 with value: 0.2450832072617246.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:57] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:57] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:58,113]\u001b[0m Trial 118 finished with value: 0.254160363086233 and parameters: {'n_estimators': 166, 'max_depth': 19, 'min_child_weight': 18, 'subsample': 0.8, 'colsample_bytree': 0.7}. Best is trial 113 with value: 0.2450832072617246.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:58] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:58] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:58,628]\u001b[0m Trial 119 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 267, 'max_depth': 12, 'min_child_weight': 18, 'subsample': 0.8, 'colsample_bytree': 0.5}. Best is trial 113 with value: 0.2450832072617246.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:58] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:58] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:59,160]\u001b[0m Trial 120 finished with value: 0.24810892586989408 and parameters: {'n_estimators': 291, 'max_depth': 11, 'min_child_weight': 19, 'subsample': 0.6, 'colsample_bytree': 0.5}. Best is trial 113 with value: 0.2450832072617246.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:59] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:59] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:20:59,905]\u001b[0m Trial 121 finished with value: 0.24810892586989408 and parameters: {'n_estimators': 343, 'max_depth': 15, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.9}. Best is trial 113 with value: 0.2450832072617246.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:20:59] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:20:59] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:00,526]\u001b[0m Trial 122 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 293, 'max_depth': 15, 'min_child_weight': 19, 'subsample': 0.9, 'colsample_bytree': 0.9}. Best is trial 113 with value: 0.2450832072617246.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:00] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:00] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:00,983]\u001b[0m Trial 123 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 210, 'max_depth': 13, 'min_child_weight': 16, 'subsample': 0.8, 'colsample_bytree': 0.7}. Best is trial 113 with value: 0.2450832072617246.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:01] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:01] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:01,626]\u001b[0m Trial 124 finished with value: 0.2526475037821483 and parameters: {'n_estimators': 318, 'max_depth': 12, 'min_child_weight': 16, 'subsample': 0.8, 'colsample_bytree': 0.7}. Best is trial 113 with value: 0.2450832072617246.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:01] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:01] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:02,340]\u001b[0m Trial 125 finished with value: 0.2450832072617246 and parameters: {'n_estimators': 362, 'max_depth': 8, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 113 with value: 0.2450832072617246.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:02] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:02] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:03,086]\u001b[0m Trial 126 finished with value: 0.2526475037821483 and parameters: {'n_estimators': 367, 'max_depth': 6, 'min_child_weight': 17, 'subsample': 0.8, 'colsample_bytree': 0.7}. Best is trial 113 with value: 0.2450832072617246.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:03] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:03] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:03,802]\u001b[0m Trial 127 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 355, 'max_depth': 7, 'min_child_weight': 18, 'subsample': 0.9, 'colsample_bytree': 0.8}. Best is trial 113 with value: 0.2450832072617246.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:03] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:03] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:04,551]\u001b[0m Trial 128 finished with value: 0.24810892586989408 and parameters: {'n_estimators': 391, 'max_depth': 15, 'min_child_weight': 18, 'subsample': 0.9, 'colsample_bytree': 0.5}. Best is trial 113 with value: 0.2450832072617246.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:04] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:04] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:05,864]\u001b[0m Trial 129 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 687, 'max_depth': 15, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.9}. Best is trial 113 with value: 0.2450832072617246.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:05] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:05] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:06,694]\u001b[0m Trial 130 finished with value: 0.24810892586989408 and parameters: {'n_estimators': 383, 'max_depth': 16, 'min_child_weight': 18, 'subsample': 0.9, 'colsample_bytree': 0.9}. Best is trial 113 with value: 0.2450832072617246.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:06] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:06] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:07,087]\u001b[0m Trial 131 finished with value: 0.2526475037821483 and parameters: {'n_estimators': 152, 'max_depth': 8, 'min_child_weight': 13, 'subsample': 0.9, 'colsample_bytree': 0.9}. Best is trial 113 with value: 0.2450832072617246.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:07] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:07] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:07,552]\u001b[0m Trial 132 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 238, 'max_depth': 5, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.5}. Best is trial 113 with value: 0.2450832072617246.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:07] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:07] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:07,993]\u001b[0m Trial 133 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 198, 'max_depth': 7, 'min_child_weight': 14, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 113 with value: 0.2450832072617246.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:08] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:08] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:08,270]\u001b[0m Trial 134 finished with value: 0.25567322239031776 and parameters: {'n_estimators': 109, 'max_depth': 8, 'min_child_weight': 15, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 113 with value: 0.2450832072617246.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:08] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:08] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:08,971]\u001b[0m Trial 135 finished with value: 0.24810892586989408 and parameters: {'n_estimators': 333, 'max_depth': 20, 'min_child_weight': 15, 'subsample': 0.7, 'colsample_bytree': 0.9}. Best is trial 113 with value: 0.2450832072617246.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-11 00:21:09,132]\u001b[0m Trial 136 finished with value: 0.24357034795764 and parameters: {'n_estimators': 49, 'max_depth': 9, 'min_child_weight': 16, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:09] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:09] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "[00:21:09] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:09] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:09,236]\u001b[0m Trial 137 finished with value: 0.24810892586989408 and parameters: {'n_estimators': 17, 'max_depth': 9, 'min_child_weight': 15, 'subsample': 0.9, 'colsample_bytree': 0.9}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-11 00:21:09,461]\u001b[0m Trial 138 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 79, 'max_depth': 8, 'min_child_weight': 16, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:09] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:09] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-11 00:21:09,588]\u001b[0m Trial 139 finished with value: 0.24659606656580935 and parameters: {'n_estimators': 33, 'max_depth': 10, 'min_child_weight': 16, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:09] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:09] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "[00:21:09] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:09] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:10,343]\u001b[0m Trial 140 finished with value: 0.24659606656580935 and parameters: {'n_estimators': 348, 'max_depth': 18, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-11 00:21:10,508]\u001b[0m Trial 141 finished with value: 0.24659606656580935 and parameters: {'n_estimators': 50, 'max_depth': 17, 'min_child_weight': 16, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:10] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:10] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "[00:21:10] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:10] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:10,648]\u001b[0m Trial 142 finished with value: 0.24659606656580935 and parameters: {'n_estimators': 36, 'max_depth': 17, 'min_child_weight': 16, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-11 00:21:10,800]\u001b[0m Trial 143 finished with value: 0.24810892586989408 and parameters: {'n_estimators': 43, 'max_depth': 17, 'min_child_weight': 16, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:10] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:10] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "[00:21:10] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:10] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:10,923]\u001b[0m Trial 144 finished with value: 0.254160363086233 and parameters: {'n_estimators': 30, 'max_depth': 10, 'min_child_weight': 17, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-11 00:21:11,070]\u001b[0m Trial 145 finished with value: 0.2526475037821483 and parameters: {'n_estimators': 46, 'max_depth': 10, 'min_child_weight': 19, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:10] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:10] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "[00:21:11] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:11] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:11,265]\u001b[0m Trial 146 finished with value: 0.24659606656580935 and parameters: {'n_estimators': 69, 'max_depth': 9, 'min_child_weight': 16, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-11 00:21:11,426]\u001b[0m Trial 147 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 51, 'max_depth': 9, 'min_child_weight': 16, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:11] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:11] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "[00:21:11] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:11] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:11,608]\u001b[0m Trial 148 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 62, 'max_depth': 17, 'min_child_weight': 16, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-11 00:21:11,683]\u001b[0m Trial 149 finished with value: 0.2677760968229954 and parameters: {'n_estimators': 6, 'max_depth': 9, 'min_child_weight': 17, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-11 00:21:11,870]\u001b[0m Trial 150 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 71, 'max_depth': 17, 'min_child_weight': 17, 'subsample': 0.6, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:11] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:11] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "[00:21:11] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:11] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-11 00:21:11,984]\u001b[0m Trial 151 finished with value: 0.24659606656580935 and parameters: {'n_estimators': 25, 'max_depth': 18, 'min_child_weight': 16, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-11 00:21:12,092]\u001b[0m Trial 152 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 24, 'max_depth': 18, 'min_child_weight': 17, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:11] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:11] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "[00:21:12] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:12] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:12] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:12] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:12,339]\u001b[0m Trial 153 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 101, 'max_depth': 8, 'min_child_weight': 16, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-11 00:21:12,406]\u001b[0m Trial 154 finished with value: 0.2708018154311649 and parameters: {'n_estimators': 3, 'max_depth': 16, 'min_child_weight': 17, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:12] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:12] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "[00:21:12] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:12] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:12,581]\u001b[0m Trial 155 finished with value: 0.24659606656580935 and parameters: {'n_estimators': 57, 'max_depth': 17, 'min_child_weight': 15, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-11 00:21:12,689]\u001b[0m Trial 156 finished with value: 0.24659606656580935 and parameters: {'n_estimators': 24, 'max_depth': 18, 'min_child_weight': 15, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-11 00:21:12,817]\u001b[0m Trial 157 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 34, 'max_depth': 19, 'min_child_weight': 16, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:12] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:12] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "[00:21:12] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:12] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-11 00:21:13,045]\u001b[0m Trial 158 finished with value: 0.2526475037821483 and parameters: {'n_estimators': 82, 'max_depth': 18, 'min_child_weight': 16, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:12] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:12] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-11 00:21:13,235]\u001b[0m Trial 159 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 67, 'max_depth': 18, 'min_child_weight': 15, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:13] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:13] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "[00:21:13] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:13] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-11 00:21:13,419]\u001b[0m Trial 160 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 53, 'max_depth': 17, 'min_child_weight': 16, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-11 00:21:13,484]\u001b[0m Trial 161 finished with value: 0.26475037821482605 and parameters: {'n_estimators': 1, 'max_depth': 19, 'min_child_weight': 17, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-11 00:21:13,591]\u001b[0m Trial 162 finished with value: 0.24659606656580935 and parameters: {'n_estimators': 24, 'max_depth': 20, 'min_child_weight': 15, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:13] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:13] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "[00:21:13] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:13] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "[00:21:13] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:13] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:13,696]\u001b[0m Trial 163 finished with value: 0.24810892586989408 and parameters: {'n_estimators': 20, 'max_depth': 20, 'min_child_weight': 15, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:13] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:13] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:13,933]\u001b[0m Trial 164 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 90, 'max_depth': 17, 'min_child_weight': 15, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-11 00:21:14,112]\u001b[0m Trial 165 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 62, 'max_depth': 18, 'min_child_weight': 16, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:13] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:13] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "[00:21:14] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:14] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:14,272]\u001b[0m Trial 166 finished with value: 0.25567322239031776 and parameters: {'n_estimators': 48, 'max_depth': 17, 'min_child_weight': 17, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-11 00:21:14,395]\u001b[0m Trial 167 finished with value: 0.24357034795764 and parameters: {'n_estimators': 29, 'max_depth': 19, 'min_child_weight': 16, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:14] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:14] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "[00:21:14] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:14] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:14,679]\u001b[0m Trial 168 finished with value: 0.24659606656580935 and parameters: {'n_estimators': 116, 'max_depth': 20, 'min_child_weight': 15, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:14] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:14] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:14,936]\u001b[0m Trial 169 finished with value: 0.24810892586989408 and parameters: {'n_estimators': 101, 'max_depth': 20, 'min_child_weight': 15, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-11 00:21:15,042]\u001b[0m Trial 170 finished with value: 0.2526475037821483 and parameters: {'n_estimators': 20, 'max_depth': 19, 'min_child_weight': 18, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:14] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:14] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "[00:21:15] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:15] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:15,321]\u001b[0m Trial 171 finished with value: 0.254160363086233 and parameters: {'n_estimators': 117, 'max_depth': 18, 'min_child_weight': 16, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-11 00:21:15,459]\u001b[0m Trial 172 finished with value: 0.24810892586989408 and parameters: {'n_estimators': 37, 'max_depth': 19, 'min_child_weight': 16, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:15] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:15] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "[00:21:15] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:15] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:15,655]\u001b[0m Trial 173 finished with value: 0.24659606656580935 and parameters: {'n_estimators': 71, 'max_depth': 17, 'min_child_weight': 17, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-11 00:21:15,859]\u001b[0m Trial 174 finished with value: 0.24810892586989408 and parameters: {'n_estimators': 73, 'max_depth': 17, 'min_child_weight': 17, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:15] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:15] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:15] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:15] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:16,147]\u001b[0m Trial 175 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 123, 'max_depth': 16, 'min_child_weight': 17, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-11 00:21:16,329]\u001b[0m Trial 176 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 57, 'max_depth': 17, 'min_child_weight': 17, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-11 00:21:16,391]\u001b[0m Trial 177 finished with value: 0.5037821482602118 and parameters: {'n_estimators': 0, 'max_depth': 18, 'min_child_weight': 16, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:16] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:16] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "[00:21:16] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-11 00:21:16,506]\u001b[0m Trial 178 finished with value: 0.24810892586989408 and parameters: {'n_estimators': 26, 'max_depth': 19, 'min_child_weight': 16, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:16] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:16] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "[00:21:16] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:16] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:16,750]\u001b[0m Trial 179 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 90, 'max_depth': 20, 'min_child_weight': 15, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:16] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:16] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:17,595]\u001b[0m Trial 180 finished with value: 0.254160363086233 and parameters: {'n_estimators': 315, 'max_depth': 20, 'min_child_weight': 4, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-11 00:21:17,733]\u001b[0m Trial 181 finished with value: 0.24810892586989408 and parameters: {'n_estimators': 36, 'max_depth': 19, 'min_child_weight': 15, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-11 00:21:17,838]\u001b[0m Trial 182 finished with value: 0.2526475037821483 and parameters: {'n_estimators': 22, 'max_depth': 18, 'min_child_weight': 18, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:17] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:17] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "[00:21:17] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:17] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-11 00:21:17,989]\u001b[0m Trial 183 finished with value: 0.24810892586989408 and parameters: {'n_estimators': 43, 'max_depth': 9, 'min_child_weight': 16, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:17] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:17] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "[00:21:18] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:18] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:18,214]\u001b[0m Trial 184 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 82, 'max_depth': 20, 'min_child_weight': 17, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:18] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:18] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:18,891]\u001b[0m Trial 185 finished with value: 0.254160363086233 and parameters: {'n_estimators': 337, 'max_depth': 16, 'min_child_weight': 14, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-11 00:21:19,073]\u001b[0m Trial 186 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 64, 'max_depth': 17, 'min_child_weight': 17, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:18] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:18] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "[00:21:19] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:19] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-11 00:21:19,172]\u001b[0m Trial 187 finished with value: 0.2571860816944024 and parameters: {'n_estimators': 15, 'max_depth': 19, 'min_child_weight': 15, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:19] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:19] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:19,883]\u001b[0m Trial 188 finished with value: 0.25567322239031776 and parameters: {'n_estimators': 372, 'max_depth': 20, 'min_child_weight': 16, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-11 00:21:20,063]\u001b[0m Trial 189 finished with value: 0.24810892586989408 and parameters: {'n_estimators': 47, 'max_depth': 10, 'min_child_weight': 16, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:19] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:19] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "[00:21:20] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:20] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:20,336]\u001b[0m Trial 190 finished with value: 0.2526475037821483 and parameters: {'n_estimators': 95, 'max_depth': 9, 'min_child_weight': 18, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-11 00:21:20,552]\u001b[0m Trial 191 finished with value: 0.24659606656580935 and parameters: {'n_estimators': 69, 'max_depth': 16, 'min_child_weight': 17, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:20] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:20] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-11 00:21:20,697]\u001b[0m Trial 192 finished with value: 0.24810892586989408 and parameters: {'n_estimators': 35, 'max_depth': 16, 'min_child_weight': 16, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:20] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:20] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "[00:21:20] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:20] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:20,920]\u001b[0m Trial 193 finished with value: 0.24810892586989408 and parameters: {'n_estimators': 68, 'max_depth': 17, 'min_child_weight': 17, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-11 00:21:21,089]\u001b[0m Trial 194 finished with value: 0.24357034795764 and parameters: {'n_estimators': 51, 'max_depth': 16, 'min_child_weight': 15, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:20] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:20] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "[00:21:21] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:21] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-11 00:21:21,306]\u001b[0m Trial 195 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 61, 'max_depth': 16, 'min_child_weight': 7, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-11 00:21:21,454]\u001b[0m Trial 196 finished with value: 0.2450832072617246 and parameters: {'n_estimators': 40, 'max_depth': 18, 'min_child_weight': 14, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-11 00:21:21,516]\u001b[0m Trial 197 finished with value: 0.26475037821482605 and parameters: {'n_estimators': 1, 'max_depth': 18, 'min_child_weight': 14, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:21] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:21] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "[00:21:21] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:21] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-11 00:21:21,745]\u001b[0m Trial 198 finished with value: 0.2526475037821483 and parameters: {'n_estimators': 84, 'max_depth': 17, 'min_child_weight': 15, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:21] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:21] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-11 00:21:21,902]\u001b[0m Trial 199 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 47, 'max_depth': 19, 'min_child_weight': 14, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:21] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:21] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "[00:21:21] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:21] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:22,037]\u001b[0m Trial 200 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 31, 'max_depth': 18, 'min_child_weight': 15, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-11 00:21:22,164]\u001b[0m Trial 201 finished with value: 0.254160363086233 and parameters: {'n_estimators': 28, 'max_depth': 16, 'min_child_weight': 17, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:22] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:22] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "[00:21:22] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:22] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:22,846]\u001b[0m Trial 202 finished with value: 0.2526475037821483 and parameters: {'n_estimators': 359, 'max_depth': 16, 'min_child_weight': 17, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-11 00:21:23,054]\u001b[0m Trial 203 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 73, 'max_depth': 8, 'min_child_weight': 16, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:22] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:22] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-11 00:21:23,158]\u001b[0m Trial 204 finished with value: 0.254160363086233 and parameters: {'n_estimators': 18, 'max_depth': 18, 'min_child_weight': 15, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:23] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:23] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "[00:21:23] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:23] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:23,810]\u001b[0m Trial 205 finished with value: 0.254160363086233 and parameters: {'n_estimators': 312, 'max_depth': 17, 'min_child_weight': 14, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:23] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:23] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:24,078]\u001b[0m Trial 206 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 107, 'max_depth': 17, 'min_child_weight': 16, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:24] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:24] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:24,774]\u001b[0m Trial 207 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 338, 'max_depth': 19, 'min_child_weight': 15, 'subsample': 0.9, 'colsample_bytree': 0.6}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-11 00:21:24,945]\u001b[0m Trial 208 finished with value: 0.24659606656580935 and parameters: {'n_estimators': 54, 'max_depth': 18, 'min_child_weight': 15, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:24] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:24] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "[00:21:25] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:25] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:25,116]\u001b[0m Trial 209 finished with value: 0.24357034795764 and parameters: {'n_estimators': 51, 'max_depth': 18, 'min_child_weight': 15, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-11 00:21:25,287]\u001b[0m Trial 210 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 50, 'max_depth': 18, 'min_child_weight': 14, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 136 with value: 0.24357034795764.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:25] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:25] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "[00:21:25] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:25] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:25,464]\u001b[0m Trial 211 finished with value: 0.24205748865355525 and parameters: {'n_estimators': 58, 'max_depth': 18, 'min_child_weight': 15, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-11 00:21:25,531]\u001b[0m Trial 212 finished with value: 0.5037821482602118 and parameters: {'n_estimators': 0, 'max_depth': 19, 'min_child_weight': 16, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:25] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:25] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:25] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:25,743]\u001b[0m Trial 213 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 75, 'max_depth': 18, 'min_child_weight': 15, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-11 00:21:25,865]\u001b[0m Trial 214 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 27, 'max_depth': 18, 'min_child_weight': 15, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:25] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:25] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "[00:21:25] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:25] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:26,178]\u001b[0m Trial 215 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 132, 'max_depth': 15, 'min_child_weight': 18, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-11 00:21:26,312]\u001b[0m Trial 216 finished with value: 0.24810892586989408 and parameters: {'n_estimators': 35, 'max_depth': 19, 'min_child_weight': 15, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:26] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:26] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "[00:21:26] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:26] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:26,493]\u001b[0m Trial 217 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 58, 'max_depth': 18, 'min_child_weight': 16, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-11 00:21:26,689]\u001b[0m Trial 218 finished with value: 0.24659606656580935 and parameters: {'n_estimators': 69, 'max_depth': 17, 'min_child_weight': 16, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:26] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:26] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:26] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:26] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:26,962]\u001b[0m Trial 219 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 98, 'max_depth': 17, 'min_child_weight': 15, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-11 00:21:27,204]\u001b[0m Trial 220 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 87, 'max_depth': 17, 'min_child_weight': 16, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:27] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:27] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:27] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:27] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:27,827]\u001b[0m Trial 221 finished with value: 0.2450832072617246 and parameters: {'n_estimators': 303, 'max_depth': 16, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:27] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:27] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:28,453]\u001b[0m Trial 222 finished with value: 0.2450832072617246 and parameters: {'n_estimators': 304, 'max_depth': 16, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:28] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:28] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:29,086]\u001b[0m Trial 223 finished with value: 0.2450832072617246 and parameters: {'n_estimators': 306, 'max_depth': 16, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:29] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:29] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:29,699]\u001b[0m Trial 224 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 300, 'max_depth': 16, 'min_child_weight': 18, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:29] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:29] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:30,329]\u001b[0m Trial 225 finished with value: 0.2450832072617246 and parameters: {'n_estimators': 310, 'max_depth': 20, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:30] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:30] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:30,761]\u001b[0m Trial 226 finished with value: 0.2526475037821483 and parameters: {'n_estimators': 286, 'max_depth': 2, 'min_child_weight': 18, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:30] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:30] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:31,394]\u001b[0m Trial 227 finished with value: 0.2450832072617246 and parameters: {'n_estimators': 304, 'max_depth': 20, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:31] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:31] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:32,005]\u001b[0m Trial 228 finished with value: 0.2526475037821483 and parameters: {'n_estimators': 306, 'max_depth': 15, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.6}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:32] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:32] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:32,575]\u001b[0m Trial 229 finished with value: 0.2450832072617246 and parameters: {'n_estimators': 271, 'max_depth': 20, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:32] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:32] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:33,139]\u001b[0m Trial 230 finished with value: 0.24659606656580935 and parameters: {'n_estimators': 269, 'max_depth': 20, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:33] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:33] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:33,801]\u001b[0m Trial 231 finished with value: 0.2450832072617246 and parameters: {'n_estimators': 295, 'max_depth': 20, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:33] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:33] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:34,409]\u001b[0m Trial 232 finished with value: 0.2450832072617246 and parameters: {'n_estimators': 288, 'max_depth': 20, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:34] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:34] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:34,994]\u001b[0m Trial 233 finished with value: 0.2450832072617246 and parameters: {'n_estimators': 280, 'max_depth': 20, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:35] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:35] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:35,622]\u001b[0m Trial 234 finished with value: 0.2450832072617246 and parameters: {'n_estimators': 282, 'max_depth': 20, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:35] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:35] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:36,236]\u001b[0m Trial 235 finished with value: 0.24659606656580935 and parameters: {'n_estimators': 262, 'max_depth': 20, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:36] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:36] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:36,899]\u001b[0m Trial 236 finished with value: 0.2450832072617246 and parameters: {'n_estimators': 283, 'max_depth': 20, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:36] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:36] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:37,550]\u001b[0m Trial 237 finished with value: 0.2450832072617246 and parameters: {'n_estimators': 283, 'max_depth': 20, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:37] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:37] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:38,212]\u001b[0m Trial 238 finished with value: 0.24659606656580935 and parameters: {'n_estimators': 286, 'max_depth': 20, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:38] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:38] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:38,825]\u001b[0m Trial 239 finished with value: 0.24659606656580935 and parameters: {'n_estimators': 279, 'max_depth': 20, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:38] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:38] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:39,382]\u001b[0m Trial 240 finished with value: 0.24659606656580935 and parameters: {'n_estimators': 257, 'max_depth': 20, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:39] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:39] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:40,013]\u001b[0m Trial 241 finished with value: 0.2450832072617246 and parameters: {'n_estimators': 305, 'max_depth': 20, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:40] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:40] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:40,612]\u001b[0m Trial 242 finished with value: 0.2450832072617246 and parameters: {'n_estimators': 290, 'max_depth': 20, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:40] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:40] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:41,243]\u001b[0m Trial 243 finished with value: 0.2450832072617246 and parameters: {'n_estimators': 302, 'max_depth': 20, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:41] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:41] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:41,837]\u001b[0m Trial 244 finished with value: 0.24659606656580935 and parameters: {'n_estimators': 285, 'max_depth': 20, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:41] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:41] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:42,455]\u001b[0m Trial 245 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 304, 'max_depth': 20, 'min_child_weight': 18, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:42] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:42] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:43,079]\u001b[0m Trial 246 finished with value: 0.2450832072617246 and parameters: {'n_estimators': 298, 'max_depth': 20, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:43] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:43] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:43,728]\u001b[0m Trial 247 finished with value: 0.2450832072617246 and parameters: {'n_estimators': 316, 'max_depth': 20, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:43] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:43] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:44,390]\u001b[0m Trial 248 finished with value: 0.2450832072617246 and parameters: {'n_estimators': 319, 'max_depth': 20, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:44] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:44] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:45,041]\u001b[0m Trial 249 finished with value: 0.2450832072617246 and parameters: {'n_estimators': 318, 'max_depth': 20, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:45] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:45] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:45,688]\u001b[0m Trial 250 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 318, 'max_depth': 20, 'min_child_weight': 18, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:45] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:45] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:46,351]\u001b[0m Trial 251 finished with value: 0.24659606656580935 and parameters: {'n_estimators': 322, 'max_depth': 20, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:46] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:46] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:46,932]\u001b[0m Trial 252 finished with value: 0.2450832072617246 and parameters: {'n_estimators': 267, 'max_depth': 20, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:46] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:46] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:47,514]\u001b[0m Trial 253 finished with value: 0.24659606656580935 and parameters: {'n_estimators': 266, 'max_depth': 20, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:47] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:47] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:48,047]\u001b[0m Trial 254 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 247, 'max_depth': 20, 'min_child_weight': 18, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:48] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:48] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:48,639]\u001b[0m Trial 255 finished with value: 0.2450832072617246 and parameters: {'n_estimators': 281, 'max_depth': 19, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:48] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:48] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:49,216]\u001b[0m Trial 256 finished with value: 0.2450832072617246 and parameters: {'n_estimators': 268, 'max_depth': 19, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:49] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:49] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:49,825]\u001b[0m Trial 257 finished with value: 0.2450832072617246 and parameters: {'n_estimators': 296, 'max_depth': 20, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:49] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:49] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:50,499]\u001b[0m Trial 258 finished with value: 0.24659606656580935 and parameters: {'n_estimators': 329, 'max_depth': 20, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:50] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:50] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:51,009]\u001b[0m Trial 259 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 230, 'max_depth': 19, 'min_child_weight': 18, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:51] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:51] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:51,614]\u001b[0m Trial 260 finished with value: 0.24659606656580935 and parameters: {'n_estimators': 286, 'max_depth': 19, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:51] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:51] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:52,176]\u001b[0m Trial 261 finished with value: 0.24659606656580935 and parameters: {'n_estimators': 266, 'max_depth': 19, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:52] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:52] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:52,825]\u001b[0m Trial 262 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 318, 'max_depth': 19, 'min_child_weight': 18, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:52] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:52] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:53,455]\u001b[0m Trial 263 finished with value: 0.2450832072617246 and parameters: {'n_estimators': 305, 'max_depth': 20, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:53] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:53] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:54,073]\u001b[0m Trial 264 finished with value: 0.2450832072617246 and parameters: {'n_estimators': 293, 'max_depth': 20, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:54] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:54] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:54,685]\u001b[0m Trial 265 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 292, 'max_depth': 20, 'min_child_weight': 18, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:54] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:54] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:55,408]\u001b[0m Trial 266 finished with value: 0.2526475037821483 and parameters: {'n_estimators': 302, 'max_depth': 20, 'min_child_weight': 10, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:55] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:55] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:55,944]\u001b[0m Trial 267 finished with value: 0.2450832072617246 and parameters: {'n_estimators': 246, 'max_depth': 19, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:56] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:56] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:56,485]\u001b[0m Trial 268 finished with value: 0.24659606656580935 and parameters: {'n_estimators': 251, 'max_depth': 20, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:56] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:56] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:57,122]\u001b[0m Trial 269 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 306, 'max_depth': 20, 'min_child_weight': 18, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:57] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:57] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:57,789]\u001b[0m Trial 270 finished with value: 0.24659606656580935 and parameters: {'n_estimators': 325, 'max_depth': 20, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:57] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:57] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:58,404]\u001b[0m Trial 271 finished with value: 0.2450832072617246 and parameters: {'n_estimators': 293, 'max_depth': 20, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:58] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:58] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:58,993]\u001b[0m Trial 272 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 282, 'max_depth': 19, 'min_child_weight': 18, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:59] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:59] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:21:59,835]\u001b[0m Trial 273 finished with value: 0.25567322239031776 and parameters: {'n_estimators': 241, 'max_depth': 20, 'min_child_weight': 2, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:21:59] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:21:59] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:22:00,597]\u001b[0m Trial 274 finished with value: 0.24659606656580935 and parameters: {'n_estimators': 331, 'max_depth': 19, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:22:00] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:22:00] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:22:01,256]\u001b[0m Trial 275 finished with value: 0.2450832072617246 and parameters: {'n_estimators': 282, 'max_depth': 20, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:22:01] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:22:01] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:22:01,825]\u001b[0m Trial 276 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 265, 'max_depth': 19, 'min_child_weight': 16, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:22:01] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:22:01] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:22:02,305]\u001b[0m Trial 277 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 222, 'max_depth': 19, 'min_child_weight': 18, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:22:02] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:22:02] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:22:02,959]\u001b[0m Trial 278 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 317, 'max_depth': 20, 'min_child_weight': 16, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:22:03] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:22:03] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:22:03,652]\u001b[0m Trial 279 finished with value: 0.24659606656580935 and parameters: {'n_estimators': 341, 'max_depth': 19, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:22:03] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:22:03] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:22:04,277]\u001b[0m Trial 280 finished with value: 0.2450832072617246 and parameters: {'n_estimators': 298, 'max_depth': 20, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:22:04] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:22:04] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:22:04,917]\u001b[0m Trial 281 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 297, 'max_depth': 20, 'min_child_weight': 16, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:22:04] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:22:04] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:22:05,572]\u001b[0m Trial 282 finished with value: 0.2450832072617246 and parameters: {'n_estimators': 317, 'max_depth': 20, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:22:05] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:22:05] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:22:06,105]\u001b[0m Trial 283 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 250, 'max_depth': 19, 'min_child_weight': 18, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:22:06] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:22:06] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:22:06,774]\u001b[0m Trial 284 finished with value: 0.24659606656580935 and parameters: {'n_estimators': 325, 'max_depth': 19, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:22:06] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:22:06] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:22:07,350]\u001b[0m Trial 285 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 274, 'max_depth': 20, 'min_child_weight': 18, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:22:07] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:22:07] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:22:07,929]\u001b[0m Trial 286 finished with value: 0.2450832072617246 and parameters: {'n_estimators': 271, 'max_depth': 20, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:22:07] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:22:07] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:22:08,521]\u001b[0m Trial 287 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 272, 'max_depth': 20, 'min_child_weight': 16, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:22:08] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:22:08] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:22:09,198]\u001b[0m Trial 288 finished with value: 0.2450832072617246 and parameters: {'n_estimators': 335, 'max_depth': 19, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:22:09] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:22:09] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:22:09,956]\u001b[0m Trial 289 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 362, 'max_depth': 19, 'min_child_weight': 16, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:22:10] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:22:10] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:22:10,538]\u001b[0m Trial 290 finished with value: 0.24659606656580935 and parameters: {'n_estimators': 265, 'max_depth': 20, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:22:10] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:22:10] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:22:11,077]\u001b[0m Trial 291 finished with value: 0.2450832072617246 and parameters: {'n_estimators': 246, 'max_depth': 19, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:22:11] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:22:11] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:22:11,717]\u001b[0m Trial 292 finished with value: 0.25113464447806355 and parameters: {'n_estimators': 308, 'max_depth': 20, 'min_child_weight': 18, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:22:11] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:22:11] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:22:12,450]\u001b[0m Trial 293 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 351, 'max_depth': 19, 'min_child_weight': 16, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:22:12] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:22:12] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:22:12,959]\u001b[0m Trial 294 finished with value: 0.24357034795764 and parameters: {'n_estimators': 234, 'max_depth': 19, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:22:13] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:22:13] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:22:13,585]\u001b[0m Trial 295 finished with value: 0.2450832072617246 and parameters: {'n_estimators': 299, 'max_depth': 19, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:22:13] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:22:13] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:22:14,071]\u001b[0m Trial 296 finished with value: 0.24962178517397882 and parameters: {'n_estimators': 224, 'max_depth': 19, 'min_child_weight': 18, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:22:14] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:22:14] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:22:14,607]\u001b[0m Trial 297 finished with value: 0.2450832072617246 and parameters: {'n_estimators': 246, 'max_depth': 19, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:22:14] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:22:14] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:22:15,143]\u001b[0m Trial 298 finished with value: 0.24357034795764 and parameters: {'n_estimators': 241, 'max_depth': 19, 'min_child_weight': 17, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  if sys.path[0] == \"\":\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  del sys.path[0]\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:22:15] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:22:15] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-11 00:22:15,613]\u001b[0m Trial 299 finished with value: 0.24659606656580935 and parameters: {'n_estimators': 202, 'max_depth': 19, 'min_child_weight': 16, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 211 with value: 0.24205748865355525.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "study = XGBoostClassifier_opt(X_res, X_test, y_res, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKzYxCTjxXo-",
        "outputId": "51c1bcca-d15a-4db0-d696-94bd7b780a30"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:22:59] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"dtrain\", \"early_stopping_rounds\", \"evals\", \"num_boost_round\", \"params\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[00:22:59] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "accuracy scores 0.75642965204236\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiAAAAGwCAYAAACQB97CAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAABnbElEQVR4nO3deVxU5f4H8M8szMK+LwoiruCOuLBct1RcyrQ0Sc2lNJe0NLr1k1vmVpGWa4VLpmTXhcos81pKZi5ppiZm7juKIIIsss16fn+gU+MAMjrD4PB59zqvl/OcZ57zPaPEd57tiARBEEBERERUg8S2DoCIiIjqHiYgREREVOOYgBAREVGNYwJCRERENY4JCBEREdU4JiBERERU45iAEBERUY1jAkJEREQ1TmrrAKyh4fT/4faZ0yi5fMnWodRpSrkEaxIew/OJP6NUpbN1OAQg79DHtg6BAGg1GuzcsQ09Y/tD6uBg63DqPEUN/CZUhk+xSDulR+3nZ9hue0BkXt62DoGIiIgqYZc9IAAg8/AAxGJAr7d1KEREVNeJ7Pb7/gOzy09EV1oKkUQCmYenrUMhIiICRCLLHHbELhMQdW4uAEDm5WXjSIiIiFDeA2KJw47Y193coc7NAQDIvDkPhIiIqDayyzkgqjs9IA4urhDLZNCr1TaOiMiUVCKGv5czxOKa61ZVlZXV2LWoclqtBlKpFCpVGXQ6rhCzGhEglTpAIpHYOhK7Gz6xBLtMQASNGprCAji4ukHm5Y2yzOu2DonIiI+HIxIn94KXuyNENfg/pox0Lk2vDQQI8Pf3R9b1qxCBv5iszcXNHd6+/jX6s2bCzoZPLMEuExAAUOfk3ElAvJiAUK0iEgHjBrVHw0BvKJzca/TaDUPq1ej1qGKCoEdxURGcnJ0h4i8mqxEEASUlJbiZnQ0A8PELsHFEtpGUlIQPPvgAmZmZaNmyJRYvXowuXbrc932//vorunXrhlatWiEtLc3o3KZNmzBjxgxcuHABjRs3xrvvvounnnrKrLjsNwHJzYVTo8bcD4RqHTdnBSJC60Hh6AqIa/ZHUKFQ1Oj1qGKCoIdGrYZCoWACYmVKpRIAcDM7G57evrYbjrFR70tKSgqmTZuGpKQkxMTEYMWKFejXrx9OnjyJBg0aVPq+goICjBo1Cj179sSNGzeMzh04cABxcXGYO3cunnrqKWzevBlDhw7Fvn370Llz52rHZrf/8tX5eRB0OkgUCkicnW0dDpGBi6MMEon9zWgnqq0cHR0BlM+9sRkbrYJZuHAhxo4di3HjxiEsLAyLFy9GUFAQli1bVuX7JkyYgOHDhyMqKsrk3OLFi9G7d28kJCQgNDQUCQkJ6NmzJxYvXmxWbPb7f0C9Huq8WwAAOXtBqBYRiUS2HYsmqmMMP2+CbeOwBJVKhcLCQqNDpVJVWFetVuPIkSOIjY01Ko+NjcX+/fsrvcaaNWtw4cIFzJw5s8LzBw4cMGmzT58+VbZZEftNQFA+DwTgtuxERGRjFtqILDExEW5ubkZHYmJihZfMycmBTqeDn5+fUbmfnx+ysrIqfM+5c+cwffp0rFu3DlJpxUPEWVlZZrVZGbudAwL8Yz8QT8/yriuB27ITEZENWGjINSEhAfHx8UZlcrm86kvf0+MqCEKFvbA6nQ7Dhw/H7Nmz0axZM4u0WRW77gHRFhVBpyqDSCKBg4e7rcMhojoqNzcX/n6+uHz5sq1DsTuffPwxBg580tZh1Bi5XA5XV1ejo7IExNvbGxKJxKRnIjs726QHAwBu376Nw4cPY8qUKZBKpZBKpZgzZw6OHTsGqVSKn3/+GQDKl5BXs82q2HUCAgDqnPJNyTgPhOjhPP/8GEjEIkjEIsgcpGgY3AAvTZqEvLw8k7r79+/H44/3h5enBxyVCrRt0xoLFyyocNOtXbt24fHH+8PH2wvOTo5o1bIF/v3aa8jIyKiJ26oR7ycm4oknBqBhw4a2DsVqdu/ejY4dIuCoVKBJ40ZYvnx5lfWTk5MN/57uPbLvLJsFyr9ZL/jwQ4Q2bwalQo7gBkFIfO89w/lxL76Iw4cOYd++fVa7N4uwwbNgZDIZIiIikJqaalSempqK6Ohok/qurq44fvw40tLSDMfEiRPRvHlzpKWlGVa4REVFmbS5Y8eOCtusiv0nILmcB0JkKX369kXG9UxcvHQZKz9dha1bv8fkyS8Z1dm8eTN6dO+GwPqB2PnzLpw8dRovvzIV7733LoYNexaC8PdMwBUrViC2dy/4+/njq6834a8TJ5G0bDkKCgqwcMGCGrsvtRV3Sy4tLcXq1Z9h7LhxD9WONWN8WJcuXcITj/fHv/7VBUf+OIrpCf/BtKmvYNOmTZW+Jy4uDhnXM42O2D590K1bN/j6+hrqTZs6FZ99tgrzP/gQJ0+dxndbvkfHTp0M5+VyOYYNG46PP/7Iqvf40Gy0CiY+Ph6rVq3C6tWrcerUKbz66qtIT0/HxIkTAZQP6YwaNQoAIBaL0apVK6PD19cXCoUCrVq1gpOTEwBg6tSp2LFjB+bNm4fTp09j3rx5+OmnnzBt2jSzYqsDCUh5D4jU1RUiB5mNoyEyJQgCyjT6GjlK1Fqj45/JQHXI5XL4+/sjMDAQsbGxGDo0Dqk7dhjOFxcXY8L4FzHgySexYuVKtGvXDg0bNsS4ceOwJvlzbPr6a3z55ZcAgGvXrmHa1Ffw8suv4LPVq9G9e3c0bNgQXbt2xaerVmHG229XGkd+fj4mjB+PAH8/OCoVaNO6FbZu3QoAmD1rFtqHtzOqv2TxYjQKaWh4/fzzY/DUU4PwfmIiAuvXQ2jzZvhPQgKioyJNrtWubRvM+sdqgDVr1qBlizA4KhVoERaKZUlJVX5mP/zwA6RSqdFyRp1Oh5dffhlNGjeGk6MSYaHNsXTJEqP3VRQjAGRkZODZZ+Pg5ekBH28vDBo00Gho59ChQ4iN7Q1fH294uLuhR/du+OOPP6qM8WGtWL4cDRo0wKLFixEWFoZx48bh+edfwMIFH1b6HqVSCX9/f8MhkUiw6+ef8fwLYw11Tp06heXLl2Hzt9/hySefREhICNq1a4devXoZtTXgySfx3bfforS01Gr3+NBs9DTcuLg4LF68GHPmzEG7du2wZ88ebNu2DcHBwQCAzMxMpKenm9VmdHQ0Nm7ciDVr1qBNmzZITk5GSkqKWXuAAHY+CRUA9GoVNLcL4eDiCpmXF1RZmbYOiciISitgxIYTNXQ14+v8NbsPHGUP9r+BixcvYvv2H+Hg4GAo27FjB3Jzc/Haa/82qT9gwAA0a9YMGzduQFxcHL7+6iuo1Wq8/sYbFbbv7u5eYbler8fj/fvh9u3bWPvFf9G4cWOcPHnS7A2mft65E66urti+I9WQiM2b975hZ0cAOHHiBI4fP44vv/oaAPDpp59i9qyZWPrRxwgPD8fRo0cxYfyLcHRywujRoyu8zt49exDRoYPJPdSrVw8bNm6Ej48v9u/fj4kTxsM/IABDhw6tNMaSkhL0fKwH/vWvLvhl9x5IpVK8++476N+vL9KO/QmZTIbbt29j1KjRWLJkKQBg4YIFeOLx/jhz9hxcXFwqjHHdunWYNHFClZ/XsuUrMGLEiArP/fbbAfTufc9Szz59sHr1Z9BoNEb/Rirzxdq1cHR0xJAhQwxlW7//Ho0aNcL/tm5F/359IQgCevbshXnz58PT09NQr0OHDtBoNPj999/RrVu3+16rrnnppZfw0ksvVXguOTm5yvfOmjULs2bNMikfMmSI0d/Vg7D7BAS4sy27iyvkTECIHsr/tm6Fq4szdDodyu482G7BgoWG8+fOngUAhIWFVfj+5qGhhjrnzp2Dq6srAgLM2x77p59+wu+//44TJ08ZZuo3atTI7HtxcnLCp5+ugkz2d89omzZtsGH9erw1YwYAYP26dejYsaPhOu++MxcffLgATz/9NAAgJCQEp06exKcrV1SagFy+chn1Aoy3wHdwcEBCQgJcXF0hEokREhKCA/v346uvvjRKQO6NcfXq1RCLxfh01SrDioPVq9fA08Mdv/zyC2JjY/HYY48ZXWv5ihXw8vTA7t278cQTT1QY45NPPnnfb69VTTCsbFmmVqtFTk5Otf6O16xZjWHDhht2LgXKk9wrV67g66+/QvLna6HT6fBa/KsY+swQ/LTzZ0M9JycnuLu74/Lly7U3AeHGgybqRgKSmwunkEacB0K1klwqwrphLWvkWm1DA41eKx3M6zXo3qMHkpKWoaSkBJ+tWoWz585iyssvm9SrbGjnn0v1HmTZHgAcS0tDYGDgfZcJ3k+r1q2Nkg8AGD58BNasWY23ZsyAIAjYuHEDpk6dBgC4efMmrl69ihfHjcWE8S8a3qPVauHm5lbpdUpLSyvcAn/16tVYv349rly5gtLSUqjVarRr167KGP84cgTnz5+Hm6txT0ZZWRkuXLgAoHw1wsy338auXT/jxo0b0Ol0KCkpwdUqutldXFwq7R2proqWZVZUXpEDBw7g5MmTSP58rVG5Xq+HSqVC8udrDX/fn676DB07RODMmTNo3ry5oa5SqURJSclD3YNVMQExUTcSkLxb5duyK5WQODlBV1xs65CIDEQiERQONbMz6oMOt9zl5OSEJk2aAACWLF2Kno/1wJzZszFn7lwAQNM7vyROnTpV4Yz4M6dPI6xFCwBAs2bNUFBQgMzMTLN6Qf75DbkiYrHYJAHSaEy34L47oe6fhg0fjoSE6fjjjz9QWlqKq1evIu7ZZwGU/zIEgBUrPzXpLahq+Mfb2xt5+cYrhb768ku8+eab+ODDDxEVFQ0XFxd8+MEH+P33g1XGqNfrERERgS/+u87kOj4+PgDK547k3LyJhYsWIzg4GHK5HDHRUVVOYn3YIZjKlmVKpVJ4eXlV2S4AfLZqFdq1a4eIiAij8oCAAEilUqNk827vWnp6ulECcuvWLcNnQI+GOpGAQK+HOj8Pci9vyLy8UcoEhMgiZrw9E4/374eJkyahXr16iI2NhaenJxYuXGCSgGzZsgXnzp3D7DnlycrgIUOQkDAdH8yfj4WLFpm0nZ+fX+E8kNZt2uDatWs4e/Zshb0g3j4+yMrKMuphSTuWVq37CQwMRNeuXbF+3TqUlpaiZ69ehqEFPz8/1K9fH5cuXqz0F3FFwtuFY926/xqV7du3D506dcKkSZMMD6O7ePHC/dtq3x5ffpkCX19fuLq6Vlhn3969+PiTJPTv3x8AcPXqVeTc2RW6Mg87BBMZGYWtW783KkvdsQMdOnS47/yPoqIifPXVl3jvPdPdPKNjYqDVao3m5Zy9M4R3dxIlAFy4cAFlZWUIDw+v8lo2JebjF+5VZ/qE7m7Lzv1AiCyne/fuaNmypWFfBicnJyxbvgJbvvsOE8aPx59//onLly/js88+wwvPj8HgIUMMcxyCgoKwcOEiLF26BOPGjsXu3btx5coV/Prrr5g4YQLeudOrcq9u3bqha9eueGbIYKSmpuLSpUv44Ycf8OOPPxpiunnzJj6YPx8XLlxA0ief4Mcffqj2PQ0bPgIpKRvx9ddfYcSI54zOvT1zFt5/PxFLlyzB2bNncfz4caxZswaLFi6spLXyyZgnTpww2i+lcZMmOHr0KLZv346zZ8/i7RkzcOjQofvGNmLECHh7e+OpQQOxd+9eXLp0Cbt378a0qVNx7do1AECTJk3w3/9+gVOnTuHgwYMY+dyI+/Yaubi4oEmTJlUeVQ3RTJg4EVeuXMFr8fE4deoUVq9ejdWrP0P8PyYjb968GS3CQk3em5KSAq1Wi+EVJHW9evVC+/btMW7sCzh69CiOHDmCSRMnoFfv3kbJ5969e9GoUSNDklIr2WgZbm1mX3dThbvLcR08PW32WGQiezTt1XisWvUprl69CqB8dvzOn3fh6rWr6N6tK8JCm2PxooX4z3/exIYNG43mBEx66SX8uH0HMq5nYPDTT6FFWCjGvzgOrq6ueO3fpitp7vrq603o0KEjRgwfhlYtW2D6/71h2OQsLCwMH3+ShKSkTxDeri1+P/R7hatyKvPMM88gNzcXJSUlGDRokNG5cePGYeWnq/D558lo26Y1enTvhrWfJ6NhSEil7bVu3RodOnQwLD8Gyp80OmDAAAwfNgxRkZ2Rm5uLSZMqXqXwT46Ojvhl9x4ENWiAIYOfRssWYRg39gWUlpYaekRWfbYa+Xl5iGgfjtGjRmLKy68Y7athDSEhIdj6v23YvfsXtA9vh3ffmYvFS5Zi8ODBhjoFBQU4c+aMyXvXrP4MTz39NDw8PEzOicVifLfle3h5e6N7t64Y8MTjCA0Lw4YNG43qbdy4AePGvWjyfqrdRIK5GwE8ApThUyos9+n+GMRyOW79/hs0FezeSJallEuwJuExPJ/4M0pVpjtg1lUN/N2w9PX+8PYNAMQ1OwoaHtagRq9H5bZt24Y3Xv83/jz+1505KnrcLiw0rIKhB/fXX3+hd6+eOH3mbKWTgcvKynD50iXUbxACeQUTghU18GOo7Pne/StVQ+nO/1ikndqgbswBuUN1KxfKgHqQeXkzASGiGtO/f3+cP3cOGRkZCAoKsnU4duX69etI/nxtlSuRagUmmibqVAKizsmBMqAe5F7eKD5/ztbhEFEd8srUqbYOwS7FxsbevxLVSnUrAbm7LbubG0QODhAqWJpHRERkcZx7aKJO9QnpVWXQFhVBJBJB5nn/telE1iAIgtnPYCGiB2f4ebNlDsBVMCbs626qQWV4Oi4TELKN2yVq6HR6QNDbOhSiOuHuDqlS6f2fSWM1NnoYXW1Wp4ZggPJ5IE7BDSH38sZtWwdDdVJBURmOnL6Onm7OUDi51+i17z6/hWxLEPRQq9UoKyvjKhgruvsAv5vZ2XBxczf7gYVkXXUuAdHk3YKg10Pi6AiJoyN0tfnZAWSXBAH4dPMfaFzfE17uJQ/0PJQHdVmsqrFrUeUECCgrLYVCqYTIpuMCdYOLmzu8ff1tGwQTTRN1LgERdDpo8vMg8/Qq35a9pPIHNBFZS05+CSYmboWflxMk4pr7H9PvX82osWtR5bRaDX77dQ8iY7radljA3onKh11qRc+HnQ2fWEKdS0AAQJWTcycB8ULpVSYgZBtanR4Z2TU7EFjRJkxU8yQaCbRaLeRyBaT3eVYKkb2qkwnI3eW4Mk+v8qyUKxKIiMiaOARjok5+ItrCAug1aogdHOBQ23fPIyKiRx9XwZiokwkI8I9eED4dl4iIqMbV4QTk7n4gTECIiMjKuBGZiTo5BwQAVDnlPSAObm4QSaUQtFobR0RERHbLzpIHS6izn4i+rBTa4mKIxGLIPD1tHQ4REVGdUmcTEIDDMEREVEM4CdVE3U5AcpiAEBFRDeAcEBN1dg4IAKhvlW/LLnVyglihhL6s1NYhERGRPbKz3gtLsK90ykyCTgtNQT4AQO7Np+MSERHVlDqdgADcD4SIiGoAh2BM2NfdPIC/54GwB4SIiKyEk1BN1PkERFNYAL1GA7GDDFJuy05ERFQj6nwCAkGA+lb5MIycwzBERGQFIpHIIoc9YQICzgMhIiLrYgJiigkI/p4H4uDuDpFEYuNoiIiI7B8TEAC60hJoS0ogEovh4MFt2YmIyMJEFjrsCBOQO+5uyy735jAMERFZFodgTDEBuYPPhSEiIqo5dXor9n9S5+ZCEARInZ0hliugV5XZOiQiIrIT9tZ7YQnsAblD0GqhKSgAwE3JiIjIsjgEY4oJyD9wHggREVkDExBTTED+4e9t2ZmAEBERWRMTkH/QFORDr9VCLJNB6uJq63CIiMhecBmuCSYg/yQI0NzZll3GYRgiIrIQDsGYYgJyD5VhW3ZORCUiIrIWLsO9h2EeiIcnIBYDer2NIyIiokedvfVeWAJ7QO6hKymGrrQUIrG4PAkhIiJ6SByCMcUEpAKGXVE5D4SIiMgqmIBUQGXYlp3zQIiI6OGxB8QUE5AK3N2W3cHFFWKZ3NbhEBHRo86Gy3CTkpIQEhIChUKBiIgI7N27t9K6+/btQ0xMDLy8vKBUKhEaGopFixYZ1UlOTq4wOSorM+8RJpyEWgFBo4G2sBAObm6QeXmhLPO6rUMiIiIyW0pKCqZNm4akpCTExMRgxYoV6NevH06ePIkGDRqY1HdycsKUKVPQpk0bODk5Yd++fZgwYQKcnJwwfvx4Qz1XV1ecOXPG6L0KhcKs2NgDUgnOAyEiIkux1RDMwoULMXbsWIwbNw5hYWFYvHgxgoKCsGzZsgrrh4eHY9iwYWjZsiUaNmyI5557Dn369DHpNRGJRPD39zc6zMUEpBJ/zwNhAkJERA/HUgmISqVCYWGh0aFSqSq8plqtxpEjRxAbG2tUHhsbi/3791cr7qNHj2L//v3o1q2bUXlRURGCg4MRGBiIJ554AkePHjX7M2ECUglNXj4ErRYSuRxSZxdbh0NERI8wSyUgiYmJcHNzMzoSExMrvGZOTg50Oh38/PyMyv38/JCVlVVlvIGBgZDL5ejQoQMmT56McePGGc6FhoYiOTkZW7ZswYYNG6BQKBATE4Nz586Z9ZlwDkhlBD3Uebcg9/GFzMsL2qLbto6IiIjquISEBMTHxxuVyeVVL5a4d+hGEIT7Dufs3bsXRUVF+O233zB9+nQ0adIEw4YNAwBERkYiMjLSUDcmJgbt27fHRx99hKVLl1b7XpiAVEGdm1uegHh7o+TKZVuHQ0REjyoLraCVy+X3TTju8vb2hkQiMentyM7ONukVuVdISAgAoHXr1rhx4wZmzZplSEDuJRaL0bFjR7N7QDgEUwXVvduyExERPQBbTEKVyWSIiIhAamqqUXlqaiqio6Or3Y4gCJXOM7l7Pi0tDQEBAWbFxx6QKuiKi6ArK4NEoYDM3QPqO0/KJSIiehTEx8dj5MiR6NChA6KiorBy5Uqkp6dj4sSJAMqHdDIyMrB27VoAwCeffIIGDRogNDQUQPm+IB9++CFefvllQ5uzZ89GZGQkmjZtisLCQixduhRpaWn45JNPzIqNCch9qHNzoKwfCJm3NxMQIiJ6ILbaxTQuLg65ubmYM2cOMjMz0apVK2zbtg3BwcEAgMzMTKSnpxvq6/V6JCQk4NKlS5BKpWjcuDHef/99TJgwwVAnPz8f48ePR1ZWFtzc3BAeHo49e/agU6dOZsUmEgRBsMxt1h7K8CkWa0sRUA9ubdpCU1iAWweqt2yJyinlEqxJeAzPJ/6MUpXO1uEQgLxDH9s6BAKg1Wiwc8c29IztD6mDg63DqfMUNfBVPGD8Jou0k7lysEXaqQ04seE+7m5I5uDqBpFMZuNoiIiI7AMTkPvQq9XQFBYCAOSefDgdERGZjw+jM8UEpBq4LTsRET0UGz6MrrZiAlINam7LTkREZFFcBVMN6rw8CDodJAoFJE7O0BUX2TokIiJ6hNjb8IklsAekOvR6qPPyAAAyL84DISIi83AOiCkmINV0dxhGzmEYIiIyExMQU0xAqsmwHNfTE7CzfwREREQ1jQlINWlv34ZOpYJYKoWDu4etwyEiokcJV8GYYAJihr9Xw3AeCBERVR+HYEwxATGDOrf8WTBy7gdCRET0UJiAmOFuD4jU1Q2iOvr8hvHPdMGprbOQ99si/LruDcSEN67W+6LaNsLtQ0vw28bpRuVhjfyx4cNxOP2/2Sg9+jGmDO9uhaiJLCtlwzr0i30MHcNb49lnnsYfRw5XWf/wod/x7DNPo2N4a/Tv0xNff5ViUue/a5Px5ON90Kl9G8T27IYP3n/P5BHoN27cQML//Rtdozujc0RbDH16IE6e+Mui90bWwR4QU0xAzKBXqaC9fRsikQiyOrgt+5DY9vjg9cGY99l2RA57H/uPXsC3H7+EIP+q58S4OCmwau5I7Pr9rMk5R4UMl67lYMbSLci8WWCt0Iks5scftmH++4l4cfwkpHz9Ldq3j8BLE15E5vXrFda/du0qJk8aj/btI5Dy9bcY9+JEfDAvEcePHzfU+d/WLViyaAEmTpqCzd9vw6w572L7j9uwdNECQ53CggKMeW4YpFIHfLL8U3yz5X947Y3pcHFxtfo908NjAmKKCYiZVHV4W/ZXnnsMyd8eQPLmAzhz6QZe/3ATrmXl4cVnulT5vkXT45Dy42Ec/POSybkjJ9Pxn8Xf4qvtR6DWaK0VOpHFfPH5Gjw1eDCeHvIMGjVujDcS3oR/gD++TNlQYf2vUjYiICAAbyS8iUaNG+PpIc9g4KCnsHv3bkOdY2lpaBfeHv2fGID69QMRHfMv9O3/BE78o3dj9Wefws/fH3PfTUTrNm1Qv34gOkdGIahBA6vfM5E1MAEx09/7gdStHhAHqQThYUHYeeCUUfnO304hsm1Ipe9r0KABGtb3xrsrfrB2iERWp1GrcerkCURF/8uoPCo6BsfSjlb4nj+PpSEqOsaoLDI6BteuXYNGowEAhLePwKmTJ3D8zz8BANeuXsW+vbvRpWt3w3t27/oZLVu2wr9ffQXdu0Rh6OBB2PTVlxa8O7Im9oCYsulW7NeuXcOyZcuwf/9+ZGVlQSQSwc/PD9HR0Zg4cSKCgoJsGV6F1Hl5EPR6SJSOkDg6QldSYuuQaoS3hzOkUgmyb902Kr+Rext+XhV3ATcK8kFYWBi6jPwQOp2+JsIksqq8/DzodDp43fMFxMvLGzk5Nyt8T05ODrzu2cDQy9MLer0e+fn5UDo6ol//x5GXdwtjRg4HIECr1WJo3DCMfXG84T3Xrl3FlykbMHL08xg7fiL+Ov4n5iW+A5lMhgEDB1n6VsnS7Ct3sAibJSD79u1Dv379EBQUhNjYWMTGxkIQBGRnZ+Pbb7/FRx99hB9++AExMTFVtqNSqUwmaikcBIjE1rs1XUE+pB6ecPLzheb6VatdpzZRyMo7y2RSMZRyiaFc5lBe/s8yABCLRVg1ZxTOnDmDjBu3oJRL4CAVQSwyrXuXSAQ43NM+WZb2zjduejC6O8OEep3O6LPUabUARBV/voIAvV5vdE6rvduOFlqNBocP/Y5VK5Zh+n/eRKvWbXD1ajo+nP8+PD/xwovjJ5bX1evRokVLvDT5ZQBA0yZNce7sGaRsXI9+/R+30h3XEdK6uajA1myWgLz66qsYN24cFi1aVOn5adOm4dChQ1W2k5iYiNmzZxuVxcXFYdiwYRaL9V6pGSJsTQc6dW6OcaFNrXad2kQkEkGv12Pu+BhkZjYylLdqFQY3ZwnWJDxmVF8qlaJdWAPo9YG49suHhjZEIhFy9i/CgQMHkJOTY/QebzclhvVqis6NmIBYy84d22wdwiNNq9VCLBZj184duHnj70mnx9L+gFhU8ecrkYiR9sdh7Ayqbyj766+/IBaLcezoYUgkEiQlJaFly5ZwdXZE+qXzAIAe3bvjs09XoGGDQIjFYjg7O0MulxldQ11WiiuXL/Hv9SENHDjQ6tewt+ETS7BZAvLXX3/hv//9b6XnJ0yYgOXLl9+3nYSEBMTHxxuVBfdKwI7Enx86xsqInV3g3CEKf97U4/lvdwGCYLVr1Sapjdvg+FU1Xv/g78/2wIbO+OJ/RzF3mfHnLRKJ0ObLE5gxugPmfn4Yao0eLwz+F7pENMXz/1mDK9dzUVKmNnpPWqd/YcNP57A8ZTfIOtJ3fWjrEB5569ZvQEmpCj1j+xvKli1fjm49HjMqu+vEydPYs+cXo3O/HfwdgYGBeKx3X0ilDli9JhkhjZoY1dHogM2bN+Ox3v0gkUiw8+dfcONGllGdtGPH0bBhSIXXpdqFCYgpmyUgAQEB2L9/P5o3b17h+QMHDiAgIOC+7cjlcsjlcqOyMo0IgM4SYVZMlQ9HtRpimQxahTM0+fnWu1Ytsnjtz/jsnVH4/fhlHPzzEsY+HYP6fh5YnrIHpSod5rz8JOr5umHcjC8AAMfOZOD27eY4diYDpSod+t0sRKlKgz9OXTO06SCVIKyR/50/S+Hj6Yqmwf4oKlXh4tWcCuOgByeto/vXWNKoMS/gzelvoFWbNmjbNhybvkpBVmYW4oaNgNTBAUsWLUB29g28mzgfADB02HCkpGzAooUfYvCQoTh27Ci2fPcthg0rX1IrdXBA9x6P4YvP1yCsZSu0btMGV9PTsTzpY3Tr8RjkCsWd6z6P0c8NQ/KazxDbpx/+Ov4nvvnma7w9aw7/Xh8BzD9M2SwB+fe//42JEyfiyJEj6N27N/z8/CASiZCVlYXU1FSsWrUKixcvtlV496XOzYEioB5kXt51JgH5escf8HRzwn/G94O/tytOnM/EoJeTkJ6ZBwDw93ZFkL+nWW0G+LjhYEqC4fWro3vh1dG9sOfwOfR5cYlF4yeyhL79+qMgPw8rlyXh5s1sNGnaDJ8sX4l69cqHWHJu3kRWZqahfmBgED5ZthIfzEtEyoZ18PH1xev/lwAPNxdDnRcnTIJIJMInSxcjO/sGPDw80a17D0yZ+qqhTqvWbbBwycdYunghViz7BPUDA/HG//0Hjz/xZM3dPJEFiQTBduMHKSkpWLRoEY4cOQKdrrzHQiKRICIiAvHx8Rg6dOgDtasMn2LJMCukqB8It1atoc7LQ97vv1n9eo8ipbx8bsjziT+jVGXFHimqtrxDH9s6BEL5ZOCdO7ahZ2x/9l7UAooa+Cre9PUfLdLOuQ/6WqSd2sCmy3Dj4uIQFxcHjUZjmJDo7e0Nh0fgB/LufiAO7u4QSaUQtNxEi4iIKsYhGFM2TUDucnBwqNZ8j9pEX1YGbVERpM7OkHl6QZV9w9YhERERPTK4E+pDuNsLIvOqe9uyExFR9XEnVFNMQB6CKjcXACDzrlvbshMRkXlEIssc9oQJyEPQ3MqFoNdD6ugEiVJp63CIiIgeGUxAHoKg0xmW4HIYhoiIKiMWiyxy2BMmIA+J80CIiOh+OARjignIQ1IZEhDOAyEiIqquWrEM91GmLSiAXqOB2MEBUjc3aAsKbB0SERHVMva2gsUS2ANiAeo7q2HkHIYhIqIKcAjG1EMlIGVlZZaK45HGeSBERFQV7gNiyuwERK/XY+7cuahfvz6cnZ1x8eJFAMCMGTPw2WefWTzAR4HRtuwSiY2jISIiqv3MTkDeeecdJCcnY/78+ZDJZIby1q1bY9WqVRYN7lGhKy2FtqQYIrEYDp6cjEpERMbYA2LK7ARk7dq1WLlyJUaMGAHJP77tt2nTBqdPn7ZocI8Sdc7deSBMQIiIyBjngJgyOwHJyMhAkyZNTMr1ej00Go1FgnoUcR4IERFR9ZmdgLRs2RJ79+41Kf/qq68QHh5ukaAeRepbuRAEAVJnZ4gVCluHQ0REtQiHYEyZvQ/IzJkzMXLkSGRkZECv1+Obb77BmTNnsHbtWmzdutUaMT4SBK0Wmvx8yDw8IPPyRlnGNVuHREREtYSd5Q4WYXYPyIABA5CSkoJt27ZBJBLh7bffxqlTp/D999+jd+/e1ojxkXF3GIbzQIiIiKr2QDuh9unTB3369LF0LI88dW4O0KQp54EQEZERexs+sQSze0AaNWqE3Ds7f/5Tfn4+GjVqZJGgHlWaggLotVqIZTJIXV1tHQ4REdUSXAVjyuwE5PLly9DpdCblKpUKGRkZFgnqkSUIhm3Z2QtCRERUuWoPwWzZssXw5+3bt8PNzc3wWqfTYefOnWjYsKFFg3sUqXNzoPDzg9zLGyWXLto6HCIiqgU4BGOq2gnIoEGDAJR/iKNHjzY65+DggIYNG2LBggUWDe5RZNiW3cMDkEiACnqLiIiobmH+YaraCYherwcAhISE4NChQ/D25hBDRXQlJdCVlkCidITMwwPqnBxbh0RERDbGHhBTZs8BuXTpEpOP+1BxHggREdUSSUlJCAkJgUKhQERERIWbid61b98+xMTEwMvLC0qlEqGhoVi0aJFJvU2bNqFFixaQy+Vo0aIFNm/ebHZcD7QMt7i4GLt370Z6ejrUarXRuVdeeeVBmrQr6pwcOAYGQe7ljSJbB0NERDZnqw6QlJQUTJs2DUlJSYiJicGKFSvQr18/nDx5Eg0aNDCp7+TkhClTpqBNmzZwcnLCvn37MGHCBDg5OWH8+PEAgAMHDiAuLg5z587FU089hc2bN2Po0KHYt28fOnfuXO3YRIIgCObczNGjR9G/f3+UlJSguLgYnp6eyMnJgaOjI3x9fXHxou0nXirDp9j0+iIHB/j06AmRSISbv/wMvUpl03hsRSmXYE3CY3g+8WeUqjgXpjbIO/SxrUMgAFqNBjt3bEPP2P6QOjjYOpw6T/FAX8XNEzVvj0XaOfB/Xc2q37lzZ7Rv3x7Lli0zlIWFhWHQoEFITEysVhtPP/00nJyc8MUXXwAA4uLiUFhYiB9++MFQp2/fvvDw8MCGDRuqHZvZQzCvvvoqBgwYgFu3bkGpVOK3337DlStXEBERgQ8//NDc5uySoNFAW1gAgMMwRERkOSqVCoWFhUaHqpIvuWq1GkeOHEFsbKxReWxsLPbv31+t6x09ehT79+9Ht27dDGUHDhwwabNPnz7VbvMusxOQtLQ0vPbaa5BIJJBIJFCpVAgKCsL8+fPxn//8x9zm7JbqzuRTxwbBkHl7cwo0EVEdZqmNyBITE+Hm5mZ0VNaTkZOTA51OBz8/P6NyPz8/ZGVlVRlvYGAg5HI5OnTogMmTJ2PcuHGGc1lZWQ/U5r3M7nhycHAwzOb18/NDeno6wsLC4ObmhvT0dHObs1uqm9lwbtwEDm5u8IjoCL1KhbIbWSjLvA5Nfr6twyMiohpkqVUwCQkJiI+PNyqTy+VmXVsQhPvGs3fvXhQVFeG3337D9OnT0aRJEwwbNuyh2ryX2QlIeHg4Dh8+jGbNmqFHjx54++23kZOTgy+++AKtW7c2tzm7pS0oQO6BX6GsVx9y/wBI5HI4NgiGY4Ng6EpKUJaVidLM69AVcZoqERFVj1wuv2/CcZe3tzckEolJz0R2drZJD8a9QkJCAACtW7fGjRs3MGvWLEMC4u/v/0Bt3svsIZj33nsPAQEBAIC5c+fCy8sLkyZNQnZ2NlauXGluc3ZNW1iI26dPIWf3LuQdPoTSjGvQa7WQODrCqVFjeMd0gWd0DBxDGkGsUNg6XCIishJbPAtGJpMhIiICqampRuWpqamIjo6udjuCIBjNM4mKijJpc8eOHWa1CZjZAyIIAnx8fNCyZUsAgI+PD7Zt22bWBeskQYA6N6d8l9STJyD38YUiIAByH184uLjCwcUVLs2aQ513C2WZmSjLyoSg0dg6aiIishBbbUQWHx+PkSNHokOHDoiKisLKlSuRnp6OiRMnAigf0snIyMDatWsBAJ988gkaNGiA0NBQAOX7gnz44Yd4+eWXDW1OnToVXbt2xbx58zBw4EB89913+Omnn7Bv3z6zYjM7AWnatClOnDiBpk2bmnUhukOvh+pGFlQ3siCSSiH384ciIAAyTy/IPDwh8/CES2gY1Lk5KMvMhCr7BgRu505ERA8gLi4Oubm5mDNnDjIzM9GqVSts27YNwcHBAIDMzEyj+Zt6vR4JCQm4dOkSpFIpGjdujPfffx8TJkww1ImOjsbGjRvx1ltvYcaMGWjcuDFSUlLM2gMEeIB9QFq2bInPPvsMkZGRZl2oJtl6H5AHIZbLofAPgCKgHhz+8aA/QaeDKvsGSjMzoc65CZj312VT3Aek9uE+ILUD9wGpXWpiH5CuC3+1SDt74mMs0k5tYPbHPn/+fLz++utYtmwZWrVqZY2Y6iS9SoWSK5dRcuUyJI5OUASUJyNSJycoAupBEVAPerX675U0eXm2DpmIiKqJOzGYMjsBee6551BSUoK2bdtCJpNBqVQanb9165bFgqurdCXFKL5wHsUXzkPq6lqegPgHQKJQwDGoARyDGkBXWoqyrEyUZV6H9vZtW4dMRERV4MPoTJmdgCxevNgKYVBltIWFKCosRNGZ03Dw9IQyoB7kfv6QKJVwCmkEp5BG0BYVoSzzOsoyr0NXWmrrkImIiO7L7ARk9OjR1oiDqkFz6xY0t24Bp05C7u1jWEkjdXaGc9NmcG7aDOr8vPLJq1mZ0N/zoEAiIrINdoCYqoGpN2Rxej1U2Tegyr5RvpLG1698JY2XN2TuHpC5e0D450qaGzcg6LS2jpqIqM7iEIwpJiCPOEGrRdn1DJRdz4BYJoP8zkoambs75N4+kHv7QGihg+pmNsoyr0N1MwcQ9LYOm4iI6jgmIHZEr1ajNP0KStOvQKJ0/HsljbNz+RJf/wDoNRqobmShNPN6+XAOERFZHTtATDEBsVO60hIUX7yA4osXIHVxNSQjEoUCysAgKAODoCsr+3slTWGhrUMmIrJbYmYgJh44ATl//jwuXLiArl27QqlUPtCT8KhmaG8Xouh2IYrOnoGDh2d5MuLvD4lCAaeGIXBqGAJtcVH5NvCZ16ErKbF1yEREZOfMTkByc3MRFxeHn3/+GSKRCOfOnUOjRo0wbtw4uLu7Y8GCBdaIkyxEk3cLmrxbuH3qJGTePlAGBEDu6wepkzOcmzSFc5Om0BTkG55Jo//HA4iIiOjB8Pu5KbOfhvvqq69CKpUiPT0djo6OhvK4uDj8+OOPFg2OrEgQoL6ZjYI/j+Hmrp0o+PMYVDezIej1cHBzh0toGLy79YB7h45Q1A+ESMrROiKiByUSiSxy2BOzf6vs2LED27dvR2BgoFF506ZNceXKFYsFRjVH0OkMG5mJHGRQ+N95QJ6HJ+Re3pB7eUNo0fLOSppMqG5mA3qupCEiqi6xfeUOFmF2AlJcXGzU83FXTk4O5HK5RYIi2xE0apReTUfp1XSIlUoo/AOgDKgHqYsLFH7+UPj5Q6/VQnUjC2WZmVDfyn2kHpBHRES1g9lDMF27dsXatWsNr0UiEfR6PT744AP06NHDosGRbelLS1Fy6SJy9+9D7q97UXzxAnSlpRBLpVDWD4RHh47w7tYDLqFhkP7jCb5ERGSMQzCmzO4B+eCDD9C9e3ccPnwYarUab7zxBk6cOIFbt27h118t87hhqn20RUUoOncWRefOwsHd/e8H5MnlcAxuCMfghtCWlPz9TJriYluHTERUa9hZ7mARZicgLVq0wJ9//olly5ZBIpGguLgYTz/9NCZPnoyAgABrxEi1jCY/H5r8fNw+fQoyL+/yZ9L4+kHq6Ajnxk3g3LgJNIWF0N3MQh4X0RARUQUeaGmDv78/Zs+ebelY6FEjCFDn3IQ65yYgkUDu4wtlQABk3j5wcHWFg6srZv8hwLFdByDjOspuZEHQaGwdNRFRjROBXSD3MjsBCQkJwXPPPYfnnnsOzZs3t0ZM9CjS6aDKKn8Kr8jBAQo/fzjWrwepuyek7p5wdfeES1gLqHNuovTuShqdztZRExHVCK6CMWX2JNSXX34ZP/74I8LCwhAREYHFixcjMzPTGrHRI0rQaFB67SpK0g5jVnstyi6chaawECKxGHJfP7i3bQef7o/BtXUbyLy9OThKRFQHmZ2AxMfH49ChQzh9+jSeeOIJLFu2DA0aNEBsbKzR6hgiAPCQA+qrl3HrwK/I2bcXRRfOQ1tSUr6Spl59eER0hE/3x+AS1gIO7u62DpeIyCq4CsaU2QnIXc2aNcPs2bNx5swZ7N27Fzdv3sTzzz9vydjIzuiKi1B8/hxy9+7Grd8OoOTKZehUKohlMjg2CIZn5yh4d+0G56bNIHV2tnW4REQWIxJZ5rAnD7W/9u+//47169cjJSUFBQUFGDJkiKXiIjunKciHpiAft8+chszTq3wljZ8/JEpHODVqDKdGjaG5XXjnAXmZ0JeV2jpkIiKyILMTkLNnz2LdunVYv349Ll++jB49euD999/H008/DRcXF2vESPZMEKDOzYE6Nwc4eQJyH9/yZMTHFw4urnBwcYVLs+ZQ592684C8LAgata2jJiIyi9jeui8swOwEJDQ0FB06dMDkyZPx7LPPwt/f3xpxUV2k10N1IwuqG1kQSaWQ+915Jo2nF2QenpB5eMIlNAzq3JzyZ9Jk34DAlTRE9Ahg/mHK7ATk9OnTaNasmTViITIQtFqUZVxDWcY1iOVyKPwDoAgIgIObO+Q+vpD7+ELQ6aDKvoHSzMzyvUj4TBoiqqXsbQKpJZidgDD5oJqmV6lQcuUySq5chsTRCYqAACgC6kHq5FS+JXxAPeg1apRllT8gT5N3y9YhExHRfVQrAfH09MTZs2fh7e0NDw+PKjO5W7f4P3+yHl1JMYovnEfxhfOQurr+/UwahQKOQQ3gGNQAutJSlGVloizzOrS3b9s6ZCIiDsFUoFoJyKJFiwwTTBctWsSuJKoVtIWFKCosRNGZ03Dw9IQyoN6dlTRKOIU0glNII2iLiu48IC8TutISW4dMRHUUJ6GaqlYCMnr0aMOfx4wZY61YiB6Y5tYtaG7dAk6dhNzbx7CSRursDOemzeDctBnU+fkoy7wOVVYm9GqupCEisiWz54BIJBJkZmbC19fXqDw3Nxe+vr7QcVUC2ZJeD1X2Daiyb5SvpPH1K19J4+UNmbs7ZO7uEP65kubGDQg6ra2jJiI7x/4PU2YnIEIlKw1UKhVkMtlDB0RkKYJWi7LrGSi7ngGxTAa5f/nkVZm7O+TePpB7+0BooYPqZnZ5MpJzE9DrbR02EdkhTl0wVe0EZOnSpQDKP8RVq1bB+R9bZet0OuzZswehoaGWj5DIAvRqNUrTr6A0/QokSse/V9I4O5cv8fUPgF6jgepGFkozr5cP5xARkdVUOwFZtGgRgPIekOXLl0MikRjOyWQyNGzYEMuXL7d8hEQWpistQfHFCyi+eAFSF5e/V9IolVAGBkEZGARdWdnfK2kKC20dMhE94sTsADFR7QTk0qVLAIAePXrgm2++gYeHh9WCIqop2tu3UXT7DIrOnoGDh2d5z4i/PyQKBZwahsCpYQi0xcV3VtJch66EK2mIyHwcgjFl9hyQXbt2WSMOIpvT5N2CJu8Wbp86CZm3D5QBAZD7+kHq5ATnJk3h3KQpNAX5d55Jkwm9SmXrkImIHlnVSkDi4+Mxd+5cODk5IT4+vsq6CxcutEhgRDYjCFDfzIb6ZjZEEonRShoHN3c4uLnDuXko1Ldy76ykyYKg5UoaIqocO0BMVSsBOXr0KDQajeHPlWEXE9kbQaczDL+IHGRQ+N95QJ6HJ+Re3pB7eUNo0fLvlTQ3s7mShohM8PejqWolIP8cduEQDNVVgkaN0qvpKL2aDrFCaVhJ4+DiAoWfPxR+/tBrtVDdKH8mjfpWLh+QR0QAOAm1ImbPAblXYWEhfv75Z4SGhnIZLtUZ+rJSlFy6iJJLF8uX8t55KJ5EqYSyfiCU9QOhU6mgysosf0BeQb6tQyYiqlXMTkCGDh2Krl27YsqUKSgtLUWHDh1w+fJlCIKAjRs3YvDgwdaIk6jW0hYVoejcWRSdOwsHd/e/l/XK5XAMbgjH4IbQlpT8vZKmuNjWIRNRDeMQjCmxuW/Ys2cPunTpAgDYvHkzBEFAfn4+li5dinfeecfiARI9SjT5+bh96iRu/vIz8o4cRun1DOi1WkgdHeHcuAm8/9UVnlExcGwYArFCYetwiaiGiCx02BOze0AKCgrg6ekJAPjxxx8xePBgODo64vHHH8frr79u8QCJHkmCAHXOTahzbgISCeQ+vlAGBEDm7QMHV1c4uLrCpXko1LdulfeM3MiCcGeiNxFRXWB2AhIUFIQDBw7A09MTP/74IzZu3AgAyMvLg4Lf6IhM6XRQZWVClZUJkYND+YTVgHqQeXoaDpewFlDn3ETp3ZU0fKgjkV0RcwjGhNkJyLRp0zBixAg4OzsjODgY3bt3B1A+NNO6dWtLx0dkVwSNBqXXrqL02lWIFYry59AE1IODqyvkvn6Q+/qVr6TJvlG+kiY3hytpiOwA8w9TZicgL730Ejp16oSrV6+id+/eEIvLp5E0atSIc0CIzKAvK0PJ5UsouXwJEifnvx+Q5+gIZb36UNarD71afeeZNJnQ5OfZOmQiIot5oGW4HTp0QIcOHSAIAgRBgEgkwuOPP27p2IjqDF1xEYrPn0Px+XNwcHOHIiAA8rsraRoEw7FBMHSlJeXbwGdeh7aoyNYhE5EZuArGlNmrYABg7dq1aN26NZRKJZRKJdq0aYMvvvjC0rER1UmagnzcPn0KObt3Ie/wIZRmXINeq4VE6QinRo3hFdMFntH/gmNII4gVSluHS0TVIBJZ5ngQSUlJCAkJgUKhQEREBPbu3Vtp3W+++Qa9e/eGj48PXF1dERUVhe3btxvVSU5OhkgkMjnKysrMisvsHpCFCxdixowZmDJlCmJiYiAIAn799VdMnDgROTk5ePXVV81tkogqIghQ5+aUzwM5eQJyH9/ynhEfXzi4uMDBpTlcmjWHOi+vfCVNVhYEjdrWURNRLZKSkoJp06YhKSkJMTExWLFiBfr164eTJ0+iQYMGJvX37NmD3r1747333oO7uzvWrFmDAQMG4ODBgwgPDzfUc3V1xZkzZ4zea+5CFLMTkI8++gjLli3DqFGjDGUDBw5Ey5YtMWvWLCYgRNag10N1IwuqG1kQSaWQ+915Jo2nF2QeHpB5eMAlNAzq3JzyZ9Jk34DAlTREtYatVsEsXLgQY8eOxbhx4wAAixcvxvbt27Fs2TIkJiaa1F+8eLHR6/feew/fffcdvv/+e6MERCQSwd/f/6FiMzsByczMRHR0tEl5dHQ0MjMzHyoYIro/QatFWcY1lGVcg1guv7OSJgAObu6Q+/hC7uMLQaczrKRR5dzkShoiG7NU/qFSqaBSqYzK5HI55HK5SV21Wo0jR45g+vTpRuWxsbHYv39/ta6n1+tx+/Ztw/5fdxUVFSE4OBg6nQ7t2rXD3LlzjRKU6jB7DkiTJk3w5ZdfmpSnpKSgadOm5jZHRA9Br1Kh5Mpl3PrtAHL27kbR+XPQFhdDJJFAEVAP7u0j4NPjMbi0aAkHD0/o9UxEiGyhojkTD3IkJibCzc3N6KioJwMAcnJyoNPp4OfnZ1Tu5+eHrKysasW9YMECFBcXY+jQoYay0NBQJCcnY8uWLdiwYQMUCgViYmJw7tw5sz4Ts3tAZs+ejbi4OOzZswcxMTEQiUTYt28fdu7cWWFiQkQ1Q1dSguIL51F84Tykrq5/P5NGoYBjUAM4BjXAkGX7sOmlf3FGPtEjKiEhAfHx8UZlFfV+/NO9P+93V6/ez4YNGzBr1ix899138PX1NZRHRkYiMjLS8DomJgbt27fHRx99hKVLl1bnNgA8QAIyePBgHDx4EIsWLcK3334LQRDQokUL/P7772Z3v1jLvs3v2ToEAqDXaXH9r73Yvm4mxJKHfvAyPQCdXsDJzALsPZeNAxdz8OfFXDSc9LWtw6rzFFJgaT8ZWr76Lcq0to6Gbqx6xurXeKAlpxWobLilIt7e3pBIJCa9HdnZ2Sa9IvdKSUnB2LFj8dVXX6FXr15V1hWLxejYsaP1e0AAICIiAv/9738f5K1EVIMkYhFa13dH6/rueLFLEwx8P9XWIRHVSbbodZTJZIiIiEBqaiqeeuopQ3lqaioGDhxY6fs2bNiAF154ARs2bKjWHl+CICAtLc3s3dAfKAHR6XTYvHkzTp06BZFIhLCwMAwcOBBSKb/lEtVWDhIxwCkgRHVKfHw8Ro4ciQ4dOiAqKgorV65Eeno6Jk6cCKB8SCcjIwNr164FUJ58jBo1CkuWLEFkZKSh90SpVMLNzQ1A+VSMyMhING3aFIWFhVi6dCnS0tLwySefmBWb2RnDX3/9hYEDByIrKwvNmzcHAJw9exY+Pj7YsmULnwdDRER0D7GNpl3FxcUhNzcXc+bMQWZmJlq1aoVt27YhODgYQPnK1vT0dEP9FStWQKvVYvLkyZg8ebKhfPTo0UhOTgYA5OfnY/z48cjKyoKbmxvCw8OxZ88edOrUyazYRIJg3vq8yMhI+Pr64vPPP4eHhweA8ifhjhkzBtnZ2Thw4IBZAVjDkcuFtg6B8PcckHqtunAOSC3R/53t969EVnd3DsgrP6g5B6QWqIk5IPFbTluknYVPhlqkndrA7N8Kx44dw+HDhw3JBwB4eHjg3XffRceOHS0aHBEREdknsyfmNm/eHDdu3DApz87ORpMmTSwSFBERkT2x1D4g9sTsHpD33nsPr7zyCmbNmmVYB/zbb79hzpw5mDdvHgoL/x7+cHV1tVykREREjyhbzQGpzcxOQJ544gkAwNChQw3Z2N1pJAMGDDC8FolE0PFZFERERFQBsxOQXbt2WSMOIiIiu2VnoycWYXYC0q1bN2vEQUREZLds9TTc2oxrI4mIiKzMUlux2xN+JkRERFTj2ANCRERkZRyBMcUEhIiIyMo4B8TUAw3BaLVa/PTTT1ixYgVu374NALh+/TqKioosGhwRERHZJ7N7QK5cuYK+ffsiPT0dKpUKvXv3houLC+bPn4+ysjIsX77cGnESERE9stgBYsrsHpCpU6eiQ4cOyMvLg1KpNJQ/9dRT2Llzp0WDIyIisgdikWUOe2J2D8i+ffvw66+/QiaTGZUHBwcjIyPDYoERERGR/TI7AdHr9RVusX7t2jW4uLhYJCgiIiJ7wkmopswegunduzcWL15seC0SiVBUVISZM2eif//+loyNiIjILohEljnsidk9IIsWLUKPHj3QokULlJWVYfjw4Th37hy8vb2xYcMGa8RIREREdsbsBKRevXpIS0vDhg0b8Mcff0Cv12Ps2LEYMWKE0aRUIiIiKmdvE0gt4YE2IlMqlXjhhRfwwgsvWDoeIiIiuyMCM5B7mZ2ArF27tsrzo0aNeuBgiIiI7BF7QEyZnYBMnTrV6LVGo0FJSQlkMhkcHR2ZgBAREdF9mZ2A5OXlmZSdO3cOkyZNwuuvv26RoIiIiOwJe0BMPdCzYO7VtGlTvP/++ya9I0RERFS+ZYUlDntikQQEACQSCa5fv26p5oiIiMiOmT0Es2XLFqPXgiAgMzMTH3/8MWJiYiwWGBERkb3gEIwpsxOQQYMGGb0WiUTw8fHBY489hgULFlgqLiIiIrthZ6MnFvFAz4IhIiIiehhmzQHRaDRo1KgRTp48aa14iIiI7I5YJLLIYU/M6gFxcHCASqWyu5m4RERE1sQ5IKbMXgXz8ssvY968edBqtdaIh4iIiOqAaveApKenIzAwEAcPHsTOnTuxY8cOtG7dGk5OTkb1vvnmG4sHSURE9CjjwIGpaicgISEhyMzMhLu7OwYPHmzNmIiIiOyKmA+jM1HtBEQQBADAmjVrrBYMERGRPWIPiCmL7YRKREREVF1mrYJZtWoVnJ2dq6zzyiuvPFRARERE9oarYEyZlYAsX74cEomk0vMikYgJCBER0T3sbQ8PSzArATl8+DB8fX2tFQsRERHVEdVOQLj5GBER0YPhr1BTZq+CISIiIvNwCMZUtVfBzJw5874TUImIiIiqo9o9IDNnzrRmHERERHaLHSCmzJqESkRERObjplum+JkQERFRjWMPCBERkZVxJakpJiBERERWxvTD1AMNwcyZMwdJSUlGZUlJSZgzZ45FgiIiIrInYpHIIoc9eaAEZM2aNdi8ebNR2aZNm5CcnGyJmIiIiMhCkpKSEBISAoVCgYiICOzdu7fSut988w169+4NHx8fuLq6IioqCtu3bzept2nTJrRo0QJyuRwtWrQwyQmq44ESkEuXLiE1NdWobOfOnbh48eKDNEdERGTXRBY6zJWSkoJp06bhzTffxNGjR9GlSxf069cP6enpFdbfs2cPevfujW3btuHIkSPo0aMHBgwYgKNHjxrqHDhwAHFxcRg5ciSOHTuGkSNHYujQoTh48KBZsYkEC25xeujQIXTs2NFSzT2wI5cLbR0CAdDrtLj+117Ua9UFYgmnG9UG/d8x/SZDNU8hBZb2k+GVH9Qo09o6Grqx6hmrX2P9H9cs0s7w9oFm1e/cuTPat2+PZcuWGcrCwsIwaNAgJCYmVquNli1bIi4uDm+//TYAIC4uDoWFhfjhhx8Mdfr27QsPDw9s2LCh2rGZ3QNSVFSE0tJSo7K0tDQMGDAAkZGR5jZHRERE1aRSqVBYWGh0qFSqCuuq1WocOXIEsbGxRuWxsbHYv39/ta6n1+tx+/ZteHp6GsoOHDhg0mafPn2q3eZd1U5Arl27hpiYGLi5ucHNzQ3x8fEoKSnBqFGj0LFjR8jlcuzbt8+sixMREdUFIpHIIkdiYqLh9/Ddo7KejJycHOh0Ovj5+RmV+/n5ISsrq1pxL1iwAMXFxRg6dKihLCsr66HavKva/eLTp09HUVERlixZgk2bNmHJkiXYvXs32rZti7NnzyIkJMSsCxMREdUVltr1MyEhAfHx8UZlcrm8yvfcuweJIAjV2pdkw4YNmDVrFr777jv4+vpapM1/qnYCsmvXLnz55ZeIiYnBkCFDUK9ePTzzzDOYPn26WRckIiKiByOXy++bcNzl7e0NiURi0jORnZ1t0oNxr5SUFIwdOxZfffUVevXqZXTO39//gdq8V7WTsqysLDRu3NhwcaVSiYEDB5p1MSIiorrIUkMw5pDJZIiIiDBZtZqamoro6OhK37dhwwaMGTMG69evx+OPP25yPioqyqTNHTt2VNlmRcxamiCRSAx/FovFUCgUZl2MiIioLrLVFmLx8fEYOXIkOnTogKioKKxcuRLp6emYOHEigPIhnYyMDKxduxZAefIxatQoLFmyBJGRkYaeDqVSCTc3NwDA1KlT0bVrV8ybNw8DBw7Ed999h59++snseaDVTkAEQUDPnj0hlZa/pbS0FAMGDIBMJjOq98cff5gVABEREVlHXFwccnNzMWfOHGRmZqJVq1bYtm0bgoODAQCZmZlGe4KsWLECWq0WkydPxuTJkw3lo0ePNmw2Gh0djY0bN+Ktt97CjBkz0LhxY6SkpKBz585mxVbtBGTmzJlGrzn8QkREVD22fBjdSy+9hJdeeqnCc/fuYP7LL79Uq80hQ4ZgyJAhDxXXAycgREREVD2WWgVjT8yaA3Lw4EFs2bIFGo0GvXr1MtmIhIiIiEzZsgektqp2ArJ582Y888wzUCgUkEqlWLBgARYsWIBp06ZZMTwiIiKyR9XuFXrvvfcwZswY5OfnIz8/H7Nnz8Y777xjzdiIiIjsgq0eRlebVTsBOXPmDN544w3DKpjXX38d+fn5yMnJsVpwRERE9kAkssxhT6qdgBQVFcHd3d3wWi6XQ6lUorCQT54lIiIi85g1CXX79u2GjUiA8qfk7dy5E3/99Zeh7Mknn7RcdERERHZAbHcDKA/PrARk9OjRJmUTJkww/FkkEkGn0z18VERERHbE3oZPLKHaCYher7dmHERERFSHVHsOyAsvvIDbt29bMxYiIiK7JLLQf/ak2gnI559/jtLSUmvGQkREZJe4CsZUtRMQQRCsGQcRERHVIWZNQuVWskRERObjKhhTZiUgzZo1u28ScuvWrYcKiIiIyN7w+7spsxKQ2bNnG+0DQkRERPfHBMSUWQnIs88+C19fX2vFQkRERHVEtRMQzv8gIiJ6MPa2hNYSqp2AcBUMERHRgxEz/zDBnVCJiIioxpk1B4SIiIjMxyEYU0xAiIiIrIzTKE1VeydUIiIiIkthDwgREZGVcQjGFBMQIiIiK+MqGFMcgiEiIqIaxx4QMkvq919h61f/Rf6tHNQPboRRE+MR2jq8wrp5t3Kwfv16ZN38GDeuX0WfgXEYNek1k3rFRbfxZXISDv26C8W3b8PHvx5GjJ+G8E4x1r4dIosY070xJvdpDl93Bc5cL8SMjWk4eC6nwrqdmnhh5jNt0CbIFWf6SXA1txhf7LmIFannDHXiooOx9IVOJu9tMHETVFpuifAo4hCMKSYgVG0HftmBtcsX4oUp/4dmLdti5/++wby3puKDT7+Et6+/SX2tRgMnJycM7DkAP36bUmGbWo0GiQmT4eruialvzYOnty9yb96AUulo7dshsoiBHQMx99l2mL7uD/x+PgejujbChqld0OXtH5Fxq9SkfolKh893ncdTjUowY2cp2oZ448OREShRafHFnkuGeoUlGkS/9YPRe5l8PLq4CsYUh2Co2rZ9sx7d+wxEj36DUL9BCEZNeg1ePn74aevXFdb38QvAwIED0aVnfzg6OVdY55ftW1B0uxDxMz9E85Zt4eMXgNBW7RDcuJk1b4XIYib2bob1+y5h3d5LOJd5GzNSjiEjrwRjujeusP5fV/Ox5fBV3L59G9dyS7Dpt3TsOpGFzk19jOoJEHCzUGV00KNLZKHDnrAHhKpFq9Hg0rnTeDJutFF564jOOHvyzwdu98hve9A0rDXWfDwPRw7sgaubO6J79MWTQ0dBLJE8bNhEVuUgEaFNsAeW/nDaqHz3iRvo0Ni7Wm20CnJHx8beeP/bv4zKneRSHJ7XHxKRCCeu5uP9b0/gr6v5lgqdyOYe+QREpVJBpTL+ZlBWUgyZXG6jiOxTQV4O9HodXFzdoNdpDeWubu7Iv5VrVHbX3TK9TgtBECAIepN62ZnXcDItC9E9YvH67AXIun4Vnyd9CJ1WjaeGj7XuTdVBikf+J7528XWTQyoRo7BYZfTZ5hWVwc9NUennrZACsbGxONdfBolEjMVbT2DTgUuG+uk3b+Pfnx/C6YwCOCsd8MJjTbE1oQf6vpOKy9lF1r8xsjgxx2BM1Or/HV29ehUzZ87E6tWrK62TmJiI2bNnG5XFxcVh2LBh1g6vTikoKAAA5F76E9f1BYbywqzL0KlLcf2vvZW+N+vUAaiLC1Cce92knqasBE5Ojuj32L8gVt1EsJcC3bt1xY7vUtC5DYdhLG1pP5mtQ7ArCkX55zktUoq85n9/ts2aSeHrVPXnvW/fPkgkEnh6euLlvi3QrV4ZMjIy7pwtAlCEHl4AUArN1SNQN+6ONWOa4/jx41a7H7Ieph+manUCcuvWLXz++edVJiAJCQmIj483Kjt+jT0gluar0UAsfh9Sj0DUa9XFUC7sOwwvv/pGZXfpdVpknToA/7AoyJw2wMmrnkk9b//1kEilCGzTzVAWVirF1q1b4ds8ElIHB+vdVB005IOdtg7BrjhINDjdS4//npRge5raUD7TRYoWkjK88oO6wvcppMD83sAbqWqUaXPxcj8pnurcHINXXaqwPgC875WLAHdlpW3Sgxs40NYR1E02TUC2bNlS5fmLFy/etw25XA75PcmGggsoLE4mkSKkaShOpB1G5y69DOUn0g4hIqorxJLK/ymJJVKIRCKIRGKTes1atsP+X7YDIjHE4vI50VmZ1+Du6Q2ZQmmdm6nDykxHyughlGkF/HklD1HN/fDd4euG8phQP2xPy7jv512mLT+0AuAgFVdZP7S+O05nFPDv8FHFLhATNk1ABg0aBJFIBEEQKq0j4rhZrdH/6eFI+mAmGjVrgaZhrfHzts3Iyc5Cz8cHAwA2rv4Yt3Ju4qU3/h4Su379OjTKsygrLUVhQR4uXzgDqdQBgcGNAAC9nxiMHVu+xNplC9Bn4FBkZVzFdxuT0XdgnE3ukchcy1PP4uOxnXHsch4OX8zFyK6NEOjpiM9/Kf8C9ebTreDvrsTLqw8BAJ7v0RjZ+SVwcipDQ18Z2oV446XY5vjs57/3AXltQAscuZiLSzeK4KyU4sWeTdEqyB0J6/+wyT3Sw+M+IKZsmoAEBATgk08+waBBgyo8n5aWhoiIiJoNiioV1T0WRbcL8M26Vci/lYPA4MZ4453F8PELAADk38pB7s0so/csXrzY8OdL505h/67t8PYLwNK15b1fXr7+mP7eR/jvikWYPnE4PLx90HfQs3hy6Kgauy+ih/HdoWvwcJIjfkAL+LkpcPp6IYYv2Ytrt0oAAL5uStT3+rtbViwS4f8GtUZjPyd0ihFwObsI73zzJ9bu/rvH183RAR+OioCvqwK3SzU4np6PQfN34eilvBq/PyJrEQlVdT9Y2ZNPPol27dphzpw5FZ4/duwYwsPDodebt/nOkcuFlgiPHpJep8X1v/aiXqsuVQ7RUM3p/852W4dAKJ8DsrSfDK/8oOaQSi1wY9UzVr/G7xcL7l+pGjo1crNIO7WBTX8rvP766yguLq70fJMmTbBr164ajIiIiMjyOABjyqYJSJcupisn/snJyQndunWrsg4RERE9etgvTkREZG3sAjHBBISIiMjKuArGFBMQIiIiK+OOEqb4NFwiIiKqcewBISIisjJ2gJhiAkJERGRtzEBMcAiGiIiIahx7QIiIiKyMq2BMsQeEiIjIykQiyxwPIikpCSEhIVAoFIiIiMDevXsrrZuZmYnhw4ejefPmEIvFmDZtmkmd5OTkO084Nz7KysrMiosJCBERkZ1KSUnBtGnT8Oabb+Lo0aPo0qUL+vXrh/T09Arrq1Qq+Pj44M0330Tbtm0rbdfV1RWZmZlGh0KhMCs2JiBERERWJrLQYa6FCxdi7NixGDduHMLCwrB48WIEBQVh2bJlFdZv2LAhlixZglGjRsHNrfIH34lEIvj7+xsd5mICQkREZG0WykBUKhUKCwuNDpVKVeEl1Wo1jhw5gtjYWKPy2NhY7N+//6Fup6ioCMHBwQgMDMQTTzyBo0ePmt0GExAiIqJHRGJiItzc3IyOxMTECuvm5ORAp9PBz8/PqNzPzw9ZWVkPHENoaCiSk5OxZcsWbNiwAQqFAjExMTh37pxZ7XAVDBERkZVZahVMQkIC4uPjjcrkcnnV175n9qogCCZl5oiMjERkZKThdUxMDNq3b4+PPvoIS5curXY7TECIiIiszFLPgpHL5fdNOO7y9vaGRCIx6e3Izs426RV5GGKxGB07djS7B4RDMERERFZmi0moMpkMERERSE1NNSpPTU1FdHT0A9/LvQRBQFpaGgICAsx6H3tAiIiI7FR8fDxGjhyJDh06ICoqCitXrkR6ejomTpwIoHxIJyMjA2vXrjW8Jy0tDUD5RNObN28iLS0NMpkMLVq0AADMnj0bkZGRaNq0KQoLC7F06VKkpaXhk08+MSs2JiBERETWZqONUOPi4pCbm4s5c+YgMzMTrVq1wrZt2xAcHAygfOOxe/cECQ8PN/z5yJEjWL9+PYKDg3H58mUAQH5+PsaPH4+srCy4ubkhPDwce/bsQadOncyKTSQIgvBwt1f7HLlcaOsQCIBep8X1v/aiXqsuEEuY69YG/d/ZbusQCIBCCiztJ8MrP6hRprV1NHRj1TNWv8aJjGKLtNOyvpNF2qkNOAeEiIiIahy/lhIREVmZpVbB2BMmIERERFbG/MMUh2CIiIioxrEHhIiIyNrYBWKCCQgREZGVWWordnvCIRgiIiKqcewBISIisjKugjHFBISIiMjKmH+YYgJCRERkbcxATHAOCBEREdU49oAQERFZGVfBmGICQkREZGWchGqKQzBERERU49gDQkREZGXsADHFBISIiMjamIGY4BAMERER1Tj2gBAREVkZV8GYYgJCRERkZVwFY4pDMERERFTj2ANCRERkZewAMcUEhIiIyNqYgZhgAkJERGRlnIRqinNAiIiIqMaxB4SIiMjKuArGFBMQIiIiK2P+YYpDMERERFTj2ANCRERkZRyCMcUEhIiIyOqYgdyLQzBERERU49gDQkREZGUcgjHFBISIiMjKmH+Y4hAMERER1Tj2gBAREVkZh2BMMQEhIiKyMj4LxhQTECIiImtj/mGCc0CIiIioxrEHhIiIyMrYAWKKCQgREZGVcRKqKQ7BEBERUY1jDwgREZGVcRWMKSYgRERE1sb8wwSHYIiIiKjGsQeEiIjIytgBYooJCBERkZVxFYwpDsEQERFRjWMCQkREZGUiC/33IJKSkhASEgKFQoGIiAjs3bu30rqZmZkYPnw4mjdvDrFYjGnTplVYb9OmTWjRogXkcjlatGiBzZs3mx0XExAiIiIrE4ksc5grJSUF06ZNw5tvvomjR4+iS5cu6NevH9LT0yusr1Kp4OPjgzfffBNt27atsM6BAwcQFxeHkSNH4tixYxg5ciSGDh2KgwcPmhWbSBAEwew7quWOXC60dQgEQK/T4vpfe1GvVReIJZxuVBv0f2e7rUMgAAopsLSfDK/8oEaZ1tbR0I1Vz1j9GnklOou04+EoMat+586d0b59eyxbtsxQFhYWhkGDBiExMbHK93bv3h3t2rXD4sWLjcrj4uJQWFiIH374wVDWt29feHh4YMOGDdWOjT0gREREjwiVSoXCwkKjQ6VSVVhXrVbjyJEjiI2NNSqPjY3F/v37HziGAwcOmLTZp08fs9tkAkJERGRllhqCSUxMhJubm9FRWU9GTk4OdDod/Pz8jMr9/PyQlZX1wPeSlZVlkTbZL05ERGRlltqKPSEhAfHx8UZlcrm86mvfM3lEEASTMnNZok0mIERERI8IuVx+34TjLm9vb0gkEpOeiezsbJMeDHP4+/tbpE0OwRAREVmZLVbByGQyREREIDU11ag8NTUV0dHRD3wvUVFRJm3u2LHD7DbZA0JERGRlttoINT4+HiNHjkSHDh0QFRWFlStXIj09HRMnTgRQPqSTkZGBtWvXGt6TlpYGACgqKsLNmzeRlpYGmUyGFi1aAACmTp2Krl27Yt68eRg4cCC+++47/PTTT9i3b59ZsTEBISIislNxcXHIzc3FnDlzkJmZiVatWmHbtm0IDg4GUL7x2L17goSHhxv+fOTIEaxfvx7BwcG4fPkyACA6OhobN27EW2+9hRkzZqBx48ZISUlB586dzYqN+4CQ1XAfkNqH+4DUDtwHpHapiX1Abqv0FmnHRW4/Myf4W4GIiMjKLLUKxp7YTypFREREjwz2gBAREVnZQ267YZeYgBAREVkZ8w9TTECIiIisjRmICc4BISIiohrHHhAiIiIr4yoYU0xAiIiIrIyTUE1xCIaIiIhqnF32gEQ0dLV1CARApVLhf18cRd++fav99EayrprY8ZHuT6VSITExEWeXJPBno45Q2OVv24djl1uxU+1QWFgINzc3FBQUwNWVSSHRXfzZIOIQDBEREdkAExAiIiKqcUxAiIiIqMYxASGrkcvlmDlzJifZEd2DPxtEnIRKRERENsAeECIiIqpxTECIiIioxjEBISIiohrHBISIiIhqHBMQspqkpCSEhIRAoVAgIiICe/futXVIRDa1Z88eDBgwAPXq1YNIJMK3335r65CIbIYJCFlFSkoKpk2bhjfffBNHjx5Fly5d0K9fP6Snp9s6NCKbKS4uRtu2bfHxxx/bOhQim+MyXLKKzp07o3379li2bJmhLCwsDIMGDUJiYqINIyOqHUQiETZv3oxBgwbZOhQim2APCFmcWq3GkSNHEBsba1QeGxuL/fv32ygqIiKqTZiAkMXl5ORAp9PBz8/PqNzPzw9ZWVk2ioqIiGoTJiBkNSKRyOi1IAgmZUREVDcxASGL8/b2hkQiMentyM7ONukVISKiuokJCFmcTCZDREQEUlNTjcpTU1MRHR1to6iIiKg2kdo6ALJP8fHxGDlyJDp06ICoqCisXLkS6enpmDhxoq1DI7KZoqIinD9/3vD60qVLSEtLg6enJxo0aGDDyIhqHpfhktUkJSVh/vz5yMzMRKtWrbBo0SJ07drV1mER2cwvv/yCHj16mJSPHj0aycnJNR8QkQ0xASEiIqIaxzkgREREVOOYgBAREVGNYwJCRERENY4JCBEREdU4JiBERERU45iAEBERUY1jAkJEREQ1jgkIERER1TgmIEQ2MGvWLLRr187WYTywy5cvQyQSIS0trcp63bt3x7Rp02okJiJ6tDABoUfKmDFjIBKJTI67z9f453kHBwc0atQI//73v1FcXAzg71+cdw83NzdERkbi+++/NzuW7t27VxiLVqu16D3XRkFBQYYt9oHyLcZFIhHy8/ON6n3zzTeYO3euDSK8v+TkZLi7u9s6DKI6iwkIPXL69u2LzMxMoyMkJMTk/MWLF/HOO+8gKSkJ//73v43a+Omnn5CZmYmDBw+iU6dOGDx4MP766y+zY3nxxRdNYpFK7f8ZjxKJBP7+/ve9V09PT7i4uNRQVOXUanWNXo+IHgwTEHrkyOVy+Pv7Gx0SicTkfFBQEIYPH44RI0bg22+/NWrDy8sL/v7+CA0NxbvvvguNRoNdu3aZHYujo6NJLADwf//3f2jWrBkcHR3RqFEjzJgxAxqNptJ2fvnlF3Tq1AlOTk5wd3dHTEwMrly5Yjj//fffIyIiAgqFAo0aNcLs2bOr7GkZM2YMBg0ahNmzZ8PX1xeurq6YMGGC0S9nlUqFV155Bb6+vlAoFPjXv/6FQ4cOGc7n5eVhxIgR8PHxgVKpRNOmTbFmzRoAxkMwly9fNjxgzcPDAyKRCGPGjAFgPASTkJCAyMhIk1jbtGmDmTNnGl6vWbMGYWFhUCgUCA0NRVJSUqX3efcaU6ZMQXx8PLy9vdG7d28AwMKFC9G6dWs4OTkhKCgIL730EoqKigyf9/PPP4+CggJDz9WsWbMAlCcwb7zxBurXrw8nJyd07twZv/zyS5UxEJH57P+rGtV5SqWy0l/+Go0Gn376KQDAwcHBUD5r1iwkJyfj8uXLD3RNFxcXJCcno169ejh+/DhefPFFuLi44I033jCpq9VqMWjQILz44ovYsGED1Go1fv/9d4hEIgDA9u3b8dxzz2Hp0qXo0qULLly4gPHjxwOA0S/ue+3cuRMKhQK7du3C5cuX8fzzz8Pb2xvvvvsuAOCNN97Apk2b8PnnnyM4OBjz589Hnz59cP78eXh6emLGjBk4efIkfvjhB3h7e+P8+fMoLS01uU5QUBA2bdqEwYMH48yZM3B1dYVSqTSpN2LECLz//vu4cOECGjduDAA4ceIEjh8/jq+//hoA8Omnn2LmzJn4+OOPER4ejqNHj+LFF1+Ek5MTRo8eXem9fv7555g0aRJ+/fVX3H2+plgsxtKlS9GwYUNcunQJL730Et544w0kJSUhOjoaixcvxttvv40zZ84AAJydnQEAzz//PC5fvoyNGzeiXr162Lx5M/r27Yvjx4+jadOmlcZARGYSiB4ho0ePFiQSieDk5GQ4hgwZYnR+4MCBhtcHDx4UvLy8hKFDhwqCIAiXLl0SAAhKpVJwcnISxGKxAEBo2LChkJuba3jfRx99JDz22GNVxtKtWzfBwcHBKJb4+PgK686fP1+IiIgwvJ45c6bQtm1bQRAEITc3VwAg/PLLLxW+t0uXLsJ7771nVPbFF18IAQEBlcY2evRowdPTUyguLjaULVu2THB2dhZ0Op1QVFQkODg4COvWrTOcV6vVQr169YT58+cLgiAIAwYMEJ5//vkK27/7OR49elQQBEHYtWuXAEDIy8szqtetWzdh6tSphtdt2rQR5syZY3idkJAgdOzY0fA6KChIWL9+vVEbc+fOFaKioiq9127dugnt2rWr9PxdX375peDl5WV4vWbNGsHNzc2ozvnz5wWRSCRkZGQYlffs2VNISEi47zWIqPrYA0KPnB49emDZsmWG105OTkbnt27dCmdnZ2i1Wmg0GgwcOBAfffSRUZ2UlBSEhobi7NmzmDZtGpYvXw5PT0/D+SlTpmDKlCn3jWXEiBF48803Da/vTmr8+uuvsXjxYpw/fx5FRUXQarVwdXWtsA1PT0+MGTMGffr0Qe/evdGrVy8MHToUAQEBAIAjR47g0KFDhp4LANDpdCgrK0NJSQkcHR0rbLdt27ZG56KiolBUVISrV6+ioKAAGo0GMTExhvMODg7o1KkTTp06BQCYNGkSBg8ejD/++AOxsbEYNGgQoqOj7/uZVGXEiBFYvXo1ZsyYAUEQsGHDBsMQzc2bN3H16lWMHTsWL774ouE9Wq0Wbm5uVbbboUMHk7Jdu3bhvffew8mTJ1FYWAitVouysjIUFxeb/Ju5648//oAgCGjWrJlRuUqlgpeXl5l3S0RVYQJCjxwnJyc0adKk0vN3ExQHBwfUq1fPaGjlrqCgIDRt2hRNmzaFs7MzBg8ejJMnT8LX19esWNzc3Exi+e233/Dss89i9uzZ6NOnD9zc3LBx40YsWLCg0nbWrFmDV155BT/++CNSUlLw1ltvITU1FZGRkdDr9Zg9ezaefvppk/cpFAqz4gUAkUhkGKa4O8xzlyAIhrJ+/frhypUr+N///oeffvoJPXv2xOTJk/Hhhx+afc27hg8fjunTp+OPP/5AaWkprl69imeffRYAoNfrAZQPw3Tu3Nnoff+c41ORexOKK1euoH///pg4cSLmzp0LT09P7Nu3D2PHjq1yLo5er4dEIsGRI0dMrnl3iIaILIMJCNmd+yUo9+rWrRtatWqFd999F0uWLHno6//6668IDg426hn554TSyoSHhyM8PBwJCQmIiorC+vXrERkZifbt2+PMmTNm3RMAHDt2DKWlpYb5GL/99hucnZ0RGBgILy8vyGQy7Nu3D8OHDwdQPh/m8OHDRvt2+Pj4YMyYMRgzZgy6dOmC119/vcIERCaTASjvmalKYGAgunbtinXr1qG0tBS9evWCn58fAMDPzw/169fHxYsXMWLECLPu9V6HDx+GVqvFggULIBaXz7X/8ssvTWK+N97w8HDodDpkZ2ejS5cuDxUDEVWNCQgRgNdeew3PPPOMYfXDxx9/jM2bN2Pnzp1mt9WkSROkp6dj48aN6NixI/73v/9h8+bNlda/dOkSVq5ciSeffBL16tXDmTNncPbsWYwaNQoA8Pbbb+OJJ55AUFAQnnnmGYjFYvz55584fvw43nnnnUrbVavVGDt2LN566y1cuXIFM2fOxJQpUyAWi+Hk5IRJkybh9ddfh6enJxo0aID58+ejpKQEY8eONVw3IiICLVu2hEqlwtatWxEWFlbhtYKDgyESibB161b0798fSqWy0h6DESNGYNasWVCr1Vi0aJHRuVmzZuGVV16Bq6sr+vXrB5VKhcOHDyMvLw/x8fFVfu7/1LhxY2i1Wnz00UcYMGAAfv31VyxfvtyoTsOGDVFUVISdO3cahquaNWuGESNGYNSoUViwYAHCw8ORk5ODn3/+Ga1bt0b//v2rHQMRVY3LcIkAPPHEE2jYsKFhnkVOTg4uXLjwQG0NHDgQr776KqZMmYJ27dph//79mDFjRqX1HR0dcfr0aQwePBjNmjXD+PHjMWXKFEyYMAEA0KdPH2zduhWpqano2LEjIiMjsXDhQgQHB1cZR8+ePdG0aVN07doVQ4cOxYABAwxLTQHg/fffx+DBgzFy5Ei0b98e58+fx/bt2+Hh4QGgvIcgISEBbdq0QdeuXSGRSLBx48YKr1W/fn3Mnj0b06dPh5+fX5XzZ5555hnk5uaipKQEgwYNMjo3btw4rFq1CsnJyWjdujW6deuG5ORko31eqqNdu3ZYuHAh5s2bh1atWmHdunVITEw0qhMdHY2JEyciLi4OPj4+mD9/PoDy4bBRo0bhtddeQ/PmzfHkk0/i4MGDCAoKMisGIqqaSLg7GExEdmPMmDHIz8832f+EiKi2YA8IERER1TgmIERERFTjOARDRERENY49IERERFTjmIAQERFRjWMCQkRERDWOCQgRERHVOCYgREREVOOYgBAREVGNYwJCRERENY4JCBEREdW4/wcc/mzuHbpmCwAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "XGBoostClassifier(X_train, X_test, y_train, y_test, study)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SsPPYpZ0GCV",
        "outputId": "e8fcba4c-2df6-4b2e-fb24-0dfc067c3806"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:926: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  self.best_estimator_.fit(X, y, **fit_params)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "GridSearchCV(estimator=RandomForestClassifier(),\n",
              "             param_grid={'max_depth': (10, 20, 30, 40, 50, None),\n",
              "                         'max_features': ('sqrt', 'log2', 'auto', None),\n",
              "                         'n_estimators': [10, 20, 30, 50, 100, 300]},\n",
              "             scoring='accuracy')"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_resampled = pd.read_csv(\"X_resampled.csv\", index_col=0)\n",
        "y_resampled = pd.read_csv(\"y_resampled.csv\", index_col=0)\n",
        "\n",
        "X_resampled = (X_resampled.mean()-X_resampled)/X_resampled.std()\n",
        "\n",
        "model = RandomForestClassifier()\n",
        "parameters = {  \n",
        "    'n_estimators': [10, 20, 30, 50, 100, 300],     # 用意する決定木モデルの数\n",
        "    'max_features': ('sqrt', 'log2','auto', None),  # ランダムに指定する特徴量の数\n",
        "    'max_depth':    (10, 20, 30, 40, 50, None),     # 決定木のノード深さの制限値\n",
        "}\n",
        "gridsearch = GridSearchCV(estimator=model, param_grid=parameters, scoring=\"accuracy\")\n",
        "gridsearch.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "_qyVENkgKfIl",
        "outputId": "e117390c-fe66-4a35-b9c8-04ba19f15a39"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # サンプルデータを生成する\n",
        "    x, y = make_blobs(n_samples=100, n_features=2, centers=2)\n",
        "\n",
        "    # 分類モデル（※今回はニューラルネットワーク）を作成する\n",
        "    estimator = MLPClassifier()\n",
        "    estimator.fit(x, y)\n",
        "\n",
        "    # サンプルデータの値域を求める\n",
        "    f1_min = x[:, 0].min() - 0.5\n",
        "    f1_max = x[:, 0].max() + 0.5\n",
        "    f2_min = x[:, 1].min() - 0.5\n",
        "    f2_max = x[:, 1].max() + 0.5\n",
        "\n",
        "    step = 0.02\n",
        "    f1_range = np.arange(f1_min, f1_max, step)\n",
        "    f2_range = np.arange(f2_min, f2_max, step)\n",
        "    f1, f2 = np.meshgrid(f1_range, f2_range)\n",
        "\n",
        "    # 決定境界を描画する\n",
        "    Z = estimator.predict_proba(np.c_[f1.ravel(), f2.ravel()])[:, 1]\n",
        "    Z = Z.reshape(f1.shape)\n",
        "\n",
        "    plt.contourf(f1, f2, Z, cmap=plt.cm.RdBu, alpha=0.8)\n",
        "    plt.scatter(x[:,0], x[:,1], c=y, cmap=plt.cm.RdBu)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CeoCJPpx2aoW",
        "outputId": "350e691b-73c5-4250-f690-eb023b3ed5f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting optuna\n",
            "  Downloading optuna-3.0.3-py3-none-any.whl (348 kB)\n",
            "\u001b[K     |████████████████████████████████| 348 kB 30.1 MB/s \n",
            "\u001b[?25hCollecting cmaes>=0.8.2\n",
            "  Downloading cmaes-0.9.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from optuna) (6.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from optuna) (1.21.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (21.3)\n",
            "Requirement already satisfied: importlib-metadata<5.0.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (4.13.0)\n",
            "Requirement already satisfied: scipy<1.9.0,>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.7.3)\n",
            "Collecting cliff\n",
            "  Downloading cliff-3.10.1-py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 11.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.44)\n",
            "Collecting colorlog\n",
            "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting alembic>=1.5.0\n",
            "  Downloading alembic-1.8.1-py3-none-any.whl (209 kB)\n",
            "\u001b[K     |████████████████████████████████| 209 kB 73.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from optuna) (4.64.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from alembic>=1.5.0->optuna) (5.10.0)\n",
            "Collecting Mako\n",
            "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 8.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<5.0.0->optuna) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<5.0.0->optuna) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->optuna) (3.0.9)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.3.0->optuna) (2.0.1)\n",
            "Requirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (3.5.0)\n",
            "Collecting stevedore>=2.0.1\n",
            "  Downloading stevedore-3.5.2-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 8.2 MB/s \n",
            "\u001b[?25hCollecting cmd2>=1.0.0\n",
            "  Downloading cmd2-2.4.2-py3-none-any.whl (147 kB)\n",
            "\u001b[K     |████████████████████████████████| 147 kB 66.7 MB/s \n",
            "\u001b[?25hCollecting autopage>=0.4.0\n",
            "  Downloading autopage-0.5.1-py3-none-any.whl (29 kB)\n",
            "Collecting pbr!=2.1.0,>=2.0.0\n",
            "  Downloading pbr-5.11.0-py2.py3-none-any.whl (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 66.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\n",
            "Collecting pyperclip>=1.6\n",
            "  Downloading pyperclip-1.8.2.tar.gz (20 kB)\n",
            "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (22.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.0.1)\n",
            "Building wheels for collected packages: pyperclip\n",
            "  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11136 sha256=4d5e3142f3a778ee0d757f9091e2a5d44b87cbee0c4deb5b2a2f8d56575fdba5\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/18/84/8f69f8b08169c7bae2dde6bd7daf0c19fca8c8e500ee620a28\n",
            "Successfully built pyperclip\n",
            "Installing collected packages: pyperclip, pbr, stevedore, Mako, cmd2, autopage, colorlog, cmaes, cliff, alembic, optuna\n",
            "Successfully installed Mako-1.2.4 alembic-1.8.1 autopage-0.5.1 cliff-3.10.1 cmaes-0.9.0 cmd2-2.4.2 colorlog-6.7.0 optuna-3.0.3 pbr-5.11.0 pyperclip-1.8.2 stevedore-3.5.2\n"
          ]
        }
      ],
      "source": [
        "!pip install optuna\n",
        "import xgboost as xgb\n",
        "import optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-6ezRbsiWsE",
        "outputId": "d98e4457-3d56-4f4a-8872-d2620f10ef80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting seaborn_analyzer\n",
            "  Downloading seaborn_analyzer-0.2.13-py3-none-any.whl (59 kB)\n",
            "\u001b[K     |████████████████████████████████| 59 kB 2.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.24.2 in /usr/local/lib/python3.7/dist-packages (from seaborn_analyzer) (1.0.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.7/dist-packages (from seaborn_analyzer) (0.11.2)\n",
            "Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.7/dist-packages (from seaborn_analyzer) (1.21.6)\n",
            "Requirement already satisfied: pandas>=1.2.4 in /usr/local/lib/python3.7/dist-packages (from seaborn_analyzer) (1.3.5)\n",
            "Requirement already satisfied: scipy>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from seaborn_analyzer) (1.7.3)\n",
            "Requirement already satisfied: matplotlib>=3.1.3 in /usr/local/lib/python3.7/dist-packages (from seaborn_analyzer) (3.2.2)\n",
            "Collecting lightgbm>=3.3.2\n",
            "  Downloading lightgbm-3.3.3-py3-none-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0 MB 9.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from lightgbm>=3.3.2->seaborn_analyzer) (0.38.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.3->seaborn_analyzer) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.3->seaborn_analyzer) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.3->seaborn_analyzer) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.3->seaborn_analyzer) (1.4.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=3.1.3->seaborn_analyzer) (4.1.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.4->seaborn_analyzer) (2022.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=3.1.3->seaborn_analyzer) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24.2->seaborn_analyzer) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24.2->seaborn_analyzer) (1.2.0)\n",
            "Installing collected packages: lightgbm, seaborn-analyzer\n",
            "  Attempting uninstall: lightgbm\n",
            "    Found existing installation: lightgbm 2.2.3\n",
            "    Uninstalling lightgbm-2.2.3:\n",
            "      Successfully uninstalled lightgbm-2.2.3\n",
            "Successfully installed lightgbm-3.3.3 seaborn-analyzer-0.2.13\n"
          ]
        }
      ],
      "source": [
        "!pip install seaborn_analyzer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "rSAsBFZdzzpY",
        "outputId": "e83fe754-71b5-49e5-b297-7d03b9a6e6c9"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-499470768259>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m               \u001b[0;34m'early_stopping_rounds'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# 学習時、評価指標がこの回数連続で改善しなくなった時点でストップ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m               \u001b[0;34m'eval_metric'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'rmse'\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# early_stopping_roundsの評価指標\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m               \u001b[0;34m'eval_set'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# early_stopping_roundsの評価指標算出用データ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m               }\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# クロスバリデーションして決定境界を可視化\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
          ]
        }
      ],
      "source": [
        "from seaborn_analyzer import regplot\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "X_resampled = pd.read_csv(\"X_resampled.csv\", index_col=0)\n",
        "y_resampled = pd.read_csv(\"y_resampled.csv\", index_col=0)\n",
        "\n",
        "X_resampled = (X_resampled.mean()-X_resampled)/X_resampled.std()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.3, stratify=y_resampled)\n",
        "\n",
        "# 乱数シード\n",
        "seed = 42\n",
        "# モデル作成\n",
        "model = XGBRegressor(booster='gbtree', objective='reg:squarederror',\n",
        "                     random_state=seed, n_estimators=10000)  # チューニング前のモデル\n",
        "# 学習時fitパラメータ指定\n",
        "fit_params = {'verbose': 0,  # 学習中のコマンドライン出力\n",
        "              'early_stopping_rounds': 10,  # 学習時、評価指標がこの回数連続で改善しなくなった時点でストップ\n",
        "              'eval_metric': 'rmse',  # early_stopping_roundsの評価指標\n",
        "              'eval_set': [(X, y)]  # early_stopping_roundsの評価指標算出用データ\n",
        "              }\n",
        "# クロスバリデーションして決定境界を可視化\n",
        "seed = 42  # 乱数シード\n",
        "cv = KFold(n_splits=3, shuffle=True, random_state=seed)  # KFoldでクロスバリデーション分割指定\n",
        "regplot.regression_heat_plot(model, USE_EXPLANATORY, OBJECTIVE_VARIALBLE, df_osaka,\n",
        "                             pair_sigmarange = 0.5, rounddigit_x1=3, rounddigit_x2=3,\n",
        "                             cv=cv, display_cv_indices=0,\n",
        "                             fit_params=fit_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "id": "yH_GLGJxfWyu",
        "outputId": "8a05bb5e-031f-4ab7-e0cd-1281b7e4d877"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-11-22 09:01:03,169]\u001b[0m A new study created in memory with name: no-name-ba2cc7f5-a24c-45b2-8edb-2577caba2e50\u001b[0m\n",
            "\u001b[33m[W 2022-11-22 09:01:03,171]\u001b[0m Trial 0 failed because of the following error: TypeError(\"objective() missing 2 required positional arguments: 'y' and 'trial'\")\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/optuna/study/_optimize.py\", line 196, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "TypeError: objective() missing 2 required positional arguments: 'y' and 'trial'\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-96be2de41a23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    426\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgc_after_trial\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m             \u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m         )\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0mreseed_sampler_rng\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0mtime_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                 \u001b[0mprogress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprogress_bar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             )\n\u001b[1;32m     78\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     ):\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: objective() missing 2 required positional arguments: 'y' and 'trial'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import xgboost as xgb\n",
        "import optuna\n",
        "\n",
        "#Objective関数の設定\n",
        "def objective(x, y, trial):\n",
        "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)\n",
        "\n",
        "    params = {\n",
        "        'objective': 'binary:logistic',\n",
        "        'max_depth': trial.suggest_int('max_depth', 1, 9),\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 10, 1000),\n",
        "        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-8, 1.0)\n",
        "    }\n",
        "\n",
        "    model = xgb.XGBClassifier(**params)\n",
        "    model.fit(x_train, y_train)\n",
        "\n",
        "    pred = model.predict(x_test)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, pred)\n",
        "    return (1-accuracy)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    study = optuna.create_study()\n",
        "    study.optimize(objective, n_trials=300)\n",
        "\n",
        "    print(study.best_params)\n",
        "    print(study.best_value)\n",
        "    print(study.best_trial)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILoD1VYgo4V7"
      },
      "outputs": [],
      "source": [
        "from lightgbm import LGBMClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gT4RQOzVrGXT"
      },
      "outputs": [],
      "source": [
        "model = LGBMClassifier()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2OJOnacrwwY"
      },
      "outputs": [],
      "source": [
        "parameters = {'max_depth': [4,6], \n",
        "              'n_estimators': [50,100,200]\n",
        "              }\n",
        "\n",
        "gridsearch = GridSearchCV(model, param_grid=parameters , verbose=0)\n",
        "gridsearch.fit(X_train, y_train)\n",
        "print(gridsearch.best_params_, gridsearch.best_score_)\n",
        "\n",
        "model = LGBMClassifier(**gridsearch.best_params_).fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "roc = roc_curve(y_test, y_pred)\n",
        "fpr, tpr, proba = roc_curve(y_test, y_pred)\n",
        "auc = auc(fpr, tpr)\n",
        "fig, ax = plt.subplots(figsize = (8,6))\n",
        "ax.plot(fpr, tpr)\n",
        "\n",
        "ax.set_xlabel('FPR')\n",
        "ax.set_ylabel('TPR')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDFUvK-QwV79"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.7.15 ('py37')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.15"
    },
    "vscode": {
      "interpreter": {
        "hash": "40523939ddebca4e52b93d036de1ef1115758b14161e2e607c68c2f27d0d25f2"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
