{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zakihir0/first-commit/blob/main/Spaceship_Titanic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fyLJluZxh37",
        "outputId": "5e20ee4c-2a3e-49b9-f97e-1a2378e1e07e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'first-commit'...\n",
            "remote: Enumerating objects: 57, done.\u001b[K\n",
            "remote: Counting objects: 100% (57/57), done.\u001b[K\n",
            "remote: Compressing objects: 100% (54/54), done.\u001b[K\n",
            "remote: Total 57 (delta 25), reused 6 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (57/57), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/zakihir0/first-commit.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7KmccXO_g46",
        "outputId": "f5720862-4e6c-4aaf-9589-b2ca86af26e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting optuna\n",
            "  Downloading optuna-3.0.4-py3-none-any.whl (348 kB)\n",
            "\u001b[K     |████████████████████████████████| 348 kB 7.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from optuna) (21.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from optuna) (1.21.6)\n",
            "Collecting cliff\n",
            "  Downloading cliff-4.1.0-py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 11.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from optuna) (4.64.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.8/dist-packages (from optuna) (1.4.44)\n",
            "Collecting colorlog\n",
            "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: importlib-metadata<5.0.0 in /usr/local/lib/python3.8/dist-packages (from optuna) (4.13.0)\n",
            "Requirement already satisfied: scipy<1.9.0,>=1.7.0 in /usr/local/lib/python3.8/dist-packages (from optuna) (1.7.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from optuna) (6.0)\n",
            "Collecting cmaes>=0.8.2\n",
            "  Downloading cmaes-0.9.0-py3-none-any.whl (23 kB)\n",
            "Collecting alembic>=1.5.0\n",
            "  Downloading alembic-1.8.1-py3-none-any.whl (209 kB)\n",
            "\u001b[K     |████████████████████████████████| 209 kB 69.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.8/dist-packages (from alembic>=1.5.0->optuna) (5.10.0)\n",
            "Collecting Mako\n",
            "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 8.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata<5.0.0->optuna) (3.11.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->optuna) (3.0.9)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.8/dist-packages (from sqlalchemy>=1.3.0->optuna) (2.0.1)\n",
            "Requirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.8/dist-packages (from cliff->optuna) (3.5.0)\n",
            "Collecting cmd2>=1.0.0\n",
            "  Downloading cmd2-2.4.2-py3-none-any.whl (147 kB)\n",
            "\u001b[K     |████████████████████████████████| 147 kB 79.7 MB/s \n",
            "\u001b[?25hCollecting autopage>=0.4.0\n",
            "  Downloading autopage-0.5.1-py3-none-any.whl (29 kB)\n",
            "Collecting stevedore>=2.0.1\n",
            "  Downloading stevedore-4.1.1-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 7.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.8/dist-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\n",
            "Collecting pyperclip>=1.6\n",
            "  Downloading pyperclip-1.8.2.tar.gz (20 kB)\n",
            "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.8/dist-packages (from cmd2>=1.0.0->cliff->optuna) (22.1.0)\n",
            "Collecting pbr!=2.1.0,>=2.0.0\n",
            "  Downloading pbr-5.11.0-py2.py3-none-any.whl (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 74.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.8/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.0.1)\n",
            "Building wheels for collected packages: pyperclip\n",
            "  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11136 sha256=c9b54c62537379b21023cadf03c9cc4a6172915a9b1eab9a10df87bc308be663\n",
            "  Stored in directory: /root/.cache/pip/wheels/7f/1a/65/84ff8c386bec21fca6d220ea1f5498a0367883a78dd5ba6122\n",
            "Successfully built pyperclip\n",
            "Installing collected packages: pyperclip, pbr, stevedore, Mako, cmd2, autopage, colorlog, cmaes, cliff, alembic, optuna\n",
            "Successfully installed Mako-1.2.4 alembic-1.8.1 autopage-0.5.1 cliff-4.1.0 cmaes-0.9.0 cmd2-2.4.2 colorlog-6.7.0 optuna-3.0.4 pbr-5.11.0 pyperclip-1.8.2 stevedore-4.1.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.8/dist-packages (0.90)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from xgboost) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from xgboost) (1.7.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install optuna\n",
        "!pip install xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8F1DxAOKhMg8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import (roc_curve, auc)\n",
        "import matplotlib.pyplot as plt\n",
        "from imblearn.under_sampling import CondensedNearestNeighbour\n",
        "from imblearn.under_sampling import EditedNearestNeighbours\n",
        "from collections import Counter\n",
        "import glob\n",
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import optuna\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from imblearn.under_sampling import ClusterCentroids\n",
        "from optuna.integration import lightgbm as lgb\n",
        "from sklearn import metrics\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (PrecisionRecallDisplay, RocCurveDisplay,\n",
        "                             accuracy_score, adjusted_mutual_info_score,\n",
        "                             adjusted_rand_score, auc, average_precision_score,\n",
        "                             balanced_accuracy_score, brier_score_loss,\n",
        "                             calinski_harabasz_score, check_scoring,\n",
        "                             classification_report, cluster, cohen_kappa_score,\n",
        "                             completeness_score, confusion_matrix,\n",
        "                             consensus_score, coverage_error, d2_tweedie_score,\n",
        "                             davies_bouldin_score, dcg_score, det_curve,\n",
        "                             euclidean_distances, explained_variance_score,\n",
        "                             f1_score, fbeta_score, fowlkes_mallows_score,\n",
        "                             get_scorer, hamming_loss, hinge_loss,\n",
        "                             homogeneity_completeness_v_measure,\n",
        "                             homogeneity_score, jaccard_score,\n",
        "                             label_ranking_average_precision_score,\n",
        "                             label_ranking_loss, log_loss, make_scorer,\n",
        "                             matthews_corrcoef, max_error, mean_absolute_error,\n",
        "                             mean_absolute_percentage_error,\n",
        "                             mean_gamma_deviance, mean_pinball_loss,\n",
        "                             mean_poisson_deviance, mean_squared_error,\n",
        "                             mean_squared_log_error, mean_tweedie_deviance,\n",
        "                             median_absolute_error,\n",
        "                             multilabel_confusion_matrix, mutual_info_score,\n",
        "                             nan_euclidean_distances, ndcg_score,\n",
        "                             normalized_mutual_info_score,\n",
        "                             pair_confusion_matrix, pairwise_distances,\n",
        "                             pairwise_distances_argmin,\n",
        "                             pairwise_distances_argmin_min,\n",
        "                             pairwise_distances_chunked, pairwise_kernels,\n",
        "                             plot_confusion_matrix, plot_det_curve,\n",
        "                             plot_precision_recall_curve, plot_roc_curve,\n",
        "                             precision_recall_curve,\n",
        "                             precision_recall_fscore_support, precision_score,\n",
        "                             r2_score, rand_score, recall_score, roc_auc_score,\n",
        "                             roc_curve, silhouette_samples, silhouette_score,\n",
        "                             top_k_accuracy_score, v_measure_score,\n",
        "                             zero_one_loss)\n",
        "from sklearn.model_selection import (BaseCrossValidator, BaseShuffleSplit,\n",
        "                                     GridSearchCV, GroupKFold,\n",
        "                                     GroupShuffleSplit, KFold,\n",
        "                                     LeaveOneGroupOut, LeaveOneOut,\n",
        "                                     LeavePGroupsOut, LeavePOut,\n",
        "                                     PredefinedSplit, RepeatedKFold,\n",
        "                                     RepeatedStratifiedKFold, ShuffleSplit,\n",
        "                                     StratifiedGroupKFold, StratifiedKFold,\n",
        "                                     StratifiedShuffleSplit,\n",
        "                                     check_cv, cross_val_predict,\n",
        "                                     cross_val_score, cross_validate,\n",
        "                                     learning_curve, permutation_test_score,\n",
        "                                     train_test_split, validation_curve)\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.manifold import TSNE\n",
        "from imblearn.under_sampling import RandomUnderSampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "rQCSsW1LxXo6"
      },
      "outputs": [],
      "source": [
        "test = pd.read_csv(\"https://raw.githubusercontent.com/zakihir0/first-commit/main/test.csv\")\n",
        "train = pd.read_csv(\"https://raw.githubusercontent.com/zakihir0/first-commit/main/train.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "EDS52sgYi3Ny"
      },
      "outputs": [],
      "source": [
        "colname = ['HomePlanet', 'CryoSleep', 'Cabin', 'Destination', 'Age', 'VIP', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'Transported']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "dBioqMV0xXo7"
      },
      "outputs": [],
      "source": [
        "df = train[colname]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 679
        },
        "id": "wqctvFzsxXo7",
        "outputId": "b8db4a9c-d6f9-43ad-f899-a01df9e307aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "df_obj_colname: Index(['HomePlanet', 'CryoSleep', 'Cabin', 'Destination', 'VIP'], dtype='object')\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PassengerId</th>\n",
              "      <th>HomePlanet</th>\n",
              "      <th>CryoSleep</th>\n",
              "      <th>Cabin</th>\n",
              "      <th>Destination</th>\n",
              "      <th>Age</th>\n",
              "      <th>VIP</th>\n",
              "      <th>RoomService</th>\n",
              "      <th>FoodCourt</th>\n",
              "      <th>ShoppingMall</th>\n",
              "      <th>Spa</th>\n",
              "      <th>VRDeck</th>\n",
              "      <th>Name</th>\n",
              "      <th>Transported</th>\n",
              "      <th>CabinDeck</th>\n",
              "      <th>CabinNum</th>\n",
              "      <th>CabinSide</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0001_01</td>\n",
              "      <td>Europa</td>\n",
              "      <td>False</td>\n",
              "      <td>B/0/P</td>\n",
              "      <td>TRAPPIST-1e</td>\n",
              "      <td>39.0</td>\n",
              "      <td>False</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Maham Ofracculy</td>\n",
              "      <td>False</td>\n",
              "      <td>B</td>\n",
              "      <td>0</td>\n",
              "      <td>P</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0002_01</td>\n",
              "      <td>Earth</td>\n",
              "      <td>False</td>\n",
              "      <td>F/0/S</td>\n",
              "      <td>TRAPPIST-1e</td>\n",
              "      <td>24.0</td>\n",
              "      <td>False</td>\n",
              "      <td>109.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>549.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>Juanna Vines</td>\n",
              "      <td>True</td>\n",
              "      <td>F</td>\n",
              "      <td>0</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0003_01</td>\n",
              "      <td>Europa</td>\n",
              "      <td>False</td>\n",
              "      <td>A/0/S</td>\n",
              "      <td>TRAPPIST-1e</td>\n",
              "      <td>58.0</td>\n",
              "      <td>True</td>\n",
              "      <td>43.0</td>\n",
              "      <td>3576.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6715.0</td>\n",
              "      <td>49.0</td>\n",
              "      <td>Altark Susent</td>\n",
              "      <td>False</td>\n",
              "      <td>A</td>\n",
              "      <td>0</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0003_02</td>\n",
              "      <td>Europa</td>\n",
              "      <td>False</td>\n",
              "      <td>A/0/S</td>\n",
              "      <td>TRAPPIST-1e</td>\n",
              "      <td>33.0</td>\n",
              "      <td>False</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1283.0</td>\n",
              "      <td>371.0</td>\n",
              "      <td>3329.0</td>\n",
              "      <td>193.0</td>\n",
              "      <td>Solam Susent</td>\n",
              "      <td>False</td>\n",
              "      <td>A</td>\n",
              "      <td>0</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0004_01</td>\n",
              "      <td>Earth</td>\n",
              "      <td>False</td>\n",
              "      <td>F/1/S</td>\n",
              "      <td>TRAPPIST-1e</td>\n",
              "      <td>16.0</td>\n",
              "      <td>False</td>\n",
              "      <td>303.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>151.0</td>\n",
              "      <td>565.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>Willy Santantines</td>\n",
              "      <td>True</td>\n",
              "      <td>F</td>\n",
              "      <td>1</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8688</th>\n",
              "      <td>9276_01</td>\n",
              "      <td>Europa</td>\n",
              "      <td>False</td>\n",
              "      <td>A/98/P</td>\n",
              "      <td>55 Cancri e</td>\n",
              "      <td>41.0</td>\n",
              "      <td>True</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6819.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1643.0</td>\n",
              "      <td>74.0</td>\n",
              "      <td>Gravior Noxnuther</td>\n",
              "      <td>False</td>\n",
              "      <td>A</td>\n",
              "      <td>98</td>\n",
              "      <td>P</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8689</th>\n",
              "      <td>9278_01</td>\n",
              "      <td>Earth</td>\n",
              "      <td>True</td>\n",
              "      <td>G/1499/S</td>\n",
              "      <td>PSO J318.5-22</td>\n",
              "      <td>18.0</td>\n",
              "      <td>False</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Kurta Mondalley</td>\n",
              "      <td>False</td>\n",
              "      <td>G</td>\n",
              "      <td>1499</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8690</th>\n",
              "      <td>9279_01</td>\n",
              "      <td>Earth</td>\n",
              "      <td>False</td>\n",
              "      <td>G/1500/S</td>\n",
              "      <td>TRAPPIST-1e</td>\n",
              "      <td>26.0</td>\n",
              "      <td>False</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1872.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Fayey Connon</td>\n",
              "      <td>True</td>\n",
              "      <td>G</td>\n",
              "      <td>1500</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8691</th>\n",
              "      <td>9280_01</td>\n",
              "      <td>Europa</td>\n",
              "      <td>False</td>\n",
              "      <td>E/608/S</td>\n",
              "      <td>55 Cancri e</td>\n",
              "      <td>32.0</td>\n",
              "      <td>False</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1049.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>353.0</td>\n",
              "      <td>3235.0</td>\n",
              "      <td>Celeon Hontichre</td>\n",
              "      <td>False</td>\n",
              "      <td>E</td>\n",
              "      <td>608</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8692</th>\n",
              "      <td>9280_02</td>\n",
              "      <td>Europa</td>\n",
              "      <td>False</td>\n",
              "      <td>E/608/S</td>\n",
              "      <td>TRAPPIST-1e</td>\n",
              "      <td>44.0</td>\n",
              "      <td>False</td>\n",
              "      <td>126.0</td>\n",
              "      <td>4688.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>Propsh Hontichre</td>\n",
              "      <td>True</td>\n",
              "      <td>E</td>\n",
              "      <td>608</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8693 rows × 17 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     PassengerId HomePlanet CryoSleep     Cabin    Destination   Age    VIP  \\\n",
              "0        0001_01     Europa     False     B/0/P    TRAPPIST-1e  39.0  False   \n",
              "1        0002_01      Earth     False     F/0/S    TRAPPIST-1e  24.0  False   \n",
              "2        0003_01     Europa     False     A/0/S    TRAPPIST-1e  58.0   True   \n",
              "3        0003_02     Europa     False     A/0/S    TRAPPIST-1e  33.0  False   \n",
              "4        0004_01      Earth     False     F/1/S    TRAPPIST-1e  16.0  False   \n",
              "...          ...        ...       ...       ...            ...   ...    ...   \n",
              "8688     9276_01     Europa     False    A/98/P    55 Cancri e  41.0   True   \n",
              "8689     9278_01      Earth      True  G/1499/S  PSO J318.5-22  18.0  False   \n",
              "8690     9279_01      Earth     False  G/1500/S    TRAPPIST-1e  26.0  False   \n",
              "8691     9280_01     Europa     False   E/608/S    55 Cancri e  32.0  False   \n",
              "8692     9280_02     Europa     False   E/608/S    TRAPPIST-1e  44.0  False   \n",
              "\n",
              "      RoomService  FoodCourt  ShoppingMall     Spa  VRDeck               Name  \\\n",
              "0             0.0        0.0           0.0     0.0     0.0    Maham Ofracculy   \n",
              "1           109.0        9.0          25.0   549.0    44.0       Juanna Vines   \n",
              "2            43.0     3576.0           0.0  6715.0    49.0      Altark Susent   \n",
              "3             0.0     1283.0         371.0  3329.0   193.0       Solam Susent   \n",
              "4           303.0       70.0         151.0   565.0     2.0  Willy Santantines   \n",
              "...           ...        ...           ...     ...     ...                ...   \n",
              "8688          0.0     6819.0           0.0  1643.0    74.0  Gravior Noxnuther   \n",
              "8689          0.0        0.0           0.0     0.0     0.0    Kurta Mondalley   \n",
              "8690          0.0        0.0        1872.0     1.0     0.0       Fayey Connon   \n",
              "8691          0.0     1049.0           0.0   353.0  3235.0   Celeon Hontichre   \n",
              "8692        126.0     4688.0           0.0     0.0    12.0   Propsh Hontichre   \n",
              "\n",
              "      Transported CabinDeck CabinNum CabinSide  \n",
              "0           False         B        0         P  \n",
              "1            True         F        0         S  \n",
              "2           False         A        0         S  \n",
              "3           False         A        0         S  \n",
              "4            True         F        1         S  \n",
              "...           ...       ...      ...       ...  \n",
              "8688        False         A       98         P  \n",
              "8689        False         G     1499         S  \n",
              "8690         True         G     1500         S  \n",
              "8691        False         E      608         S  \n",
              "8692         True         E      608         S  \n",
              "\n",
              "[8693 rows x 17 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "df_obj_colname = df.columns[df.dtypes == \"object\"]\n",
        "print(f\"df_obj_colname: {df_obj_colname}\")\n",
        "\n",
        "train[['CabinDeck','CabinNum','CabinSide']] = train['Cabin'].str.split('/', expand=True)\n",
        "test[['CabinDeck','CabinNum','CabinSide']] = test['Cabin'].str.split('/', expand=True)\n",
        "display(train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        },
        "id": "LxpeYI4GZJef",
        "outputId": "4bcadb4b-ee62-40d6-8e21-84f870eba1b6"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CabinDeck\n",
            "A     256\n",
            "B     779\n",
            "C     747\n",
            "D     478\n",
            "E     876\n",
            "F    2794\n",
            "G    2559\n",
            "T       5\n",
            "Name: PassengerId, dtype: int64\n",
            "CabinSide\n",
            "P    4206\n",
            "S    4288\n",
            "Name: PassengerId, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "plt.hist(train.groupby(by=[\"CabinNum\"])[\"PassengerId\"].count())\n",
        "plt.title(\"CabinNum-PassengerID\")\n",
        "plt.show()\n",
        "\n",
        "train[\"CabinNum\"] = pd.to_numeric(train[\"CabinNum\"])\n",
        "test[\"CabinNum\"] = pd.to_numeric(test[\"CabinNum\"])\n",
        "\n",
        "print(train.groupby(by=[\"CabinDeck\"])[\"PassengerId\"].count())\n",
        "\n",
        "print(train.groupby(by=[\"CabinSide\"])[\"PassengerId\"].count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        },
        "id": "3yRhDjM6ZFvG",
        "outputId": "a505a4c9-281d-456c-f844-1b1cd42bcd32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_dummy_colname: Index(['HomePlanet', 'CryoSleep', 'Destination', 'VIP', 'CabinDeck',\n",
            "       'CabinNum', 'CabinSide'],\n",
            "      dtype='object')\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>HomePlanet</th>\n",
              "      <th>CryoSleep</th>\n",
              "      <th>Destination</th>\n",
              "      <th>Age</th>\n",
              "      <th>VIP</th>\n",
              "      <th>RoomService</th>\n",
              "      <th>FoodCourt</th>\n",
              "      <th>ShoppingMall</th>\n",
              "      <th>Spa</th>\n",
              "      <th>VRDeck</th>\n",
              "      <th>Name</th>\n",
              "      <th>Transported</th>\n",
              "      <th>CabinDeck</th>\n",
              "      <th>CabinNum</th>\n",
              "      <th>CabinSide</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Europa</td>\n",
              "      <td>False</td>\n",
              "      <td>TRAPPIST-1e</td>\n",
              "      <td>39.0</td>\n",
              "      <td>False</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Maham Ofracculy</td>\n",
              "      <td>False</td>\n",
              "      <td>B</td>\n",
              "      <td>0.0</td>\n",
              "      <td>P</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Earth</td>\n",
              "      <td>False</td>\n",
              "      <td>TRAPPIST-1e</td>\n",
              "      <td>24.0</td>\n",
              "      <td>False</td>\n",
              "      <td>109.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>549.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>Juanna Vines</td>\n",
              "      <td>True</td>\n",
              "      <td>F</td>\n",
              "      <td>0.0</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Europa</td>\n",
              "      <td>False</td>\n",
              "      <td>TRAPPIST-1e</td>\n",
              "      <td>58.0</td>\n",
              "      <td>True</td>\n",
              "      <td>43.0</td>\n",
              "      <td>3576.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6715.0</td>\n",
              "      <td>49.0</td>\n",
              "      <td>Altark Susent</td>\n",
              "      <td>False</td>\n",
              "      <td>A</td>\n",
              "      <td>0.0</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Europa</td>\n",
              "      <td>False</td>\n",
              "      <td>TRAPPIST-1e</td>\n",
              "      <td>33.0</td>\n",
              "      <td>False</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1283.0</td>\n",
              "      <td>371.0</td>\n",
              "      <td>3329.0</td>\n",
              "      <td>193.0</td>\n",
              "      <td>Solam Susent</td>\n",
              "      <td>False</td>\n",
              "      <td>A</td>\n",
              "      <td>0.0</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Earth</td>\n",
              "      <td>False</td>\n",
              "      <td>TRAPPIST-1e</td>\n",
              "      <td>16.0</td>\n",
              "      <td>False</td>\n",
              "      <td>303.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>151.0</td>\n",
              "      <td>565.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>Willy Santantines</td>\n",
              "      <td>True</td>\n",
              "      <td>F</td>\n",
              "      <td>1.0</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8688</th>\n",
              "      <td>Europa</td>\n",
              "      <td>False</td>\n",
              "      <td>55 Cancri e</td>\n",
              "      <td>41.0</td>\n",
              "      <td>True</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6819.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1643.0</td>\n",
              "      <td>74.0</td>\n",
              "      <td>Gravior Noxnuther</td>\n",
              "      <td>False</td>\n",
              "      <td>A</td>\n",
              "      <td>98.0</td>\n",
              "      <td>P</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8689</th>\n",
              "      <td>Earth</td>\n",
              "      <td>True</td>\n",
              "      <td>PSO J318.5-22</td>\n",
              "      <td>18.0</td>\n",
              "      <td>False</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Kurta Mondalley</td>\n",
              "      <td>False</td>\n",
              "      <td>G</td>\n",
              "      <td>1499.0</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8690</th>\n",
              "      <td>Earth</td>\n",
              "      <td>False</td>\n",
              "      <td>TRAPPIST-1e</td>\n",
              "      <td>26.0</td>\n",
              "      <td>False</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1872.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Fayey Connon</td>\n",
              "      <td>True</td>\n",
              "      <td>G</td>\n",
              "      <td>1500.0</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8691</th>\n",
              "      <td>Europa</td>\n",
              "      <td>False</td>\n",
              "      <td>55 Cancri e</td>\n",
              "      <td>32.0</td>\n",
              "      <td>False</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1049.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>353.0</td>\n",
              "      <td>3235.0</td>\n",
              "      <td>Celeon Hontichre</td>\n",
              "      <td>False</td>\n",
              "      <td>E</td>\n",
              "      <td>608.0</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8692</th>\n",
              "      <td>Europa</td>\n",
              "      <td>False</td>\n",
              "      <td>TRAPPIST-1e</td>\n",
              "      <td>44.0</td>\n",
              "      <td>False</td>\n",
              "      <td>126.0</td>\n",
              "      <td>4688.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>Propsh Hontichre</td>\n",
              "      <td>True</td>\n",
              "      <td>E</td>\n",
              "      <td>608.0</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6606 rows × 15 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     HomePlanet CryoSleep    Destination   Age    VIP  RoomService  FoodCourt  \\\n",
              "0        Europa     False    TRAPPIST-1e  39.0  False          0.0        0.0   \n",
              "1         Earth     False    TRAPPIST-1e  24.0  False        109.0        9.0   \n",
              "2        Europa     False    TRAPPIST-1e  58.0   True         43.0     3576.0   \n",
              "3        Europa     False    TRAPPIST-1e  33.0  False          0.0     1283.0   \n",
              "4         Earth     False    TRAPPIST-1e  16.0  False        303.0       70.0   \n",
              "...         ...       ...            ...   ...    ...          ...        ...   \n",
              "8688     Europa     False    55 Cancri e  41.0   True          0.0     6819.0   \n",
              "8689      Earth      True  PSO J318.5-22  18.0  False          0.0        0.0   \n",
              "8690      Earth     False    TRAPPIST-1e  26.0  False          0.0        0.0   \n",
              "8691     Europa     False    55 Cancri e  32.0  False          0.0     1049.0   \n",
              "8692     Europa     False    TRAPPIST-1e  44.0  False        126.0     4688.0   \n",
              "\n",
              "      ShoppingMall     Spa  VRDeck               Name  Transported CabinDeck  \\\n",
              "0              0.0     0.0     0.0    Maham Ofracculy        False         B   \n",
              "1             25.0   549.0    44.0       Juanna Vines         True         F   \n",
              "2              0.0  6715.0    49.0      Altark Susent        False         A   \n",
              "3            371.0  3329.0   193.0       Solam Susent        False         A   \n",
              "4            151.0   565.0     2.0  Willy Santantines         True         F   \n",
              "...            ...     ...     ...                ...          ...       ...   \n",
              "8688           0.0  1643.0    74.0  Gravior Noxnuther        False         A   \n",
              "8689           0.0     0.0     0.0    Kurta Mondalley        False         G   \n",
              "8690        1872.0     1.0     0.0       Fayey Connon         True         G   \n",
              "8691           0.0   353.0  3235.0   Celeon Hontichre        False         E   \n",
              "8692           0.0     0.0    12.0   Propsh Hontichre         True         E   \n",
              "\n",
              "      CabinNum CabinSide  \n",
              "0          0.0         P  \n",
              "1          0.0         S  \n",
              "2          0.0         S  \n",
              "3          0.0         S  \n",
              "4          1.0         S  \n",
              "...        ...       ...  \n",
              "8688      98.0         P  \n",
              "8689    1499.0         S  \n",
              "8690    1500.0         S  \n",
              "8691     608.0         S  \n",
              "8692     608.0         S  \n",
              "\n",
              "[6606 rows x 15 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "train = train.drop(\"Cabin\", axis=1)\n",
        "test = test.drop(\"Cabin\", axis=1)\n",
        "\n",
        "train_dummy_colname = train[['HomePlanet', 'CryoSleep', 'Destination', 'VIP', \"CabinDeck\", \"CabinNum\", \"CabinSide\"]].columns\n",
        "print(f\"train_dummy_colname: {train_dummy_colname}\")\n",
        "\n",
        "train_drop = train.drop(\"PassengerId\", axis=1)\n",
        "train_drop.dropna(inplace=True)\n",
        "\n",
        "display(train_drop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHta1Upvtp89",
        "outputId": "818aee66-ba25-4621-c100-e9d088d7ed22"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[array(['Europa', 'Earth', 'Mars', nan], dtype=object),\n",
              " array([False, True, nan], dtype=object),\n",
              " array(['B/0/P', 'F/0/S', 'A/0/S', ..., 'G/1499/S', 'G/1500/S', 'E/608/S'],\n",
              "       dtype=object),\n",
              " array(['TRAPPIST-1e', 'PSO J318.5-22', '55 Cancri e', nan], dtype=object),\n",
              " array([39., 24., 58., 33., 16., 44., 26., 28., 35., 14., 34., 45., 32.,\n",
              "        48., 31., 27.,  0.,  1., 49., 29., 10.,  7., 21., 62., 15., 43.,\n",
              "        47.,  2., 20., 23., 30., 17., 55.,  4., 19., 56., nan, 25., 38.,\n",
              "        36., 22., 18., 42., 37., 13.,  8., 40.,  3., 54.,  9.,  6., 64.,\n",
              "        67., 61., 50., 41., 57., 11., 52., 51., 46., 60., 63., 59.,  5.,\n",
              "        79., 68., 74., 12., 53., 65., 71., 75., 70., 76., 78., 73., 66.,\n",
              "        69., 72., 77.])]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_data_list = []\n",
        "for i in range(len(df_obj_colname)):\n",
        "  df_colname = df.columns[i]\n",
        "  df_data = pd.unique(df[df_colname])\n",
        "  df_data_list.append(df_data)\n",
        "df_data_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "sDYlrNn96iCB"
      },
      "outputs": [],
      "source": [
        "dataset = pd.get_dummies(train_drop[train_dummy_colname])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "lt9R4FDTxXo8"
      },
      "outputs": [],
      "source": [
        "y = train[\"Transported\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPSmjbhDxXo8",
        "outputId": "65da6f2f-55fc-4ba9-9680-19135a4a9084"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Int64Index([   0,    1,    2,    3,    4,    5,    6,    8,    9,   11,\n",
              "            ...\n",
              "            8681, 8682, 8683, 8685, 8686, 8688, 8689, 8690, 8691, 8692],\n",
              "           dtype='int64', length=6606)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset.index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "id": "I69NVGyTxXo9",
        "outputId": "7e6f5a0d-6753-47aa-a719-d0950915f8a3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CabinNum</th>\n",
              "      <th>HomePlanet_Earth</th>\n",
              "      <th>HomePlanet_Europa</th>\n",
              "      <th>HomePlanet_Mars</th>\n",
              "      <th>CryoSleep_False</th>\n",
              "      <th>CryoSleep_True</th>\n",
              "      <th>Destination_55 Cancri e</th>\n",
              "      <th>Destination_PSO J318.5-22</th>\n",
              "      <th>Destination_TRAPPIST-1e</th>\n",
              "      <th>VIP_False</th>\n",
              "      <th>...</th>\n",
              "      <th>CabinDeck_A</th>\n",
              "      <th>CabinDeck_B</th>\n",
              "      <th>CabinDeck_C</th>\n",
              "      <th>CabinDeck_D</th>\n",
              "      <th>CabinDeck_E</th>\n",
              "      <th>CabinDeck_F</th>\n",
              "      <th>CabinDeck_G</th>\n",
              "      <th>CabinDeck_T</th>\n",
              "      <th>CabinSide_P</th>\n",
              "      <th>CabinSide_S</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8688</th>\n",
              "      <td>98.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8689</th>\n",
              "      <td>1499.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8690</th>\n",
              "      <td>1500.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8691</th>\n",
              "      <td>608.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8692</th>\n",
              "      <td>608.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6606 rows × 21 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      CabinNum  HomePlanet_Earth  HomePlanet_Europa  HomePlanet_Mars  \\\n",
              "0          0.0                 0                  1                0   \n",
              "1          0.0                 1                  0                0   \n",
              "2          0.0                 0                  1                0   \n",
              "3          0.0                 0                  1                0   \n",
              "4          1.0                 1                  0                0   \n",
              "...        ...               ...                ...              ...   \n",
              "8688      98.0                 0                  1                0   \n",
              "8689    1499.0                 1                  0                0   \n",
              "8690    1500.0                 1                  0                0   \n",
              "8691     608.0                 0                  1                0   \n",
              "8692     608.0                 0                  1                0   \n",
              "\n",
              "      CryoSleep_False  CryoSleep_True  Destination_55 Cancri e  \\\n",
              "0                   1               0                        0   \n",
              "1                   1               0                        0   \n",
              "2                   1               0                        0   \n",
              "3                   1               0                        0   \n",
              "4                   1               0                        0   \n",
              "...               ...             ...                      ...   \n",
              "8688                1               0                        1   \n",
              "8689                0               1                        0   \n",
              "8690                1               0                        0   \n",
              "8691                1               0                        1   \n",
              "8692                1               0                        0   \n",
              "\n",
              "      Destination_PSO J318.5-22  Destination_TRAPPIST-1e  VIP_False  ...  \\\n",
              "0                             0                        1          1  ...   \n",
              "1                             0                        1          1  ...   \n",
              "2                             0                        1          0  ...   \n",
              "3                             0                        1          1  ...   \n",
              "4                             0                        1          1  ...   \n",
              "...                         ...                      ...        ...  ...   \n",
              "8688                          0                        0          0  ...   \n",
              "8689                          1                        0          1  ...   \n",
              "8690                          0                        1          1  ...   \n",
              "8691                          0                        0          1  ...   \n",
              "8692                          0                        1          1  ...   \n",
              "\n",
              "      CabinDeck_A  CabinDeck_B  CabinDeck_C  CabinDeck_D  CabinDeck_E  \\\n",
              "0               0            1            0            0            0   \n",
              "1               0            0            0            0            0   \n",
              "2               1            0            0            0            0   \n",
              "3               1            0            0            0            0   \n",
              "4               0            0            0            0            0   \n",
              "...           ...          ...          ...          ...          ...   \n",
              "8688            1            0            0            0            0   \n",
              "8689            0            0            0            0            0   \n",
              "8690            0            0            0            0            0   \n",
              "8691            0            0            0            0            1   \n",
              "8692            0            0            0            0            1   \n",
              "\n",
              "      CabinDeck_F  CabinDeck_G  CabinDeck_T  CabinSide_P  CabinSide_S  \n",
              "0               0            0            0            1            0  \n",
              "1               1            0            0            0            1  \n",
              "2               0            0            0            0            1  \n",
              "3               0            0            0            0            1  \n",
              "4               1            0            0            0            1  \n",
              "...           ...          ...          ...          ...          ...  \n",
              "8688            0            0            0            1            0  \n",
              "8689            0            1            0            0            1  \n",
              "8690            0            1            0            0            1  \n",
              "8691            0            0            0            0            1  \n",
              "8692            0            0            0            0            1  \n",
              "\n",
              "[6606 rows x 21 columns]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Uxfr8HttxXo9"
      },
      "outputs": [],
      "source": [
        "y = y.iloc[dataset.index, ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ocTxHfuFm5Y9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "id": "-B97lUGuqY1U",
        "outputId": "99e46c10-cd15-4bfd-ea22-d7b049c1a03f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CabinNum</th>\n",
              "      <th>HomePlanet_Earth</th>\n",
              "      <th>HomePlanet_Europa</th>\n",
              "      <th>HomePlanet_Mars</th>\n",
              "      <th>CryoSleep_False</th>\n",
              "      <th>CryoSleep_True</th>\n",
              "      <th>Destination_55 Cancri e</th>\n",
              "      <th>Destination_PSO J318.5-22</th>\n",
              "      <th>Destination_TRAPPIST-1e</th>\n",
              "      <th>VIP_False</th>\n",
              "      <th>...</th>\n",
              "      <th>CabinDeck_A</th>\n",
              "      <th>CabinDeck_B</th>\n",
              "      <th>CabinDeck_C</th>\n",
              "      <th>CabinDeck_D</th>\n",
              "      <th>CabinDeck_E</th>\n",
              "      <th>CabinDeck_F</th>\n",
              "      <th>CabinDeck_G</th>\n",
              "      <th>CabinDeck_T</th>\n",
              "      <th>CabinSide_P</th>\n",
              "      <th>CabinSide_S</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8688</th>\n",
              "      <td>98.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8689</th>\n",
              "      <td>1499.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8690</th>\n",
              "      <td>1500.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8691</th>\n",
              "      <td>608.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8692</th>\n",
              "      <td>608.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6606 rows × 21 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      CabinNum  HomePlanet_Earth  HomePlanet_Europa  HomePlanet_Mars  \\\n",
              "0          0.0                 0                  1                0   \n",
              "1          0.0                 1                  0                0   \n",
              "2          0.0                 0                  1                0   \n",
              "3          0.0                 0                  1                0   \n",
              "4          1.0                 1                  0                0   \n",
              "...        ...               ...                ...              ...   \n",
              "8688      98.0                 0                  1                0   \n",
              "8689    1499.0                 1                  0                0   \n",
              "8690    1500.0                 1                  0                0   \n",
              "8691     608.0                 0                  1                0   \n",
              "8692     608.0                 0                  1                0   \n",
              "\n",
              "      CryoSleep_False  CryoSleep_True  Destination_55 Cancri e  \\\n",
              "0                   1               0                        0   \n",
              "1                   1               0                        0   \n",
              "2                   1               0                        0   \n",
              "3                   1               0                        0   \n",
              "4                   1               0                        0   \n",
              "...               ...             ...                      ...   \n",
              "8688                1               0                        1   \n",
              "8689                0               1                        0   \n",
              "8690                1               0                        0   \n",
              "8691                1               0                        1   \n",
              "8692                1               0                        0   \n",
              "\n",
              "      Destination_PSO J318.5-22  Destination_TRAPPIST-1e  VIP_False  ...  \\\n",
              "0                             0                        1          1  ...   \n",
              "1                             0                        1          1  ...   \n",
              "2                             0                        1          0  ...   \n",
              "3                             0                        1          1  ...   \n",
              "4                             0                        1          1  ...   \n",
              "...                         ...                      ...        ...  ...   \n",
              "8688                          0                        0          0  ...   \n",
              "8689                          1                        0          1  ...   \n",
              "8690                          0                        1          1  ...   \n",
              "8691                          0                        0          1  ...   \n",
              "8692                          0                        1          1  ...   \n",
              "\n",
              "      CabinDeck_A  CabinDeck_B  CabinDeck_C  CabinDeck_D  CabinDeck_E  \\\n",
              "0               0            1            0            0            0   \n",
              "1               0            0            0            0            0   \n",
              "2               1            0            0            0            0   \n",
              "3               1            0            0            0            0   \n",
              "4               0            0            0            0            0   \n",
              "...           ...          ...          ...          ...          ...   \n",
              "8688            1            0            0            0            0   \n",
              "8689            0            0            0            0            0   \n",
              "8690            0            0            0            0            0   \n",
              "8691            0            0            0            0            1   \n",
              "8692            0            0            0            0            1   \n",
              "\n",
              "      CabinDeck_F  CabinDeck_G  CabinDeck_T  CabinSide_P  CabinSide_S  \n",
              "0               0            0            0            1            0  \n",
              "1               1            0            0            0            1  \n",
              "2               0            0            0            0            1  \n",
              "3               0            0            0            0            1  \n",
              "4               1            0            0            0            1  \n",
              "...           ...          ...          ...          ...          ...  \n",
              "8688            0            0            0            1            0  \n",
              "8689            0            1            0            0            1  \n",
              "8690            0            1            0            0            1  \n",
              "8691            0            0            0            0            1  \n",
              "8692            0            0            0            0            1  \n",
              "\n",
              "[6606 rows x 21 columns]"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "cVUxV1AfqbLU"
      },
      "outputs": [],
      "source": [
        "dataset[\"Transported\"] = y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "G3zh8nu6qfqS"
      },
      "outputs": [],
      "source": [
        "df = dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "1yzRvnqggOBe"
      },
      "outputs": [],
      "source": [
        "X = df.iloc[:, 0:-1]\n",
        "y = df.iloc[:, -1] "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "DHhfUsD-lpcL"
      },
      "outputs": [],
      "source": [
        "X.to_csv(\"X.csv\")\n",
        "y.to_csv(\"y.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "ynGUFq9DxXo9"
      },
      "outputs": [],
      "source": [
        "X_scaled = (X-X.mean())/X.std()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.1, stratify=y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "rBq5n0FFxXo-"
      },
      "outputs": [],
      "source": [
        "class XGB_optuna:\n",
        "    def __init__(self, X, y, method):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.method = method\n",
        "\n",
        "    def validation(self):\n",
        "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X, self.y, test_size=0.1)\n",
        "        return self.X_train, self.X_test, self.y_train, self.y_test\n",
        "        \n",
        "    def under_sampling(self):\n",
        "        X_train, X_test, y_train, y_test = self.validation()\n",
        "        X_res, y_res = self.method.fit_resample(X_train, y_train)\n",
        "        return X_res, y_res\n",
        "\n",
        "    def objective(self, trial):\n",
        "        params = {\n",
        "            'n_estimators': trial.suggest_int('n_estimators', 0, 1000), \n",
        "            'max_depth': trial.suggest_int('max_depth', 1, 20), \n",
        "            'min_child_weight': trial.suggest_int('min_child_weight', 1, 20), \n",
        "            'subsample':trial.suggest_discrete_uniform('subsample', 0.5, 0.9, 0.1), \n",
        "            'colsample_bytree': trial.suggest_discrete_uniform('colsample_bytree', 0.5, 0.9, 0.1),\n",
        "            'verbose': 0\n",
        "            }\n",
        "        model = XGBClassifier(**params)\n",
        "        X_res, y_res = self.under_sampling()\n",
        "        model.fit(X_res, y_res)\n",
        "        y_pred = model.predict(self.X_test)\n",
        "        accuracy = accuracy_score(self.y_test, y_pred)\n",
        "        return (1-accuracy)\n",
        "\n",
        "    def find_params(self):\n",
        "        study = optuna.create_study()\n",
        "        study.optimize(self.objective, n_trials=300)\n",
        "        return study.best_params\n",
        "\n",
        "    def build_model(self):\n",
        "        params = self.find_params()\n",
        "        model = XGBClassifier(**params)\n",
        "        X_res, y_res = self.under_sampling()\n",
        "        model.fit(X_res, y_res)\n",
        "\n",
        "        return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "5ZqhzyxD9gve"
      },
      "outputs": [],
      "source": [
        "method = RandomUnderSampler()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "3WcGDsocqPgb"
      },
      "outputs": [],
      "source": [
        "A = XGB_optuna(X_train, y_train, method)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Counter({1: 2994, 0: 2951})\n"
          ]
        }
      ],
      "source": [
        "print(Counter(y_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9k57F4K3qyRM",
        "outputId": "19c6ef4c-24b1-4252-c363-abaaba232ea3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:05:11,218]\u001b[0m A new study created in memory with name: no-name-40b5679b-5bf8-4456-a2eb-fbcf36465c8a\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:05:11] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:05:11] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-14 03:05:15,155]\u001b[0m Trial 0 finished with value: 0.3243697478991596 and parameters: {'n_estimators': 953, 'max_depth': 16, 'min_child_weight': 6, 'subsample': 0.5, 'colsample_bytree': 0.6}. Best is trial 0 with value: 0.3243697478991596.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:05:15] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:05:15] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:05:16,445]\u001b[0m Trial 1 finished with value: 0.2974789915966387 and parameters: {'n_estimators': 386, 'max_depth': 16, 'min_child_weight': 18, 'subsample': 0.5, 'colsample_bytree': 0.5}. Best is trial 1 with value: 0.2974789915966387.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:05:16] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:05:16] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:05:19,025]\u001b[0m Trial 2 finished with value: 0.28403361344537814 and parameters: {'n_estimators': 590, 'max_depth': 19, 'min_child_weight': 15, 'subsample': 0.7, 'colsample_bytree': 0.8}. Best is trial 2 with value: 0.28403361344537814.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:05:19] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:05:19] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:05:21,089]\u001b[0m Trial 3 finished with value: 0.29075630252100837 and parameters: {'n_estimators': 950, 'max_depth': 2, 'min_child_weight': 13, 'subsample': 0.6, 'colsample_bytree': 0.8}. Best is trial 2 with value: 0.28403361344537814.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:05:21] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:05:21] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:05:21,473]\u001b[0m Trial 4 finished with value: 0.27226890756302524 and parameters: {'n_estimators': 59, 'max_depth': 8, 'min_child_weight': 1, 'subsample': 0.9, 'colsample_bytree': 0.9}. Best is trial 4 with value: 0.27226890756302524.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:05:21] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:05:21] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:05:27,022]\u001b[0m Trial 5 finished with value: 0.3176470588235294 and parameters: {'n_estimators': 852, 'max_depth': 16, 'min_child_weight': 6, 'subsample': 0.9, 'colsample_bytree': 0.9}. Best is trial 4 with value: 0.27226890756302524.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:05:27] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:05:27] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:05:28,190]\u001b[0m Trial 6 finished with value: 0.2941176470588235 and parameters: {'n_estimators': 780, 'max_depth': 1, 'min_child_weight': 17, 'subsample': 0.7, 'colsample_bytree': 0.8}. Best is trial 4 with value: 0.27226890756302524.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:05:28] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:05:28] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:05:30,809]\u001b[0m Trial 7 finished with value: 0.3142857142857143 and parameters: {'n_estimators': 437, 'max_depth': 14, 'min_child_weight': 3, 'subsample': 0.8, 'colsample_bytree': 0.8}. Best is trial 4 with value: 0.27226890756302524.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:05:30] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:05:30] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:05:34,498]\u001b[0m Trial 8 finished with value: 0.31932773109243695 and parameters: {'n_estimators': 808, 'max_depth': 9, 'min_child_weight': 7, 'subsample': 0.5, 'colsample_bytree': 0.8}. Best is trial 4 with value: 0.27226890756302524.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:05:34] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:05:34] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:05:35,015]\u001b[0m Trial 9 finished with value: 0.29579831932773104 and parameters: {'n_estimators': 78, 'max_depth': 16, 'min_child_weight': 4, 'subsample': 0.5, 'colsample_bytree': 0.9}. Best is trial 4 with value: 0.27226890756302524.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-14 03:05:35,231]\u001b[0m Trial 10 finished with value: 0.27226890756302524 and parameters: {'n_estimators': 40, 'max_depth': 8, 'min_child_weight': 11, 'subsample': 0.9, 'colsample_bytree': 0.6}. Best is trial 4 with value: 0.27226890756302524.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:05:35] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:05:35] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-14 03:05:35,374]\u001b[0m Trial 11 finished with value: 0.26890756302521013 and parameters: {'n_estimators': 22, 'max_depth': 7, 'min_child_weight': 10, 'subsample': 0.9, 'colsample_bytree': 0.6}. Best is trial 11 with value: 0.26890756302521013.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:05:35] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:05:35] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "[03:05:35] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:05:35] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:05:36,021]\u001b[0m Trial 12 finished with value: 0.2974789915966387 and parameters: {'n_estimators': 218, 'max_depth': 5, 'min_child_weight': 1, 'subsample': 0.8, 'colsample_bytree': 0.6}. Best is trial 11 with value: 0.26890756302521013.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:05:36] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:05:36] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:05:36,834]\u001b[0m Trial 13 finished with value: 0.2890756302521008 and parameters: {'n_estimators': 265, 'max_depth': 6, 'min_child_weight': 9, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 11 with value: 0.26890756302521013.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:05:36] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:05:36] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:05:37,536]\u001b[0m Trial 14 finished with value: 0.28403361344537814 and parameters: {'n_estimators': 176, 'max_depth': 12, 'min_child_weight': 10, 'subsample': 0.8, 'colsample_bytree': 0.5}. Best is trial 11 with value: 0.26890756302521013.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-14 03:05:37,667]\u001b[0m Trial 15 finished with value: 0.27226890756302524 and parameters: {'n_estimators': 25, 'max_depth': 5, 'min_child_weight': 20, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 11 with value: 0.26890756302521013.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:05:37] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:05:37] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "[03:05:37] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:05:37] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:05:40,458]\u001b[0m Trial 16 finished with value: 0.3445378151260504 and parameters: {'n_estimators': 610, 'max_depth': 11, 'min_child_weight': 1, 'subsample': 0.8, 'colsample_bytree': 0.6}. Best is trial 11 with value: 0.26890756302521013.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:05:40] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:05:40] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:05:41,531]\u001b[0m Trial 17 finished with value: 0.3008403361344538 and parameters: {'n_estimators': 343, 'max_depth': 7, 'min_child_weight': 13, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 11 with value: 0.26890756302521013.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:05:41] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:05:41] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:05:41,811]\u001b[0m Trial 18 finished with value: 0.26722689075630257 and parameters: {'n_estimators': 107, 'max_depth': 3, 'min_child_weight': 8, 'subsample': 0.6, 'colsample_bytree': 0.9}. Best is trial 18 with value: 0.26722689075630257.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:05:41] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:05:41] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:05:42,132]\u001b[0m Trial 19 finished with value: 0.31596638655462184 and parameters: {'n_estimators': 139, 'max_depth': 3, 'min_child_weight': 9, 'subsample': 0.6, 'colsample_bytree': 0.5}. Best is trial 18 with value: 0.26722689075630257.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:05:42] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:05:42] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:05:42,820]\u001b[0m Trial 20 finished with value: 0.26386554621848735 and parameters: {'n_estimators': 291, 'max_depth': 4, 'min_child_weight': 13, 'subsample': 0.6, 'colsample_bytree': 0.6}. Best is trial 20 with value: 0.26386554621848735.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:05:42] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:05:42] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:05:43,461]\u001b[0m Trial 21 finished with value: 0.3176470588235294 and parameters: {'n_estimators': 250, 'max_depth': 4, 'min_child_weight': 12, 'subsample': 0.6, 'colsample_bytree': 0.6}. Best is trial 20 with value: 0.26386554621848735.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:05:43] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:05:43] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:05:43,794]\u001b[0m Trial 22 finished with value: 0.26890756302521013 and parameters: {'n_estimators': 138, 'max_depth': 3, 'min_child_weight': 15, 'subsample': 0.6, 'colsample_bytree': 0.7}. Best is trial 20 with value: 0.26386554621848735.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:05:43] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:05:43] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:05:44,288]\u001b[0m Trial 23 finished with value: 0.28403361344537814 and parameters: {'n_estimators': 332, 'max_depth': 1, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 0.6}. Best is trial 20 with value: 0.26386554621848735.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:05:44] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:05:44] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:05:44,674]\u001b[0m Trial 24 finished with value: 0.2941176470588235 and parameters: {'n_estimators': 117, 'max_depth': 6, 'min_child_weight': 11, 'subsample': 0.6, 'colsample_bytree': 0.5}. Best is trial 20 with value: 0.26386554621848735.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:05:44] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:05:44] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:05:45,644]\u001b[0m Trial 25 finished with value: 0.2857142857142857 and parameters: {'n_estimators': 478, 'max_depth': 3, 'min_child_weight': 15, 'subsample': 0.6, 'colsample_bytree': 0.7}. Best is trial 20 with value: 0.26386554621848735.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:05:45] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:05:45] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:05:46,297]\u001b[0m Trial 26 finished with value: 0.2621848739495798 and parameters: {'n_estimators': 289, 'max_depth': 3, 'min_child_weight': 15, 'subsample': 0.6, 'colsample_bytree': 0.7}. Best is trial 26 with value: 0.2621848739495798.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:05:46] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:05:46] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:05:46,751]\u001b[0m Trial 27 finished with value: 0.27226890756302524 and parameters: {'n_estimators': 299, 'max_depth': 1, 'min_child_weight': 17, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 26 with value: 0.2621848739495798.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:05:46] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:05:46] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:05:48,093]\u001b[0m Trial 28 finished with value: 0.23697478991596643 and parameters: {'n_estimators': 583, 'max_depth': 4, 'min_child_weight': 14, 'subsample': 0.6, 'colsample_bytree': 0.9}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:05:48] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:05:48] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:05:50,072]\u001b[0m Trial 29 finished with value: 0.3243697478991596 and parameters: {'n_estimators': 571, 'max_depth': 10, 'min_child_weight': 14, 'subsample': 0.5, 'colsample_bytree': 0.8}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:05:50] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:05:50] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:05:51,741]\u001b[0m Trial 30 finished with value: 0.2504201680672269 and parameters: {'n_estimators': 684, 'max_depth': 5, 'min_child_weight': 20, 'subsample': 0.5, 'colsample_bytree': 0.7}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:05:51] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:05:51] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:05:53,462]\u001b[0m Trial 31 finished with value: 0.2857142857142857 and parameters: {'n_estimators': 711, 'max_depth': 5, 'min_child_weight': 20, 'subsample': 0.5, 'colsample_bytree': 0.7}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:05:53] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:05:53] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:05:54,942]\u001b[0m Trial 32 finished with value: 0.27563025210084036 and parameters: {'n_estimators': 667, 'max_depth': 4, 'min_child_weight': 18, 'subsample': 0.5, 'colsample_bytree': 0.7}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:05:54] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:05:54] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:05:55,998]\u001b[0m Trial 33 finished with value: 0.253781512605042 and parameters: {'n_estimators': 416, 'max_depth': 4, 'min_child_weight': 16, 'subsample': 0.6, 'colsample_bytree': 0.8}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:05:56] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:05:56] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:05:56,963]\u001b[0m Trial 34 finished with value: 0.2571428571428571 and parameters: {'n_estimators': 531, 'max_depth': 2, 'min_child_weight': 16, 'subsample': 0.5, 'colsample_bytree': 0.8}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:05:57] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:05:57] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:05:58,885]\u001b[0m Trial 35 finished with value: 0.29075630252100837 and parameters: {'n_estimators': 542, 'max_depth': 20, 'min_child_weight': 17, 'subsample': 0.5, 'colsample_bytree': 0.9}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:05:58] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:05:58] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:05:59,633]\u001b[0m Trial 36 finished with value: 0.26050420168067223 and parameters: {'n_estimators': 402, 'max_depth': 2, 'min_child_weight': 19, 'subsample': 0.5, 'colsample_bytree': 0.8}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:05:59] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:05:59] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:06:01,231]\u001b[0m Trial 37 finished with value: 0.30756302521008405 and parameters: {'n_estimators': 516, 'max_depth': 6, 'min_child_weight': 16, 'subsample': 0.5, 'colsample_bytree': 0.8}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:06:01] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:06:01] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:06:02,663]\u001b[0m Trial 38 finished with value: 0.2739495798319328 and parameters: {'n_estimators': 658, 'max_depth': 2, 'min_child_weight': 18, 'subsample': 0.5, 'colsample_bytree': 0.9}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:06:02] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:06:02] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:06:05,801]\u001b[0m Trial 39 finished with value: 0.29075630252100837 and parameters: {'n_estimators': 911, 'max_depth': 8, 'min_child_weight': 16, 'subsample': 0.7, 'colsample_bytree': 0.8}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:06:05] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:06:05] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:06:06,639]\u001b[0m Trial 40 finished with value: 0.3025210084033614 and parameters: {'n_estimators': 468, 'max_depth': 2, 'min_child_weight': 19, 'subsample': 0.5, 'colsample_bytree': 0.9}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:06:06] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:06:06] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:06:07,355]\u001b[0m Trial 41 finished with value: 0.2890756302521008 and parameters: {'n_estimators': 406, 'max_depth': 2, 'min_child_weight': 19, 'subsample': 0.5, 'colsample_bytree': 0.8}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:06:07] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:06:07] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:06:08,334]\u001b[0m Trial 42 finished with value: 0.3058823529411765 and parameters: {'n_estimators': 402, 'max_depth': 4, 'min_child_weight': 19, 'subsample': 0.5, 'colsample_bytree': 0.8}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:06:08] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:06:08] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:06:09,374]\u001b[0m Trial 43 finished with value: 0.27899159663865547 and parameters: {'n_estimators': 739, 'max_depth': 1, 'min_child_weight': 16, 'subsample': 0.6, 'colsample_bytree': 0.8}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:06:09] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:06:09] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:06:10,467]\u001b[0m Trial 44 finished with value: 0.31596638655462184 and parameters: {'n_estimators': 615, 'max_depth': 2, 'min_child_weight': 20, 'subsample': 0.5, 'colsample_bytree': 0.9}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:06:10] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:06:10] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:06:11,861]\u001b[0m Trial 45 finished with value: 0.2924369747899159 and parameters: {'n_estimators': 526, 'max_depth': 5, 'min_child_weight': 14, 'subsample': 0.5, 'colsample_bytree': 0.8}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:06:11] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:06:11] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:06:13,011]\u001b[0m Trial 46 finished with value: 0.28403361344537814 and parameters: {'n_estimators': 366, 'max_depth': 7, 'min_child_weight': 18, 'subsample': 0.6, 'colsample_bytree': 0.8}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:06:13] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:06:13] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:06:14,579]\u001b[0m Trial 47 finished with value: 0.26050420168067223 and parameters: {'n_estimators': 445, 'max_depth': 13, 'min_child_weight': 17, 'subsample': 0.5, 'colsample_bytree': 0.9}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:06:14] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:06:14] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:06:16,373]\u001b[0m Trial 48 finished with value: 0.29579831932773104 and parameters: {'n_estimators': 445, 'max_depth': 14, 'min_child_weight': 17, 'subsample': 0.7, 'colsample_bytree': 0.9}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:06:16] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:06:16] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:06:17,208]\u001b[0m Trial 49 finished with value: 0.25546218487394956 and parameters: {'n_estimators': 574, 'max_depth': 1, 'min_child_weight': 14, 'subsample': 0.6, 'colsample_bytree': 0.8}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:06:17] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:06:17] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:06:18,131]\u001b[0m Trial 50 finished with value: 0.29075630252100837 and parameters: {'n_estimators': 664, 'max_depth': 1, 'min_child_weight': 14, 'subsample': 0.7, 'colsample_bytree': 0.8}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:06:18] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:06:18] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:06:19,495]\u001b[0m Trial 51 finished with value: 0.28739495798319326 and parameters: {'n_estimators': 573, 'max_depth': 4, 'min_child_weight': 12, 'subsample': 0.6, 'colsample_bytree': 0.8}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:06:19] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:06:19] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:06:21,833]\u001b[0m Trial 52 finished with value: 0.29579831932773104 and parameters: {'n_estimators': 625, 'max_depth': 13, 'min_child_weight': 16, 'subsample': 0.6, 'colsample_bytree': 0.9}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:06:21] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:06:21] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:06:23,181]\u001b[0m Trial 53 finished with value: 0.27563025210084036 and parameters: {'n_estimators': 804, 'max_depth': 2, 'min_child_weight': 19, 'subsample': 0.5, 'colsample_bytree': 0.8}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:06:23] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:06:23] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:06:25,130]\u001b[0m Trial 54 finished with value: 0.2739495798319328 and parameters: {'n_estimators': 498, 'max_depth': 12, 'min_child_weight': 13, 'subsample': 0.5, 'colsample_bytree': 0.9}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:06:25] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:06:25] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:06:28,309]\u001b[0m Trial 55 finished with value: 0.3008403361344538 and parameters: {'n_estimators': 722, 'max_depth': 16, 'min_child_weight': 14, 'subsample': 0.6, 'colsample_bytree': 0.9}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:06:28] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:06:28] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:06:29,413]\u001b[0m Trial 56 finished with value: 0.2823529411764706 and parameters: {'n_estimators': 555, 'max_depth': 3, 'min_child_weight': 18, 'subsample': 0.6, 'colsample_bytree': 0.8}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:06:29] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:06:29] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:06:31,052]\u001b[0m Trial 57 finished with value: 0.2773109243697479 and parameters: {'n_estimators': 446, 'max_depth': 18, 'min_child_weight': 15, 'subsample': 0.5, 'colsample_bytree': 0.9}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:06:31] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:06:31] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:06:33,764]\u001b[0m Trial 58 finished with value: 0.3327731092436975 and parameters: {'n_estimators': 594, 'max_depth': 14, 'min_child_weight': 12, 'subsample': 0.6, 'colsample_bytree': 0.9}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:06:33] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:06:33] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:06:36,298]\u001b[0m Trial 59 finished with value: 0.3008403361344538 and parameters: {'n_estimators': 764, 'max_depth': 10, 'min_child_weight': 17, 'subsample': 0.6, 'colsample_bytree': 0.7}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:06:36] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:06:36] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:06:38,316]\u001b[0m Trial 60 finished with value: 0.2924369747899159 and parameters: {'n_estimators': 495, 'max_depth': 15, 'min_child_weight': 16, 'subsample': 0.7, 'colsample_bytree': 0.9}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:06:38] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:06:38] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:06:39,411]\u001b[0m Trial 61 finished with value: 0.28403361344537814 and parameters: {'n_estimators': 411, 'max_depth': 5, 'min_child_weight': 20, 'subsample': 0.5, 'colsample_bytree': 0.8}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:06:39] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:06:39] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:06:39,968]\u001b[0m Trial 62 finished with value: 0.28739495798319326 and parameters: {'n_estimators': 362, 'max_depth': 1, 'min_child_weight': 18, 'subsample': 0.5, 'colsample_bytree': 0.8}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:06:40] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:06:40] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:06:41,338]\u001b[0m Trial 63 finished with value: 0.2823529411764706 and parameters: {'n_estimators': 691, 'max_depth': 3, 'min_child_weight': 15, 'subsample': 0.5, 'colsample_bytree': 0.7}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:06:41] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:06:41] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:06:42,382]\u001b[0m Trial 64 finished with value: 0.30420168067226894 and parameters: {'n_estimators': 638, 'max_depth': 2, 'min_child_weight': 19, 'subsample': 0.8, 'colsample_bytree': 0.7}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:06:42] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:06:42] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:06:43,399]\u001b[0m Trial 65 finished with value: 0.2588235294117647 and parameters: {'n_estimators': 433, 'max_depth': 4, 'min_child_weight': 17, 'subsample': 0.5, 'colsample_bytree': 0.8}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:06:43] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:06:43] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:06:44,671]\u001b[0m Trial 66 finished with value: 0.3126050420168067 and parameters: {'n_estimators': 535, 'max_depth': 4, 'min_child_weight': 4, 'subsample': 0.5, 'colsample_bytree': 0.8}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:06:44] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:06:44] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:06:46,115]\u001b[0m Trial 67 finished with value: 0.2773109243697479 and parameters: {'n_estimators': 475, 'max_depth': 6, 'min_child_weight': 17, 'subsample': 0.6, 'colsample_bytree': 0.9}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:06:46] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:06:46] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:06:48,711]\u001b[0m Trial 68 finished with value: 0.3243697478991596 and parameters: {'n_estimators': 848, 'max_depth': 8, 'min_child_weight': 13, 'subsample': 0.5, 'colsample_bytree': 0.7}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:06:48] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:06:48] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:06:49,603]\u001b[0m Trial 69 finished with value: 0.29915966386554627 and parameters: {'n_estimators': 324, 'max_depth': 5, 'min_child_weight': 15, 'subsample': 0.6, 'colsample_bytree': 0.8}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:06:49] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:06:49] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:06:51,526]\u001b[0m Trial 70 finished with value: 0.30756302521008405 and parameters: {'n_estimators': 593, 'max_depth': 7, 'min_child_weight': 14, 'subsample': 0.5, 'colsample_bytree': 0.9}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:06:51] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:06:51] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:06:52,440]\u001b[0m Trial 71 finished with value: 0.2655462184873949 and parameters: {'n_estimators': 438, 'max_depth': 3, 'min_child_weight': 17, 'subsample': 0.5, 'colsample_bytree': 0.8}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:06:52] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:06:52] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:06:53,125]\u001b[0m Trial 72 finished with value: 0.238655462184874 and parameters: {'n_estimators': 372, 'max_depth': 2, 'min_child_weight': 16, 'subsample': 0.5, 'colsample_bytree': 0.8}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:06:53] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:06:53] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:06:54,210]\u001b[0m Trial 73 finished with value: 0.28739495798319326 and parameters: {'n_estimators': 463, 'max_depth': 4, 'min_child_weight': 16, 'subsample': 0.5, 'colsample_bytree': 0.8}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:06:54] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:06:54] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:06:54,604]\u001b[0m Trial 74 finished with value: 0.2823529411764706 and parameters: {'n_estimators': 240, 'max_depth': 1, 'min_child_weight': 16, 'subsample': 0.5, 'colsample_bytree': 0.8}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:06:54] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:06:54] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:06:55,870]\u001b[0m Trial 75 finished with value: 0.28739495798319326 and parameters: {'n_estimators': 369, 'max_depth': 11, 'min_child_weight': 15, 'subsample': 0.6, 'colsample_bytree': 0.7}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:06:55] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:06:55] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:06:56,610]\u001b[0m Trial 76 finished with value: 0.2823529411764706 and parameters: {'n_estimators': 412, 'max_depth': 2, 'min_child_weight': 18, 'subsample': 0.5, 'colsample_bytree': 0.8}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:06:56] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:06:56] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:06:58,040]\u001b[0m Trial 77 finished with value: 0.26890756302521013 and parameters: {'n_estimators': 518, 'max_depth': 6, 'min_child_weight': 17, 'subsample': 0.5, 'colsample_bytree': 0.7}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:06:58] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:06:58] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:07:00,018]\u001b[0m Trial 78 finished with value: 0.2890756302521008 and parameters: {'n_estimators': 557, 'max_depth': 17, 'min_child_weight': 15, 'subsample': 0.5, 'colsample_bytree': 0.9}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:00] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:00] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:07:00,706]\u001b[0m Trial 79 finished with value: 0.280672268907563 and parameters: {'n_estimators': 318, 'max_depth': 3, 'min_child_weight': 11, 'subsample': 0.6, 'colsample_bytree': 0.8}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:00] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:00] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:07:01,401]\u001b[0m Trial 80 finished with value: 0.28403361344537814 and parameters: {'n_estimators': 190, 'max_depth': 9, 'min_child_weight': 16, 'subsample': 0.5, 'colsample_bytree': 0.8}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:01] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:01] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:07:02,088]\u001b[0m Trial 81 finished with value: 0.2655462184873949 and parameters: {'n_estimators': 384, 'max_depth': 2, 'min_child_weight': 19, 'subsample': 0.5, 'colsample_bytree': 0.8}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:02] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:02] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:07:02,961]\u001b[0m Trial 82 finished with value: 0.24705882352941178 and parameters: {'n_estimators': 345, 'max_depth': 4, 'min_child_weight': 20, 'subsample': 0.5, 'colsample_bytree': 0.8}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:03] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:03] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:07:03,841]\u001b[0m Trial 83 finished with value: 0.27226890756302524 and parameters: {'n_estimators': 351, 'max_depth': 4, 'min_child_weight': 20, 'subsample': 0.5, 'colsample_bytree': 0.8}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:03] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:03] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:07:04,969]\u001b[0m Trial 84 finished with value: 0.31092436974789917 and parameters: {'n_estimators': 424, 'max_depth': 5, 'min_child_weight': 18, 'subsample': 0.5, 'colsample_bytree': 0.8}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:05] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:05] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:07:05,675]\u001b[0m Trial 85 finished with value: 0.24873949579831933 and parameters: {'n_estimators': 273, 'max_depth': 4, 'min_child_weight': 20, 'subsample': 0.7, 'colsample_bytree': 0.8}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:05] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:05] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:07:06,434]\u001b[0m Trial 86 finished with value: 0.2621848739495798 and parameters: {'n_estimators': 268, 'max_depth': 5, 'min_child_weight': 20, 'subsample': 0.7, 'colsample_bytree': 0.8}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:06] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:06] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:07:06,993]\u001b[0m Trial 87 finished with value: 0.253781512605042 and parameters: {'n_estimators': 228, 'max_depth': 4, 'min_child_weight': 20, 'subsample': 0.8, 'colsample_bytree': 0.7}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:07] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:07] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:07:07,650]\u001b[0m Trial 88 finished with value: 0.2504201680672269 and parameters: {'n_estimators': 312, 'max_depth': 3, 'min_child_weight': 20, 'subsample': 0.8, 'colsample_bytree': 0.7}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:07] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:07] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:07:08,095]\u001b[0m Trial 89 finished with value: 0.26050420168067223 and parameters: {'n_estimators': 203, 'max_depth': 3, 'min_child_weight': 20, 'subsample': 0.8, 'colsample_bytree': 0.7}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:08] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:08] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:07:08,605]\u001b[0m Trial 90 finished with value: 0.2890756302521008 and parameters: {'n_estimators': 242, 'max_depth': 3, 'min_child_weight': 20, 'subsample': 0.8, 'colsample_bytree': 0.6}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:08] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:08] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:07:09,314]\u001b[0m Trial 91 finished with value: 0.2705882352941177 and parameters: {'n_estimators': 300, 'max_depth': 4, 'min_child_weight': 19, 'subsample': 0.8, 'colsample_bytree': 0.7}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:09] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:09] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:07:09,740]\u001b[0m Trial 92 finished with value: 0.27563025210084036 and parameters: {'n_estimators': 276, 'max_depth': 1, 'min_child_weight': 20, 'subsample': 0.8, 'colsample_bytree': 0.7}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:09] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:09] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:07:10,162]\u001b[0m Trial 93 finished with value: 0.2773109243697479 and parameters: {'n_estimators': 214, 'max_depth': 2, 'min_child_weight': 19, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:10] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:10] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:07:10,711]\u001b[0m Trial 94 finished with value: 0.27563025210084036 and parameters: {'n_estimators': 157, 'max_depth': 5, 'min_child_weight': 10, 'subsample': 0.8, 'colsample_bytree': 0.7}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:10] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:10] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:07:11,430]\u001b[0m Trial 95 finished with value: 0.25546218487394956 and parameters: {'n_estimators': 311, 'max_depth': 3, 'min_child_weight': 18, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:11] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:11] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:07:12,408]\u001b[0m Trial 96 finished with value: 0.26890756302521013 and parameters: {'n_estimators': 386, 'max_depth': 3, 'min_child_weight': 18, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:12] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:12] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:07:13,729]\u001b[0m Trial 97 finished with value: 0.2773109243697479 and parameters: {'n_estimators': 309, 'max_depth': 6, 'min_child_weight': 19, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:13] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:13] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:07:14,814]\u001b[0m Trial 98 finished with value: 0.24873949579831933 and parameters: {'n_estimators': 339, 'max_depth': 4, 'min_child_weight': 20, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:14] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:14] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:07:15,744]\u001b[0m Trial 99 finished with value: 0.2705882352941177 and parameters: {'n_estimators': 338, 'max_depth': 4, 'min_child_weight': 20, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:15] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:15] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:07:16,615]\u001b[0m Trial 100 finished with value: 0.26722689075630257 and parameters: {'n_estimators': 278, 'max_depth': 6, 'min_child_weight': 20, 'subsample': 0.7, 'colsample_bytree': 0.6}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:16] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:16] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:07:17,362]\u001b[0m Trial 101 finished with value: 0.24369747899159666 and parameters: {'n_estimators': 245, 'max_depth': 3, 'min_child_weight': 19, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:17] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:17] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:07:18,081]\u001b[0m Trial 102 finished with value: 0.25210084033613445 and parameters: {'n_estimators': 239, 'max_depth': 4, 'min_child_weight': 19, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:18] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:18] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:07:19,041]\u001b[0m Trial 103 finished with value: 0.2705882352941177 and parameters: {'n_estimators': 256, 'max_depth': 5, 'min_child_weight': 19, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:19] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:19] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:07:19,418]\u001b[0m Trial 104 finished with value: 0.24705882352941178 and parameters: {'n_estimators': 78, 'max_depth': 4, 'min_child_weight': 20, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:19] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:19] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:07:20,547]\u001b[0m Trial 105 finished with value: 0.2504201680672269 and parameters: {'n_estimators': 347, 'max_depth': 4, 'min_child_weight': 19, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-14 03:07:20,665]\u001b[0m Trial 106 finished with value: 0.2823529411764706 and parameters: {'n_estimators': 8, 'max_depth': 5, 'min_child_weight': 19, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:20] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:20] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "[03:07:20] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:20] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:07:21,475]\u001b[0m Trial 107 finished with value: 0.3126050420168067 and parameters: {'n_estimators': 286, 'max_depth': 3, 'min_child_weight': 20, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:21] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:21] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:07:21,765]\u001b[0m Trial 108 finished with value: 0.26722689075630257 and parameters: {'n_estimators': 83, 'max_depth': 4, 'min_child_weight': 19, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:21] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:21] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:07:22,125]\u001b[0m Trial 109 finished with value: 0.2504201680672269 and parameters: {'n_estimators': 117, 'max_depth': 3, 'min_child_weight': 20, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:22] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:22] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:07:22,444]\u001b[0m Trial 110 finished with value: 0.26722689075630257 and parameters: {'n_estimators': 56, 'max_depth': 3, 'min_child_weight': 20, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 28 with value: 0.23697478991596643.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:22] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:22] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:07:22,890]\u001b[0m Trial 111 finished with value: 0.23529411764705888 and parameters: {'n_estimators': 105, 'max_depth': 4, 'min_child_weight': 19, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 111 with value: 0.23529411764705888.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:22] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:22] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:07:23,608]\u001b[0m Trial 112 finished with value: 0.2739495798319328 and parameters: {'n_estimators': 129, 'max_depth': 5, 'min_child_weight': 18, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 111 with value: 0.23529411764705888.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-14 03:07:23,870]\u001b[0m Trial 113 finished with value: 0.2823529411764706 and parameters: {'n_estimators': 86, 'max_depth': 2, 'min_child_weight': 20, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 111 with value: 0.23529411764705888.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:23] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:23] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:23] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:23] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:07:24,277]\u001b[0m Trial 114 finished with value: 0.280672268907563 and parameters: {'n_estimators': 161, 'max_depth': 3, 'min_child_weight': 19, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 111 with value: 0.23529411764705888.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:24] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:24] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:07:24,666]\u001b[0m Trial 115 finished with value: 0.2739495798319328 and parameters: {'n_estimators': 108, 'max_depth': 4, 'min_child_weight': 20, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 111 with value: 0.23529411764705888.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-14 03:07:24,872]\u001b[0m Trial 116 finished with value: 0.2890756302521008 and parameters: {'n_estimators': 28, 'max_depth': 4, 'min_child_weight': 20, 'subsample': 0.7, 'colsample_bytree': 0.6}. Best is trial 111 with value: 0.23529411764705888.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:24] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:24] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:24] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:24] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:07:25,501]\u001b[0m Trial 117 finished with value: 0.26386554621848735 and parameters: {'n_estimators': 184, 'max_depth': 3, 'min_child_weight': 19, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 111 with value: 0.23529411764705888.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:25] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:25] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:07:25,785]\u001b[0m Trial 118 finished with value: 0.25546218487394956 and parameters: {'n_estimators': 56, 'max_depth': 5, 'min_child_weight': 18, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 111 with value: 0.23529411764705888.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:25] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:25] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:07:26,435]\u001b[0m Trial 119 finished with value: 0.26890756302521013 and parameters: {'n_estimators': 340, 'max_depth': 2, 'min_child_weight': 19, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 111 with value: 0.23529411764705888.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:26] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:26] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:07:26,988]\u001b[0m Trial 120 finished with value: 0.2705882352941177 and parameters: {'n_estimators': 163, 'max_depth': 6, 'min_child_weight': 20, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 111 with value: 0.23529411764705888.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:27] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:27] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:07:27,854]\u001b[0m Trial 121 finished with value: 0.2588235294117647 and parameters: {'n_estimators': 360, 'max_depth': 4, 'min_child_weight': 19, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 111 with value: 0.23529411764705888.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:27] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:27] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:07:28,235]\u001b[0m Trial 122 finished with value: 0.24705882352941178 and parameters: {'n_estimators': 132, 'max_depth': 4, 'min_child_weight': 19, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 111 with value: 0.23529411764705888.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:28] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:28] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:07:28,634]\u001b[0m Trial 123 finished with value: 0.2924369747899159 and parameters: {'n_estimators': 127, 'max_depth': 4, 'min_child_weight': 20, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 111 with value: 0.23529411764705888.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-14 03:07:28,847]\u001b[0m Trial 124 finished with value: 0.26386554621848735 and parameters: {'n_estimators': 46, 'max_depth': 5, 'min_child_weight': 20, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 111 with value: 0.23529411764705888.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:28] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:28] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:28] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:28] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:07:29,617]\u001b[0m Trial 125 finished with value: 0.24033613445378155 and parameters: {'n_estimators': 330, 'max_depth': 3, 'min_child_weight': 6, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 111 with value: 0.23529411764705888.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:29] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:29] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:07:30,316]\u001b[0m Trial 126 finished with value: 0.28403361344537814 and parameters: {'n_estimators': 330, 'max_depth': 2, 'min_child_weight': 2, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 111 with value: 0.23529411764705888.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:30] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:30] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:07:31,084]\u001b[0m Trial 127 finished with value: 0.2571428571428571 and parameters: {'n_estimators': 199, 'max_depth': 3, 'min_child_weight': 6, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 111 with value: 0.23529411764705888.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-14 03:07:31,177]\u001b[0m Trial 128 finished with value: 0.4756302521008403 and parameters: {'n_estimators': 0, 'max_depth': 3, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 111 with value: 0.23529411764705888.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:31] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:31] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:31] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:07:32,003]\u001b[0m Trial 129 finished with value: 0.2705882352941177 and parameters: {'n_estimators': 78, 'max_depth': 4, 'min_child_weight': 18, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 111 with value: 0.23529411764705888.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:32] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:32] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:07:34,632]\u001b[0m Trial 130 finished with value: 0.30420168067226894 and parameters: {'n_estimators': 992, 'max_depth': 3, 'min_child_weight': 6, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 111 with value: 0.23529411764705888.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:34] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:34] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:07:35,751]\u001b[0m Trial 131 finished with value: 0.2504201680672269 and parameters: {'n_estimators': 375, 'max_depth': 4, 'min_child_weight': 19, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 111 with value: 0.23529411764705888.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:35] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:35] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:07:37,263]\u001b[0m Trial 132 finished with value: 0.27563025210084036 and parameters: {'n_estimators': 389, 'max_depth': 4, 'min_child_weight': 19, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 111 with value: 0.23529411764705888.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:37] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:37] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:07:38,466]\u001b[0m Trial 133 finished with value: 0.2924369747899159 and parameters: {'n_estimators': 290, 'max_depth': 5, 'min_child_weight': 3, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 111 with value: 0.23529411764705888.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:38] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:38] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:07:39,768]\u001b[0m Trial 134 finished with value: 0.28403361344537814 and parameters: {'n_estimators': 367, 'max_depth': 4, 'min_child_weight': 7, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 111 with value: 0.23529411764705888.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:39] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:39] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:07:40,094]\u001b[0m Trial 135 finished with value: 0.26722689075630257 and parameters: {'n_estimators': 98, 'max_depth': 3, 'min_child_weight': 20, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 111 with value: 0.23529411764705888.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:40] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:40] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:07:40,415]\u001b[0m Trial 136 finished with value: 0.2823529411764706 and parameters: {'n_estimators': 145, 'max_depth': 2, 'min_child_weight': 18, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 111 with value: 0.23529411764705888.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:40] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:40] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:07:41,030]\u001b[0m Trial 137 finished with value: 0.2588235294117647 and parameters: {'n_estimators': 321, 'max_depth': 2, 'min_child_weight': 19, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 111 with value: 0.23529411764705888.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:41] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:41] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:07:41,758]\u001b[0m Trial 138 finished with value: 0.2823529411764706 and parameters: {'n_estimators': 346, 'max_depth': 3, 'min_child_weight': 19, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 111 with value: 0.23529411764705888.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:41] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:41] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:07:42,822]\u001b[0m Trial 139 finished with value: 0.2739495798319328 and parameters: {'n_estimators': 382, 'max_depth': 5, 'min_child_weight': 18, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 111 with value: 0.23529411764705888.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:42] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:42] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:07:43,395]\u001b[0m Trial 140 finished with value: 0.27563025210084036 and parameters: {'n_estimators': 260, 'max_depth': 3, 'min_child_weight': 20, 'subsample': 0.8, 'colsample_bytree': 0.7}. Best is trial 111 with value: 0.23529411764705888.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:43] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:43] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:07:44,186]\u001b[0m Trial 141 finished with value: 0.22016806722689075 and parameters: {'n_estimators': 308, 'max_depth': 4, 'min_child_weight': 20, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:44] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:44] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:07:44,966]\u001b[0m Trial 142 finished with value: 0.2823529411764706 and parameters: {'n_estimators': 303, 'max_depth': 4, 'min_child_weight': 19, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:45] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:45] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:07:45,811]\u001b[0m Trial 143 finished with value: 0.2773109243697479 and parameters: {'n_estimators': 324, 'max_depth': 4, 'min_child_weight': 20, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:45] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:45] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:07:47,537]\u001b[0m Trial 144 finished with value: 0.30420168067226894 and parameters: {'n_estimators': 678, 'max_depth': 5, 'min_child_weight': 19, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:47] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:47] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:07:48,442]\u001b[0m Trial 145 finished with value: 0.26386554621848735 and parameters: {'n_estimators': 341, 'max_depth': 4, 'min_child_weight': 5, 'subsample': 0.7, 'colsample_bytree': 0.5}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:48] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:48] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:07:48,909]\u001b[0m Trial 146 finished with value: 0.26050420168067223 and parameters: {'n_estimators': 122, 'max_depth': 5, 'min_child_weight': 20, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:48] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:48] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:07:50,313]\u001b[0m Trial 147 finished with value: 0.2823529411764706 and parameters: {'n_estimators': 648, 'max_depth': 3, 'min_child_weight': 20, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:50] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:50] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:07:50,862]\u001b[0m Trial 148 finished with value: 0.23697478991596643 and parameters: {'n_estimators': 215, 'max_depth': 3, 'min_child_weight': 20, 'subsample': 0.7, 'colsample_bytree': 0.6}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:50] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:50] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:07:51,606]\u001b[0m Trial 149 finished with value: 0.2857142857142857 and parameters: {'n_estimators': 224, 'max_depth': 7, 'min_child_weight': 19, 'subsample': 0.7, 'colsample_bytree': 0.6}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:51] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:51] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:07:52,277]\u001b[0m Trial 150 finished with value: 0.2655462184873949 and parameters: {'n_estimators': 171, 'max_depth': 6, 'min_child_weight': 20, 'subsample': 0.7, 'colsample_bytree': 0.5}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:52] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:52] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:07:53,060]\u001b[0m Trial 151 finished with value: 0.27226890756302524 and parameters: {'n_estimators': 266, 'max_depth': 4, 'min_child_weight': 19, 'subsample': 0.7, 'colsample_bytree': 0.6}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-14 03:07:53,323]\u001b[0m Trial 152 finished with value: 0.25210084033613445 and parameters: {'n_estimators': 76, 'max_depth': 4, 'min_child_weight': 20, 'subsample': 0.7, 'colsample_bytree': 0.6}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:53] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:53] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:53] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:53] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:07:53,965]\u001b[0m Trial 153 finished with value: 0.2504201680672269 and parameters: {'n_estimators': 293, 'max_depth': 3, 'min_child_weight': 19, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:54] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:54] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:07:55,468]\u001b[0m Trial 154 finished with value: 0.2739495798319328 and parameters: {'n_estimators': 751, 'max_depth': 3, 'min_child_weight': 12, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:55] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:55] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:07:56,158]\u001b[0m Trial 155 finished with value: 0.24705882352941178 and parameters: {'n_estimators': 283, 'max_depth': 4, 'min_child_weight': 18, 'subsample': 0.7, 'colsample_bytree': 0.6}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:56] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:56] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:07:56,701]\u001b[0m Trial 156 finished with value: 0.22521008403361342 and parameters: {'n_estimators': 268, 'max_depth': 2, 'min_child_weight': 20, 'subsample': 0.7, 'colsample_bytree': 0.6}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:56] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:56] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:07:57,137]\u001b[0m Trial 157 finished with value: 0.2621848739495798 and parameters: {'n_estimators': 219, 'max_depth': 2, 'min_child_weight': 18, 'subsample': 0.7, 'colsample_bytree': 0.6}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:57] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:57] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:07:57,682]\u001b[0m Trial 158 finished with value: 0.29579831932773104 and parameters: {'n_estimators': 285, 'max_depth': 2, 'min_child_weight': 20, 'subsample': 0.7, 'colsample_bytree': 0.6}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:57] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:57] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:07:58,082]\u001b[0m Trial 159 finished with value: 0.24537815126050422 and parameters: {'n_estimators': 251, 'max_depth': 1, 'min_child_weight': 19, 'subsample': 0.7, 'colsample_bytree': 0.6}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:58] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:58] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:07:58,479]\u001b[0m Trial 160 finished with value: 0.2504201680672269 and parameters: {'n_estimators': 252, 'max_depth': 1, 'min_child_weight': 17, 'subsample': 0.7, 'colsample_bytree': 0.6}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:58] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:58] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:07:58,863]\u001b[0m Trial 161 finished with value: 0.2621848739495798 and parameters: {'n_estimators': 243, 'max_depth': 1, 'min_child_weight': 19, 'subsample': 0.7, 'colsample_bytree': 0.6}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:58] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:58] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:07:59,296]\u001b[0m Trial 162 finished with value: 0.2974789915966387 and parameters: {'n_estimators': 272, 'max_depth': 1, 'min_child_weight': 20, 'subsample': 0.7, 'colsample_bytree': 0.6}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:59] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:59] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:07:59,694]\u001b[0m Trial 163 finished with value: 0.29579831932773104 and parameters: {'n_estimators': 245, 'max_depth': 1, 'min_child_weight': 17, 'subsample': 0.7, 'colsample_bytree': 0.6}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:07:59] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:07:59] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:08:00,241]\u001b[0m Trial 164 finished with value: 0.2739495798319328 and parameters: {'n_estimators': 298, 'max_depth': 2, 'min_child_weight': 19, 'subsample': 0.7, 'colsample_bytree': 0.6}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:08:00] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:00] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:08:00,874]\u001b[0m Trial 165 finished with value: 0.253781512605042 and parameters: {'n_estimators': 307, 'max_depth': 3, 'min_child_weight': 20, 'subsample': 0.7, 'colsample_bytree': 0.6}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:08:00] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:00] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:08:01,233]\u001b[0m Trial 166 finished with value: 0.2588235294117647 and parameters: {'n_estimators': 186, 'max_depth': 2, 'min_child_weight': 20, 'subsample': 0.7, 'colsample_bytree': 0.6}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:08:01] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:01] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:08:01,643]\u001b[0m Trial 167 finished with value: 0.2739495798319328 and parameters: {'n_estimators': 260, 'max_depth': 1, 'min_child_weight': 18, 'subsample': 0.7, 'colsample_bytree': 0.6}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:08:01] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:01] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:08:02,002]\u001b[0m Trial 168 finished with value: 0.29075630252100837 and parameters: {'n_estimators': 226, 'max_depth': 1, 'min_child_weight': 18, 'subsample': 0.7, 'colsample_bytree': 0.6}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:08:02] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:02] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:08:02,540]\u001b[0m Trial 169 finished with value: 0.23529411764705888 and parameters: {'n_estimators': 201, 'max_depth': 4, 'min_child_weight': 19, 'subsample': 0.7, 'colsample_bytree': 0.6}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:08:02] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:02] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:08:02,826]\u001b[0m Trial 170 finished with value: 0.2890756302521008 and parameters: {'n_estimators': 98, 'max_depth': 4, 'min_child_weight': 20, 'subsample': 0.7, 'colsample_bytree': 0.6}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:08:02] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:02] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:08:03,260]\u001b[0m Trial 171 finished with value: 0.28739495798319326 and parameters: {'n_estimators': 283, 'max_depth': 1, 'min_child_weight': 17, 'subsample': 0.7, 'colsample_bytree': 0.6}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:08:03] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:03] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:08:03,706]\u001b[0m Trial 172 finished with value: 0.2705882352941177 and parameters: {'n_estimators': 202, 'max_depth': 3, 'min_child_weight': 20, 'subsample': 0.7, 'colsample_bytree': 0.5}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:08:03] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:03] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:08:04,094]\u001b[0m Trial 173 finished with value: 0.27563025210084036 and parameters: {'n_estimators': 146, 'max_depth': 4, 'min_child_weight': 19, 'subsample': 0.7, 'colsample_bytree': 0.6}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:08:04] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:04] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:08:05,033]\u001b[0m Trial 174 finished with value: 0.2857142857142857 and parameters: {'n_estimators': 323, 'max_depth': 5, 'min_child_weight': 19, 'subsample': 0.7, 'colsample_bytree': 0.6}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:08:05] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:05] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:08:05,640]\u001b[0m Trial 175 finished with value: 0.2924369747899159 and parameters: {'n_estimators': 213, 'max_depth': 5, 'min_child_weight': 19, 'subsample': 0.7, 'colsample_bytree': 0.6}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:08:05] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:05] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:08:06,315]\u001b[0m Trial 176 finished with value: 0.23529411764705888 and parameters: {'n_estimators': 273, 'max_depth': 4, 'min_child_weight': 19, 'subsample': 0.7, 'colsample_bytree': 0.6}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:08:06] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:06] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:08:07,054]\u001b[0m Trial 177 finished with value: 0.28739495798319326 and parameters: {'n_estimators': 270, 'max_depth': 4, 'min_child_weight': 18, 'subsample': 0.7, 'colsample_bytree': 0.6}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:08:07] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:07] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:08:07,765]\u001b[0m Trial 178 finished with value: 0.24873949579831933 and parameters: {'n_estimators': 247, 'max_depth': 4, 'min_child_weight': 10, 'subsample': 0.7, 'colsample_bytree': 0.6}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:08:07] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:07] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:08:08,496]\u001b[0m Trial 179 finished with value: 0.26386554621848735 and parameters: {'n_estimators': 232, 'max_depth': 4, 'min_child_weight': 5, 'subsample': 0.7, 'colsample_bytree': 0.6}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:08:08] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:08] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:08:08,938]\u001b[0m Trial 180 finished with value: 0.2705882352941177 and parameters: {'n_estimators': 183, 'max_depth': 3, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 0.6}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:08:09] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:09] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:08:09,430]\u001b[0m Trial 181 finished with value: 0.2588235294117647 and parameters: {'n_estimators': 250, 'max_depth': 2, 'min_child_weight': 9, 'subsample': 0.7, 'colsample_bytree': 0.6}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:08:09] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:09] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:08:10,001]\u001b[0m Trial 182 finished with value: 0.31596638655462184 and parameters: {'n_estimators': 206, 'max_depth': 4, 'min_child_weight': 13, 'subsample': 0.7, 'colsample_bytree': 0.6}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:08:10] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:10] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:08:10,649]\u001b[0m Trial 183 finished with value: 0.2941176470588235 and parameters: {'n_estimators': 297, 'max_depth': 3, 'min_child_weight': 19, 'subsample': 0.7, 'colsample_bytree': 0.6}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:08:10] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:10] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:08:11,432]\u001b[0m Trial 184 finished with value: 0.27226890756302524 and parameters: {'n_estimators': 266, 'max_depth': 5, 'min_child_weight': 20, 'subsample': 0.5, 'colsample_bytree': 0.6}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:08:11] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:11] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:08:12,207]\u001b[0m Trial 185 finished with value: 0.2504201680672269 and parameters: {'n_estimators': 280, 'max_depth': 4, 'min_child_weight': 11, 'subsample': 0.7, 'colsample_bytree': 0.6}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:08:12] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:12] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:08:13,299]\u001b[0m Trial 186 finished with value: 0.2924369747899159 and parameters: {'n_estimators': 348, 'max_depth': 4, 'min_child_weight': 10, 'subsample': 0.7, 'colsample_bytree': 0.6}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:08:13] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:13] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:08:14,350]\u001b[0m Trial 187 finished with value: 0.27899159663865547 and parameters: {'n_estimators': 295, 'max_depth': 20, 'min_child_weight': 19, 'subsample': 0.7, 'colsample_bytree': 0.6}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:08:14] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:14] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:08:15,267]\u001b[0m Trial 188 finished with value: 0.2773109243697479 and parameters: {'n_estimators': 374, 'max_depth': 4, 'min_child_weight': 19, 'subsample': 0.7, 'colsample_bytree': 0.8}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:08:15] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:15] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:08:16,013]\u001b[0m Trial 189 finished with value: 0.2571428571428571 and parameters: {'n_estimators': 318, 'max_depth': 3, 'min_child_weight': 20, 'subsample': 0.6, 'colsample_bytree': 0.6}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:08:16] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:16] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:08:16,782]\u001b[0m Trial 190 finished with value: 0.27899159663865547 and parameters: {'n_estimators': 228, 'max_depth': 5, 'min_child_weight': 20, 'subsample': 0.5, 'colsample_bytree': 0.9}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:08:16] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:16] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:08:17,436]\u001b[0m Trial 191 finished with value: 0.3008403361344538 and parameters: {'n_estimators': 237, 'max_depth': 4, 'min_child_weight': 18, 'subsample': 0.7, 'colsample_bytree': 0.6}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:08:17] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:17] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:08:18,200]\u001b[0m Trial 192 finished with value: 0.28403361344537814 and parameters: {'n_estimators': 312, 'max_depth': 3, 'min_child_weight': 20, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:08:18] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:18] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:08:19,076]\u001b[0m Trial 193 finished with value: 0.27226890756302524 and parameters: {'n_estimators': 331, 'max_depth': 4, 'min_child_weight': 20, 'subsample': 0.8, 'colsample_bytree': 0.7}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:08:19] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:19] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:08:19,862]\u001b[0m Trial 194 finished with value: 0.2857142857142857 and parameters: {'n_estimators': 338, 'max_depth': 3, 'min_child_weight': 19, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:08:19] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:19] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:08:20,945]\u001b[0m Trial 195 finished with value: 0.27226890756302524 and parameters: {'n_estimators': 359, 'max_depth': 5, 'min_child_weight': 7, 'subsample': 0.7, 'colsample_bytree': 0.8}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:08:21] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:21] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:08:21,372]\u001b[0m Trial 196 finished with value: 0.2823529411764706 and parameters: {'n_estimators': 254, 'max_depth': 1, 'min_child_weight': 17, 'subsample': 0.7, 'colsample_bytree': 0.6}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:08:21] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:21] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:08:24,189]\u001b[0m Trial 197 finished with value: 0.3092436974789916 and parameters: {'n_estimators': 864, 'max_depth': 9, 'min_child_weight': 18, 'subsample': 0.7, 'colsample_bytree': 0.6}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:08:24] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:24] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:08:24,861]\u001b[0m Trial 198 finished with value: 0.2621848739495798 and parameters: {'n_estimators': 288, 'max_depth': 3, 'min_child_weight': 19, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:08:24] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:24] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:08:25,289]\u001b[0m Trial 199 finished with value: 0.2705882352941177 and parameters: {'n_estimators': 148, 'max_depth': 2, 'min_child_weight': 20, 'subsample': 0.7, 'colsample_bytree': 0.6}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:08:25] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:25] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:08:25,587]\u001b[0m Trial 200 finished with value: 0.26722689075630257 and parameters: {'n_estimators': 69, 'max_depth': 4, 'min_child_weight': 19, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:08:25] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:25] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:08:27,511]\u001b[0m Trial 201 finished with value: 0.2773109243697479 and parameters: {'n_estimators': 707, 'max_depth': 4, 'min_child_weight': 19, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:08:27] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:27] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:08:28,033]\u001b[0m Trial 202 finished with value: 0.29075630252100837 and parameters: {'n_estimators': 255, 'max_depth': 2, 'min_child_weight': 16, 'subsample': 0.7, 'colsample_bytree': 0.6}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:08:28] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:28] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:08:28,407]\u001b[0m Trial 203 finished with value: 0.26890756302521013 and parameters: {'n_estimators': 116, 'max_depth': 3, 'min_child_weight': 20, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:08:28] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:28] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:08:29,354]\u001b[0m Trial 204 finished with value: 0.3058823529411765 and parameters: {'n_estimators': 273, 'max_depth': 4, 'min_child_weight': 12, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:08:29] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:29] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:08:29,879]\u001b[0m Trial 205 finished with value: 0.2420168067226891 and parameters: {'n_estimators': 173, 'max_depth': 4, 'min_child_weight': 18, 'subsample': 0.7, 'colsample_bytree': 0.6}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:08:29] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:29] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:08:30,361]\u001b[0m Trial 206 finished with value: 0.280672268907563 and parameters: {'n_estimators': 179, 'max_depth': 4, 'min_child_weight': 18, 'subsample': 0.7, 'colsample_bytree': 0.6}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:08:30] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:30] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:08:31,079]\u001b[0m Trial 207 finished with value: 0.26722689075630257 and parameters: {'n_estimators': 221, 'max_depth': 5, 'min_child_weight': 11, 'subsample': 0.7, 'colsample_bytree': 0.6}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:08:31] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:31] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:08:32,655]\u001b[0m Trial 208 finished with value: 0.2823529411764706 and parameters: {'n_estimators': 619, 'max_depth': 5, 'min_child_weight': 19, 'subsample': 0.5, 'colsample_bytree': 0.6}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:08:32] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:32] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:08:33,138]\u001b[0m Trial 209 finished with value: 0.2705882352941177 and parameters: {'n_estimators': 199, 'max_depth': 3, 'min_child_weight': 20, 'subsample': 0.7, 'colsample_bytree': 0.8}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:08:33] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:33] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:08:33,615]\u001b[0m Trial 210 finished with value: 0.29075630252100837 and parameters: {'n_estimators': 163, 'max_depth': 4, 'min_child_weight': 13, 'subsample': 0.7, 'colsample_bytree': 0.6}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:08:33] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:33] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:08:34,468]\u001b[0m Trial 211 finished with value: 0.2588235294117647 and parameters: {'n_estimators': 308, 'max_depth': 4, 'min_child_weight': 20, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:08:34] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:34] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:08:34,843]\u001b[0m Trial 212 finished with value: 0.29075630252100837 and parameters: {'n_estimators': 132, 'max_depth': 3, 'min_child_weight': 20, 'subsample': 0.9, 'colsample_bytree': 0.7}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:08:34] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:34] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:08:35,588]\u001b[0m Trial 213 finished with value: 0.26722689075630257 and parameters: {'n_estimators': 243, 'max_depth': 4, 'min_child_weight': 18, 'subsample': 0.7, 'colsample_bytree': 0.6}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:08:35] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:35] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:08:36,271]\u001b[0m Trial 214 finished with value: 0.26722689075630257 and parameters: {'n_estimators': 251, 'max_depth': 2, 'min_child_weight': 17, 'subsample': 0.7, 'colsample_bytree': 0.6}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:08:36] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:36] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:08:36,618]\u001b[0m Trial 215 finished with value: 0.2504201680672269 and parameters: {'n_estimators': 112, 'max_depth': 3, 'min_child_weight': 20, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:08:36] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:36] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:08:37,885]\u001b[0m Trial 216 finished with value: 0.23697478991596643 and parameters: {'n_estimators': 399, 'max_depth': 4, 'min_child_weight': 19, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:08:37] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:37] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:08:38,778]\u001b[0m Trial 217 finished with value: 0.2571428571428571 and parameters: {'n_estimators': 278, 'max_depth': 4, 'min_child_weight': 19, 'subsample': 0.7, 'colsample_bytree': 0.9}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:08:38] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:38] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:08:39,857]\u001b[0m Trial 218 finished with value: 0.30420168067226894 and parameters: {'n_estimators': 376, 'max_depth': 4, 'min_child_weight': 10, 'subsample': 0.7, 'colsample_bytree': 0.6}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:08:39] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:39] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:08:41,022]\u001b[0m Trial 219 finished with value: 0.280672268907563 and parameters: {'n_estimators': 398, 'max_depth': 5, 'min_child_weight': 19, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:08:41] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:41] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:08:41,878]\u001b[0m Trial 220 finished with value: 0.2588235294117647 and parameters: {'n_estimators': 324, 'max_depth': 5, 'min_child_weight': 20, 'subsample': 0.5, 'colsample_bytree': 0.7}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-14 03:08:42,117]\u001b[0m Trial 221 finished with value: 0.24033613445378155 and parameters: {'n_estimators': 88, 'max_depth': 3, 'min_child_weight': 19, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:08:41] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:41] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-14 03:08:42,366]\u001b[0m Trial 222 finished with value: 0.27563025210084036 and parameters: {'n_estimators': 91, 'max_depth': 3, 'min_child_weight': 19, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:08:42] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:42] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:08:42] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:42] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:08:43,283]\u001b[0m Trial 223 finished with value: 0.24705882352941178 and parameters: {'n_estimators': 358, 'max_depth': 4, 'min_child_weight': 19, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:08:43] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:43] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:08:44,161]\u001b[0m Trial 224 finished with value: 0.27563025210084036 and parameters: {'n_estimators': 360, 'max_depth': 4, 'min_child_weight': 18, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:08:44] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:44] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:08:44,818]\u001b[0m Trial 225 finished with value: 0.27563025210084036 and parameters: {'n_estimators': 271, 'max_depth': 4, 'min_child_weight': 14, 'subsample': 0.7, 'colsample_bytree': 0.6}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-14 03:08:45,074]\u001b[0m Trial 226 finished with value: 0.2739495798319328 and parameters: {'n_estimators': 95, 'max_depth': 3, 'min_child_weight': 19, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:08:44] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:44] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:08:45] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:45] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:08:45,669]\u001b[0m Trial 227 finished with value: 0.2823529411764706 and parameters: {'n_estimators': 214, 'max_depth': 4, 'min_child_weight': 1, 'subsample': 0.7, 'colsample_bytree': 0.6}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-14 03:08:45,889]\u001b[0m Trial 228 finished with value: 0.2571428571428571 and parameters: {'n_estimators': 68, 'max_depth': 3, 'min_child_weight': 19, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:08:45] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:45] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:08:45] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:45] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:08:47,529]\u001b[0m Trial 229 finished with value: 0.2773109243697479 and parameters: {'n_estimators': 596, 'max_depth': 5, 'min_child_weight': 18, 'subsample': 0.7, 'colsample_bytree': 0.8}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-14 03:08:47,708]\u001b[0m Trial 230 finished with value: 0.2588235294117647 and parameters: {'n_estimators': 45, 'max_depth': 4, 'min_child_weight': 19, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:08:47] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:47] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "[03:08:47] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:47] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-14 03:08:48,678]\u001b[0m Trial 231 finished with value: 0.28403361344537814 and parameters: {'n_estimators': 403, 'max_depth': 4, 'min_child_weight': 19, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:08:48] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:48] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:08:49,421]\u001b[0m Trial 232 finished with value: 0.26890756302521013 and parameters: {'n_estimators': 356, 'max_depth': 3, 'min_child_weight': 19, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:08:49] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:49] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:08:50,069]\u001b[0m Trial 233 finished with value: 0.2773109243697479 and parameters: {'n_estimators': 304, 'max_depth': 3, 'min_child_weight': 20, 'subsample': 0.5, 'colsample_bytree': 0.7}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:08:50] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:50] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:08:50,975]\u001b[0m Trial 234 finished with value: 0.2420168067226891 and parameters: {'n_estimators': 345, 'max_depth': 4, 'min_child_weight': 19, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:08:51] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:51] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:08:51,970]\u001b[0m Trial 235 finished with value: 0.28739495798319326 and parameters: {'n_estimators': 324, 'max_depth': 4, 'min_child_weight': 20, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:08:52] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:52] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:08:52,982]\u001b[0m Trial 236 finished with value: 0.2890756302521008 and parameters: {'n_estimators': 361, 'max_depth': 4, 'min_child_weight': 18, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:08:53] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:53] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:08:54,266]\u001b[0m Trial 237 finished with value: 0.26722689075630257 and parameters: {'n_estimators': 502, 'max_depth': 5, 'min_child_weight': 19, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:08:54] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:54] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:08:54,964]\u001b[0m Trial 238 finished with value: 0.26890756302521013 and parameters: {'n_estimators': 292, 'max_depth': 4, 'min_child_weight': 19, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:08:55] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:55] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:08:56,141]\u001b[0m Trial 239 finished with value: 0.27899159663865547 and parameters: {'n_estimators': 424, 'max_depth': 4, 'min_child_weight': 2, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:08:56] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:56] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:08:57,192]\u001b[0m Trial 240 finished with value: 0.2823529411764706 and parameters: {'n_estimators': 378, 'max_depth': 4, 'min_child_weight': 9, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:08:57] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:57] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:08:57,987]\u001b[0m Trial 241 finished with value: 0.26386554621848735 and parameters: {'n_estimators': 348, 'max_depth': 3, 'min_child_weight': 20, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:08:58] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:58] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:08:58,405]\u001b[0m Trial 242 finished with value: 0.26722689075630257 and parameters: {'n_estimators': 254, 'max_depth': 1, 'min_child_weight': 18, 'subsample': 0.7, 'colsample_bytree': 0.6}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:08:58] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:58] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:08:59,244]\u001b[0m Trial 243 finished with value: 0.3058823529411765 and parameters: {'n_estimators': 335, 'max_depth': 4, 'min_child_weight': 19, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:08:59] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:59] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:08:59,676]\u001b[0m Trial 244 finished with value: 0.2621848739495798 and parameters: {'n_estimators': 131, 'max_depth': 3, 'min_child_weight': 20, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:08:59] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:08:59] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:09:00,752]\u001b[0m Trial 245 finished with value: 0.27563025210084036 and parameters: {'n_estimators': 382, 'max_depth': 5, 'min_child_weight': 19, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-14 03:09:01,016]\u001b[0m Trial 246 finished with value: 0.29075630252100837 and parameters: {'n_estimators': 107, 'max_depth': 2, 'min_child_weight': 18, 'subsample': 0.7, 'colsample_bytree': 0.6}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:09:00] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:09:00] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:09:01] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:09:01] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:09:01,884]\u001b[0m Trial 247 finished with value: 0.26722689075630257 and parameters: {'n_estimators': 334, 'max_depth': 4, 'min_child_weight': 20, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:09:01] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:09:01] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:09:02,642]\u001b[0m Trial 248 finished with value: 0.2705882352941177 and parameters: {'n_estimators': 316, 'max_depth': 3, 'min_child_weight': 19, 'subsample': 0.7, 'colsample_bytree': 0.9}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:09:02] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:09:02] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:09:03,070]\u001b[0m Trial 249 finished with value: 0.26050420168067223 and parameters: {'n_estimators': 236, 'max_depth': 1, 'min_child_weight': 17, 'subsample': 0.7, 'colsample_bytree': 0.6}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:09:03] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:09:03] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:09:03,477]\u001b[0m Trial 250 finished with value: 0.27899159663865547 and parameters: {'n_estimators': 196, 'max_depth': 2, 'min_child_weight': 20, 'subsample': 0.6, 'colsample_bytree': 0.6}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:09:03] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:09:03] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:09:03,888]\u001b[0m Trial 251 finished with value: 0.2739495798319328 and parameters: {'n_estimators': 169, 'max_depth': 2, 'min_child_weight': 16, 'subsample': 0.7, 'colsample_bytree': 0.6}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:09:03] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:09:03] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:09:04,874]\u001b[0m Trial 252 finished with value: 0.2941176470588235 and parameters: {'n_estimators': 348, 'max_depth': 4, 'min_child_weight': 19, 'subsample': 0.5, 'colsample_bytree': 0.8}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-14 03:09:05,120]\u001b[0m Trial 253 finished with value: 0.27563025210084036 and parameters: {'n_estimators': 89, 'max_depth': 3, 'min_child_weight': 20, 'subsample': 0.7, 'colsample_bytree': 0.5}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:09:04] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:09:04] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:09:05] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:09:05] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:09:05,868]\u001b[0m Trial 254 finished with value: 0.29075630252100837 and parameters: {'n_estimators': 308, 'max_depth': 4, 'min_child_weight': 19, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:09:05] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:09:05] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:09:06,690]\u001b[0m Trial 255 finished with value: 0.2941176470588235 and parameters: {'n_estimators': 342, 'max_depth': 4, 'min_child_weight': 20, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:09:06] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:09:06] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:09:07,024]\u001b[0m Trial 256 finished with value: 0.28403361344537814 and parameters: {'n_estimators': 135, 'max_depth': 3, 'min_child_weight': 5, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:09:07] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:09:07] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:09:07,397]\u001b[0m Trial 257 finished with value: 0.24537815126050422 and parameters: {'n_estimators': 115, 'max_depth': 5, 'min_child_weight': 20, 'subsample': 0.7, 'colsample_bytree': 0.8}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-14 03:09:07,637]\u001b[0m Trial 258 finished with value: 0.2823529411764706 and parameters: {'n_estimators': 64, 'max_depth': 5, 'min_child_weight': 19, 'subsample': 0.7, 'colsample_bytree': 0.8}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:09:07] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:09:07] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:09:07] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:09:07] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:09:08,772]\u001b[0m Trial 259 finished with value: 0.3025210084033614 and parameters: {'n_estimators': 285, 'max_depth': 12, 'min_child_weight': 19, 'subsample': 0.7, 'colsample_bytree': 0.8}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:09:08] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:09:08] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:09:09,371]\u001b[0m Trial 260 finished with value: 0.27563025210084036 and parameters: {'n_estimators': 217, 'max_depth': 4, 'min_child_weight': 20, 'subsample': 0.7, 'colsample_bytree': 0.8}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:09:09] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:09:09] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:09:09,991]\u001b[0m Trial 261 finished with value: 0.2621848739495798 and parameters: {'n_estimators': 278, 'max_depth': 3, 'min_child_weight': 18, 'subsample': 0.7, 'colsample_bytree': 0.8}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-14 03:09:10,159]\u001b[0m Trial 262 finished with value: 0.24369747899159666 and parameters: {'n_estimators': 31, 'max_depth': 5, 'min_child_weight': 20, 'subsample': 0.7, 'colsample_bytree': 0.8}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:09:10] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:09:10] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "[03:09:10] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:09:10] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-14 03:09:10,345]\u001b[0m Trial 263 finished with value: 0.25210084033613445 and parameters: {'n_estimators': 37, 'max_depth': 5, 'min_child_weight': 20, 'subsample': 0.7, 'colsample_bytree': 0.8}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:09:10] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:09:10] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:09:11,123]\u001b[0m Trial 264 finished with value: 0.2823529411764706 and parameters: {'n_estimators': 160, 'max_depth': 15, 'min_child_weight': 15, 'subsample': 0.7, 'colsample_bytree': 0.8}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-14 03:09:11,268]\u001b[0m Trial 265 finished with value: 0.280672268907563 and parameters: {'n_estimators': 16, 'max_depth': 6, 'min_child_weight': 20, 'subsample': 0.7, 'colsample_bytree': 0.8}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:09:11] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:09:11] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "[03:09:11] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:09:11] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:09:11,559]\u001b[0m Trial 266 finished with value: 0.2621848739495798 and parameters: {'n_estimators': 75, 'max_depth': 5, 'min_child_weight': 20, 'subsample': 0.7, 'colsample_bytree': 0.8}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:09:11] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:09:11] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:09:11,900]\u001b[0m Trial 267 finished with value: 0.25546218487394956 and parameters: {'n_estimators': 116, 'max_depth': 4, 'min_child_weight': 20, 'subsample': 0.7, 'colsample_bytree': 0.8}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-14 03:09:12,079]\u001b[0m Trial 268 finished with value: 0.27226890756302524 and parameters: {'n_estimators': 36, 'max_depth': 5, 'min_child_weight': 19, 'subsample': 0.6, 'colsample_bytree': 0.8}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:09:11] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:09:11] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:09:12] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:09:12] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:09:12,676]\u001b[0m Trial 269 finished with value: 0.28739495798319326 and parameters: {'n_estimators': 143, 'max_depth': 4, 'min_child_weight': 20, 'subsample': 0.7, 'colsample_bytree': 0.8}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:09:12] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:09:12] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:09:13,382]\u001b[0m Trial 270 finished with value: 0.28403361344537814 and parameters: {'n_estimators': 249, 'max_depth': 4, 'min_child_weight': 19, 'subsample': 0.7, 'colsample_bytree': 0.6}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:09:13] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:09:13] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:09:14,121]\u001b[0m Trial 271 finished with value: 0.29579831932773104 and parameters: {'n_estimators': 199, 'max_depth': 6, 'min_child_weight': 20, 'subsample': 0.7, 'colsample_bytree': 0.8}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:09:14] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:09:14] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:09:15,203]\u001b[0m Trial 272 finished with value: 0.29915966386554627 and parameters: {'n_estimators': 457, 'max_depth': 4, 'min_child_weight': 20, 'subsample': 0.7, 'colsample_bytree': 0.6}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:09:15] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:09:15] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:09:15,594]\u001b[0m Trial 273 finished with value: 0.26890756302521013 and parameters: {'n_estimators': 99, 'max_depth': 5, 'min_child_weight': 4, 'subsample': 0.7, 'colsample_bytree': 0.6}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:09:15] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:09:15] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:09:16,278]\u001b[0m Trial 274 finished with value: 0.27226890756302524 and parameters: {'n_estimators': 228, 'max_depth': 4, 'min_child_weight': 19, 'subsample': 0.7, 'colsample_bytree': 0.8}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:09:16] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:09:16] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:09:17,173]\u001b[0m Trial 275 finished with value: 0.28403361344537814 and parameters: {'n_estimators': 398, 'max_depth': 3, 'min_child_weight': 20, 'subsample': 0.7, 'colsample_bytree': 0.9}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:09:17] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:09:17] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:09:18,094]\u001b[0m Trial 276 finished with value: 0.30756302521008405 and parameters: {'n_estimators': 266, 'max_depth': 5, 'min_child_weight': 19, 'subsample': 0.7, 'colsample_bytree': 0.6}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-14 03:09:18,329]\u001b[0m Trial 277 finished with value: 0.24537815126050422 and parameters: {'n_estimators': 54, 'max_depth': 4, 'min_child_weight': 18, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:09:18] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:09:18] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-14 03:09:18,569]\u001b[0m Trial 278 finished with value: 0.2823529411764706 and parameters: {'n_estimators': 61, 'max_depth': 4, 'min_child_weight': 18, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:09:18] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:09:18] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-14 03:09:18,809]\u001b[0m Trial 279 finished with value: 0.280672268907563 and parameters: {'n_estimators': 50, 'max_depth': 3, 'min_child_weight': 18, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:09:18] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:09:18] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-14 03:09:19,033]\u001b[0m Trial 280 finished with value: 0.23697478991596643 and parameters: {'n_estimators': 38, 'max_depth': 4, 'min_child_weight': 18, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:09:18] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:09:18] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-14 03:09:19,256]\u001b[0m Trial 281 finished with value: 0.26386554621848735 and parameters: {'n_estimators': 51, 'max_depth': 4, 'min_child_weight': 18, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:09:19] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:09:19] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "[03:09:19] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-14 03:09:19,370]\u001b[0m Trial 282 finished with value: 0.2857142857142857 and parameters: {'n_estimators': 12, 'max_depth': 3, 'min_child_weight': 18, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-14 03:09:19,493]\u001b[0m Trial 283 finished with value: 0.253781512605042 and parameters: {'n_estimators': 13, 'max_depth': 5, 'min_child_weight': 18, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:09:19] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "[03:09:19] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:09:19] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-14 03:09:19,645]\u001b[0m Trial 284 finished with value: 0.29075630252100837 and parameters: {'n_estimators': 38, 'max_depth': 3, 'min_child_weight': 17, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:09:19] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:09:19] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "[03:09:19] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:09:19] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:09:19,867]\u001b[0m Trial 285 finished with value: 0.31092436974789917 and parameters: {'n_estimators': 80, 'max_depth': 2, 'min_child_weight': 18, 'subsample': 0.7, 'colsample_bytree': 0.6}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-14 03:09:20,100]\u001b[0m Trial 286 finished with value: 0.253781512605042 and parameters: {'n_estimators': 65, 'max_depth': 4, 'min_child_weight': 19, 'subsample': 0.6, 'colsample_bytree': 0.7}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:09:19] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:09:19] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-14 03:09:20,267]\u001b[0m Trial 287 finished with value: 0.2739495798319328 and parameters: {'n_estimators': 27, 'max_depth': 5, 'min_child_weight': 18, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:09:20] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:09:20] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "[03:09:20] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:09:20] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:09:21,531]\u001b[0m Trial 288 finished with value: 0.253781512605042 and parameters: {'n_estimators': 553, 'max_depth': 4, 'min_child_weight': 17, 'subsample': 0.7, 'colsample_bytree': 0.6}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:09:21] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:09:21] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:09:21,840]\u001b[0m Trial 289 finished with value: 0.26722689075630257 and parameters: {'n_estimators': 101, 'max_depth': 4, 'min_child_weight': 7, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-14 03:09:22,064]\u001b[0m Trial 290 finished with value: 0.253781512605042 and parameters: {'n_estimators': 74, 'max_depth': 3, 'min_child_weight': 19, 'subsample': 0.7, 'colsample_bytree': 0.8}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:09:21] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:09:21] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-14 03:09:22,247]\u001b[0m Trial 291 finished with value: 0.24705882352941178 and parameters: {'n_estimators': 28, 'max_depth': 10, 'min_child_weight': 19, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:09:22] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:09:22] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "[03:09:22] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:09:22] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-14 03:09:22,453]\u001b[0m Trial 292 finished with value: 0.24873949579831933 and parameters: {'n_estimators': 36, 'max_depth': 11, 'min_child_weight': 19, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-14 03:09:22,705]\u001b[0m Trial 293 finished with value: 0.28403361344537814 and parameters: {'n_estimators': 50, 'max_depth': 9, 'min_child_weight': 19, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:09:22] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:09:22] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:09:22] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:09:22] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:09:23,099]\u001b[0m Trial 294 finished with value: 0.2504201680672269 and parameters: {'n_estimators': 82, 'max_depth': 17, 'min_child_weight': 18, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-14 03:09:23,214]\u001b[0m Trial 295 finished with value: 0.23529411764705888 and parameters: {'n_estimators': 4, 'max_depth': 13, 'min_child_weight': 19, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-14 03:09:23,318]\u001b[0m Trial 296 finished with value: 0.29915966386554627 and parameters: {'n_estimators': 7, 'max_depth': 2, 'min_child_weight': 18, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:09:23] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:09:23] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "[03:09:23] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:09:23] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-14 03:09:23,492]\u001b[0m Trial 297 finished with value: 0.2739495798319328 and parameters: {'n_estimators': 24, 'max_depth': 10, 'min_child_weight': 19, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:09:23] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:09:23] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "[03:09:23] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:09:23] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-12-14 03:09:23,627]\u001b[0m Trial 298 finished with value: 0.280672268907563 and parameters: {'n_estimators': 14, 'max_depth': 14, 'min_child_weight': 19, 'subsample': 0.7, 'colsample_bytree': 0.7}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
            "\u001b[32m[I 2022-12-14 03:09:23,821]\u001b[0m Trial 299 finished with value: 0.2621848739495798 and parameters: {'n_estimators': 26, 'max_depth': 12, 'min_child_weight': 18, 'subsample': 0.7, 'colsample_bytree': 0.9}. Best is trial 141 with value: 0.22016806722689075.\u001b[0m\n",
            "c:\\Users\\hiroto\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[03:09:23] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
            "Parameters: { \"verbose\" } might not be used.\n",
            "\n",
            "  This could be a false alarm, with some parameters getting used by language bindings but\n",
            "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
            "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
            "\n",
            "\n",
            "[03:09:23] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "[03:09:23] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        }
      ],
      "source": [
        "study = A.build_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "bhE-SFDDSc20"
      },
      "outputs": [],
      "source": [
        "y_pred = study.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qd2lNxJjTSEA",
        "outputId": "07341358-9fa3-4c7d-c177-4077d5b3d966"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.7307110438729199"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "accuracy_score(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(cm,\n",
        "                          target_names,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=None,\n",
        "                          normalize=True):\n",
        "    import itertools\n",
        "\n",
        "    accuracy = np.trace(cm) / np.sum(cm).astype('float')\n",
        "    misclass = 1 - accuracy\n",
        "\n",
        "    if cmap is None:\n",
        "        cmap = plt.get_cmap('Blues')\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "\n",
        "    if target_names is not None:\n",
        "        tick_marks = np.arange(len(target_names))\n",
        "        plt.xticks(tick_marks, target_names, rotation=45)\n",
        "        plt.yticks(tick_marks, target_names)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "\n",
        "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        if normalize:\n",
        "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
        "                     horizontalalignment=\"center\",\n",
        "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "        else:\n",
        "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
        "                     horizontalalignment=\"center\",\n",
        "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "matrix = metrics.confusion_matrix(y_test, y_pred, normalize=\"all\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "target_names = [\"False\", \"True\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plot_confusion_matrix(matrix, target_names, title='Confusion matrix', cmap=None, normalize=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "8ncQiM28xXo-"
      },
      "outputs": [],
      "source": [
        "def result(X_train, X_test, y_train, y_test, model):\n",
        "  print(\"accuracy scores\", accuracy_score(y_test, y_pred))\n",
        "  metrics.roc_auc_score(y_test, y_pred)\n",
        "  matrix = metrics.confusion_matrix(y_test, y_pred, normalize=\"all\", labels={0:\"liquid\", 1:\"solid\"})\n",
        "  ax = sns.heatmap(matrix, annot=True, cmap=\"Blues\")\n",
        "  \n",
        "  y_proba = model.predict_proba(X_test)\n",
        "\n",
        "  fpr, tpr, thresholds = metrics.roc_curve(y_test, y_proba[:, 1])\n",
        "  auc = metrics.auc(fpr, tpr)\n",
        "  \n",
        "  fig = plt.figure()\n",
        "  ax = fig.add_subplot()\n",
        "  ax.plot(fpr, tpr, label='ROC curve (area = %.2f)'%auc)\n",
        "  ax.legend()\n",
        "  ax.set_xlabel('FPR: False positive rate')\n",
        "  ax.set_ylabel('TPR: True positive rate')\n",
        "  ax.grid()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "py37",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.15"
    },
    "vscode": {
      "interpreter": {
        "hash": "13829c6cee335f7d4f78a61190ede447d35a3772f027e7e85d688cdb17698982"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
